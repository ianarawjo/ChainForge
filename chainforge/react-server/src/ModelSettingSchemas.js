/**
 * A place to put the react-jsonschema-form JSON schemas for all 
 * models supported by ChainForge. These describe the structure of HTML
 * settings forms for that specific model. 
 * 
 * By convention, the key used for a 'property' should be the exact same 
 * parameter name in the back-end for that API call (e.g., 'top_k' for OpenAI chat completions)
 * 
 * Descriptions of OpenAI model parameters copied from OpenAI's official chat completions documentation: https://platform.openai.com/docs/models/model-endpoint-compatibility
 */

const ChatGPTSettings = {
    fullName: "GPT-3.5 (ChatGPT)",
    schema: {
        "type": "object",
        "required": [
            "shortname"
        ],
        "properties": {
            "shortname": {
                "type": "string",
                "title": "Nickname",
                "description": "Unique identifier to appear in ChainForge. Keep it short.",
                "default": "GPT3.5"
            },
            "model": {
                "type": "string",
                "title": "Model Version",
                "description": "Select a version of GPT3.5 to query. For more details on the differences, see the OpenAI API documentation. (Note that all ChainForge OpenAI calls use the Chat Completions API; we intend to support just Completions in the future.)",
                "enum": ["gpt-3.5-turbo", "gpt-3.5-turbo-0301", "text-davinci-003", "text-davinci-002", "code-davinci-002"],
                "default": "gpt-3.5-turbo"
            },
            "system_msg": {
                "type": "string",
                "title": "System Message",
                "description": "Many conversations begin with a system message to gently instruct the assistant. By default, ChainForge includes the suggested 'You are a helpful assistant.'",
                "default": "You are a helpful assistant."
            },
            "temperature": {
                "type": "number",
                "title": "temperature",
                "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.",
                "default": 1,
                "minimum": 0,
                "maximum": 2,
                "multipleOf": 0.01
            },
            "top_p": {
                "type": "number",
                "title": "top_p",
                "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.",
                "default": 1,
                "minimum": 0,
                "maximum": 1,
                "multipleOf": 0.005
            },
            "stop": {
                "type": "string",
                "title": "stop sequences",
                "description": "Up to 4 sequences where the API will stop generating further tokens. Enclose stop sequences in double-quotes \"\" and use whitespace to separate them.",
                "default": ""
            },
            "max_tokens": {
                "type": "integer",
                "title": "max_tokens",
                "description": "The maximum number of tokens to generate in the chat completion. (The total length of input tokens and generated tokens is limited by the model's context length.)"
            },
            "presence_penalty": {
                "type": "number",
                "title": "presence_penalty",
                "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
                "default": 0,
                "minimum": -2,
                "maximum": 2,
                "multipleOf": 0.005
            },
            "frequency_penalty": {
                "type": "number",
                "title": "frequency_penalty",
                "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
                "default": 0,
                "minimum": -2,
                "maximum": 2,
                "multipleOf": 0.005
            },
            "logit_bias": {
                "type": "string",
                "title": "logit_bias",
                "description": "Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."
            }
        }
    },

    uiSchema: {
        'ui:submitButtonOptions': {
            props: {
              disabled: false,
              className: 'mantine-UnstyledButton-root mantine-Button-root',
            },
            norender: false,
            submitText: 'Submit',
        },
        "shortname": {
          "ui:autofocus": true
        },
        "model": {
          "ui:help": "Defaults to gpt-3.5-turbo.",
        },
        "system_msg": {
          "ui:widget": "textarea",
          "ui:help": "For more details, see the OpenAI documentation: https://platform.openai.com/docs/guides/chat/instructing-chat-models"
        },
        "temperature": {
          "ui:help": "Defaults to 1.0. Leave at default if you prefer to set top_p.",
          "ui:widget": "range"
        },
        "top_p": {
          "ui:help": "Defaults to 1.0. Leave at default if you prefer to set temperature.",
          "ui:widget": "range"
        },
        "presence_penalty": {
          "ui:help": "Defaults to 0.",
          "ui:widget": "range"
        },
        "frequency_penalty": {
          "ui:help": "Defaults to 0.",
          "ui:widget": "range"
        },
        "stop": {
          "ui:widget": "textarea",
          "ui:help": "Defaults to empty."
        },
        "max_tokens": {
          "ui:help": "Defaults to infinity."
        },
        "logit_bias": {
          "ui:widget": "textarea",
          "ui:help": "Defaults to none."
        }
    },

    postprocessors: {
      'stop': (str) => {
        if (str.trim().length === 0) return [];
        return str.match(/"((?:[^"\\]|\\.)*)"/g).map(s => s.substring(1, s.length-1)); // split on double-quotes but exclude escaped double-quotes inside the group
      }
    }
};

const GPT4Settings = {
    fullName: "GPT-4",
    schema: {
      "type": "object",
        "required": [
            "shortname"
        ],
        "properties": {
            ...ChatGPTSettings.schema.properties,
            "shortname": {
              "type": "string",
              "title": "Nickname",
              "description": "Unique identifier to appear in ChainForge. Keep it short.",
              "default": "GPT-4"
            },
            "model": {
              "type": "string",
              "title": "Model Version",
              "description": "Select a version of GPT-4 to query. For more details on the differences, see the OpenAI API documentation. (Note that all ChainForge OpenAI calls use the Chat Completions API; we intend to support just Completions in the future.)",
              "enum": ["gpt-4", "gpt-4-0314", "gpt-4-32k", "gpt-4-32k-0314"],
              "default": "gpt-4"
            },
        }
    },
    uiSchema: ChatGPTSettings.uiSchema,
    postprocessors: ChatGPTSettings.postprocessors,
};

const ClaudeSettings = {
    fullName: "Claude (Anthropic)",
    schema: {
        "type": "object",
        "required": [
            "shortname"
        ],
        "properties": {
            "shortname": {
                "type": "string",
                "title": "Nickname",
                "description": "Unique identifier to appear in ChainForge. Keep it short.",
                "default": "Claude"
            },
            "model": {
                "type": "string",
                "title": "Model Version",
                "description": "Select a version of Claude to query. For more details on the differences, see the Anthropic API documentation.",
                "enum": ["claude-v1", "claude-v1-100k", "claude-instant-v1", "claude-instant-v1-100k", "claude-v1.3", 
                         "claude-v1.3-100k", "claude-v1.2", "claude-v1.0", "claude-instant-v1.1", "claude-instant-v1.1-100k", "claude-instant-v1.0"],
                "default": "claude-v1"
            },
            "temperature": {
                "type": "number",
                "title": "temperature",
                "description": "Amount of randomness injected into the response. Ranges from 0 to 1. Use temp closer to 0 for analytical / multiple choice, and temp closer to 1 for creative and generative tasks.",
                "default": 1,
                "minimum": 0,
                "maximum": 1,
                "multipleOf": 0.01
            },
            "max_tokens_to_sample": {
                "type": "integer",
                "title": "max_tokens_to_sample",
                "description": "A maximum number of tokens to generate before stopping. Lower this if you want shorter responses. By default, ChainForge uses the value 1024, although the Anthropic API does not specify a default value.",
                "default": 1024,
                "minimum": 1
            },
            "custom_prompt_wrapper": {
                "type": "string",
                "title": "Prompt Wrapper (ChainForge)",
                "description": "Anthropic models expect prompts in the form \"\\n\\nHuman: ${prompt}\\n\\nAssistant:\". ChainForge wraps all prompts in this template by default. If you wish to \
                explore custom prompt wrappers that deviate, write a Python template here with a single variable, ${prompt}, where the actual prompt text should go. Otherwise, leave this field blank. (Note that you should enter newlines as newlines, not escape codes like \\n.)",
                "default": ""
            },
            "stop_sequences": {
                "type": "string",
                "title": "stop_sequences",
                "description": "Anthropic models stop on \"\\n\\nHuman:\", and may include additional built-in stop sequences in the future. By providing the stop_sequences parameter, you may include additional strings that will cause the model to stop generating.\nEnclose stop sequences in double-quotes \"\" and use whitespace to separate them.",
                "default": "\"\n\nHuman:\""
            },
            "top_k": {
                "type": "integer",
                "title": "top_k",
                "description": "Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Defaults to -1, which disables it.",
                "minimum": -1,
                "default": -1
            },
            "top_p": {
                "type": "number",
                "title": "top_p",
                "description": "Does nucleus sampling, in which we compute the cumulative distribution over all the options for each subsequent token in decreasing probability order and cut it off once it reaches a particular probability specified by top_p. Defaults to -1, which disables it. Note that you should either alter temperature or top_p, but not both.",
                "default": -1,
                "minimum": -1,
                "maximum": 1,
                "multipleOf": 0.001,
            },
        }
    },

    uiSchema: {
        'ui:submitButtonOptions': {
            props: {
              disabled: false,
              className: 'mantine-UnstyledButton-root mantine-Button-root',
            },
            norender: false,
            submitText: 'Submit',
        },
        "shortname": {
          "ui:autofocus": true
        },
        "model": {
            "ui:help": "Defaults to claude-v1."
        },
        "temperature": {
          "ui:help": "Defaults to 1.0.",
          "ui:widget": "range"
        },
        "max_tokens_to_sample": {
            "ui:help": "Defaults to 1024."
        },
        "top_k": {
            "ui:help": "Defaults to -1 (none)."
        },
        "top_p": {
          "ui:help": "Defaults to -1 (none)."
        },
        "stop_sequences": {
          "ui:widget": "textarea",
          "ui:help": "Defaults to one stop sequence, \"\\n\\nHuman: \""
        },
        "custom_prompt_wrapper": {
          "ui:widget": "textarea",
          "ui:help": "Defaults to Anthropic's internal wrapper \"\\n\\nHuman: {prompt}\\n\\nAssistant\"."
        }
    },

    postprocessors: {
      'stop_sequences': (str) => {
        if (str.trim().length === 0) return ["\n\nHuman:"];
        return str.match(/"((?:[^"\\]|\\.)*)"/g).map(s => s.substring(1, s.length-1)); // split on double-quotes but exclude escaped double-quotes inside the group
      }
    }
}

const PaLM2Settings = {
  fullName: "PaLM (Google)",
  schema: {
      "type": "object",
      "required": [
          "shortname"
      ],
      "properties": {
          "shortname": {
              "type": "string",
              "title": "Nickname",
              "description": "Unique identifier to appear in ChainForge. Keep it short.",
              "default": "chat-bison"
          },
          "model": {
              "type": "string",
              "title": "Model",
              "description": "Select a PaLM model to query. For more details on the differences, see the Google PaLM API documentation.",
              "enum": ["text-bison-001", "chat-bison-001"],
              "default": "chat-bison-001"
          },
          "temperature": {
              "type": "number",
              "title": "temperature",
              "description": "Controls the randomness of the output. Must be positive. Typical values are in the range: [0.0, 1.0]. Higher values produce a more random and varied response. A temperature of zero will be deterministic. (ChainForge only allows a max 1.0 temperature for PaLM).",
              "default": 0.5,
              "minimum": 0,
              "maximum": 1,
              "multipleOf": 0.01
          },
          "top_k": {
            "type": "integer",
            "title": "top_k",
            "description": "Sets the maximum number of tokens to sample from on each step. (The PaLM API uses combined nucleus and top-k sampling.) Set to -1 to use the default value.",
            "minimum": -1,
            "default": -1
          },
          "top_p": {
              "type": "number",
              "title": "top_p",
              "description": "Sets the maximum cumulative probability of tokens to sample from. (The PaLM API uses combined nucleus and top-k sampling.) Set to -1 to use the default value.",
              "default": -1,
              "minimum": -1,
              "maximum": 1,
              "multipleOf": 0.001,
          },
          "max_output_tokens": {
              "type": "integer",
              "title": "max_output_tokens (ignored for chat models)",
              "description": "Maximum number of tokens to include in each response of a text-bison model. Must be greater than zero. If unset, will default to 512. Ignored for chat models.",
              "default": 512,
              "minimum": 1
          },
          "stop_sequences": {
              "type": "string",
              "title": "stop_sequences (ignored for chat models)",
              "description": "A set of up to 5 character sequences that will stop output generation. If specified, the API will stop at the first appearance of a stop sequence. The stop sequence will not be included as part of the response.\nEnclose stop sequences in double-quotes \"\" and use whitespace to separate them. Ignored for chat models.",
              "default": ""
          },
      }
  },

  uiSchema: {
      'ui:submitButtonOptions': {
          props: {
            disabled: false,
            className: 'mantine-UnstyledButton-root mantine-Button-root',
          },
          norender: false,
          submitText: 'Submit',
      },
      "shortname": {
        "ui:autofocus": true
      },
      "model": {
          "ui:help": "Defaults to chat-bison."
      },
      "temperature": {
        "ui:help": "Defaults to 0.5.",
        "ui:widget": "range"
      },
      "max_output_tokens": {
          "ui:help": "Defaults to 512. Only relevant to text completions models (text-bison)."
      },
      "top_k": {
          "ui:help": "Defaults to -1 (none)."
      },
      "top_p": {
        "ui:help": "Defaults to -1 (none)."
      },
      "stop_sequences": {
        "ui:widget": "textarea",
        "ui:help": "Defaults to no additional stop sequences (empty). Ignored for chat models."
      },
  },

  postprocessors: {
    'stop_sequences': (str) => {
      if (str.trim().length === 0) return [];
      return str.match(/"((?:[^"\\]|\\.)*)"/g).map(s => s.substring(1, s.length-1)); // split on double-quotes but exclude escaped double-quotes inside the group
    }
  }
}

export const ModelSettings = {
  'gpt-3.5-turbo': ChatGPTSettings,
  'gpt-4': GPT4Settings,
  'claude-v1': ClaudeSettings,
  'palm2-bison': PaLM2Settings,
}

export const getTemperatureSpecForModel = (modelName) => {
  if (modelName in ModelSettings) {
    const temperature_property = ModelSettings[modelName].schema?.properties?.temperature;
    if (temperature_property) {
      return {minimum: temperature_property.minimum, maximum: temperature_property.maximum, default: temperature_property.default};
    }
  }
  return null;
};

export const postProcessFormData = (settingsSpec, formData) => {
  // Strip all 'model' and 'shortname' props in the submitted form, as these are passed elsewhere or unecessary for the backend
  const skip_keys = {'model': true, 'shortname': true};

  let new_data = {};
  let postprocessors = settingsSpec.postprocessors ? settingsSpec.postprocessors : {};

  Object.keys(formData).forEach(key => {
    if (key in skip_keys) return;
    if (key in postprocessors)
      new_data[key] = postprocessors[key](formData[key]);
    else
      new_data[key] = formData[key];
  });
    
  return new_data;
};

export const getDefaultModelFormData = (settingsSpec) => {
  let default_formdata = {};
  const schema = settingsSpec.schema;
  Object.keys(schema.properties).forEach(key => {
    default_formdata[key] = 'default' in schema.properties[key] ? schema.properties[key]['default'] : undefined;
  });
  return default_formdata;
};

export const getDefaultModelSettings = (modelName) => {
  if (!(modelName in ModelSettings)) {
    console.warn(`Model ${modelName} not found in list of available model settings.`);
    return {};
  }
  const settingsSpec = ModelSettings[modelName];
  return postProcessFormData(settingsSpec, getDefaultModelFormData(settingsSpec));
};