{
  "flow": {
    "nodes": [
      {
        "width": 345,
        "height": 377,
        "data": {
          "evaluators": [
            {
              "name": "Faithfulness",
              "state": {
                "format": "num",
                "grader": {
                  "name": "GPT4o-mini",
                  "emoji": "ðŸ”¬",
                  "model": "gpt-4o-mini",
                  "base_model": "gpt-4",
                  "temp": 1,
                  "key": "e8f7a7ed-2657-4561-a665-b03226274788",
                  "settings": {
                    "system_msg": "You are a helpful assistant.",
                    "temperature": 1,
                    "response_format": {
                      "type": "text"
                    },
                    "tools": [],
                    "tool_choice": "",
                    "parallel_tool_calls": true,
                    "top_p": 1,
                    "stop": [],
                    "seed": null,
                    "max_completion_tokens": null,
                    "presence_penalty": 0,
                    "frequency_penalty": 0,
                    "logit_bias": null
                  },
                  "formData": {
                    "shortname": "GPT4o-mini",
                    "model": "gpt-4o-mini"
                  }
                },
                "prompt": "You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the faithfulness of a generated answer based solely on the provided context.\n\nFaithfulness measures whether the generated answer contains only information that is supported by the provided context. An answer is considered faithful if all its factual claims are directly supported by or can be reasonably inferred from the context\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the context (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the context (assign 0 points)\n3. Calculate the faithfulness score as: (sum of points) / (total number of claims)\n\nReturn a final faithfulness score between 0 and 1, where:\n- 1.0: Perfect faithfulness (all claims fully supported)\n- 0.0: Complete unfaithfulness (no claims supported by context)\n\nProvided Context: {#context}\n\nAnswer:",
                "reasonBeforeScoring": true
              },
              "type": "llm",
              "uid": "68714d6a-c9ef-4925-9d52-8d0521615192"
            },
            {
              "name": "Answer Relevancy",
              "uid": "b6ded17b-a8ad-4794-82ef-23d779e81b83",
              "type": "llm",
              "state": {
                "prompt": "You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant a generated answer is to the given question, regardless of factual accuracy\n\nAnswer Relevancy measures whether the generated answer directly addresses what was asked in the question. An answer is highly relevant if it provides information that specifically responds to the query's intent and information needs.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. Assess whether the answer:\n   - Directly addresses all aspects of the question (assign 1 point)\n   - Only partially addresses the question (assign 0.5 points)\n   - Fails to address the question or is off-topic (assign 0 points)\n\n\nReturn a final answer relevancy score between 0 and 1, where:\n- 1.0: Perfect relevancy (completely addresses the question with focus)\n- 0.0: Complete irrelevancy (does not address the question at all)\n\n```\n{#query}\n```\n\nAnswer:",
                "format": "num",
                "reasonBeforeScoring": true,
                "grader": {
                  "name": "GPT4o-mini",
                  "emoji": "ðŸ”¬",
                  "model": "gpt-4o-mini",
                  "base_model": "gpt-4",
                  "temp": 1,
                  "key": "fc48a1ab-fdd8-42ab-9b36-55c247bba13a",
                  "settings": {
                    "system_msg": "You are a helpful assistant.",
                    "temperature": 1,
                    "response_format": {
                      "type": "text"
                    },
                    "tools": [],
                    "tool_choice": "",
                    "parallel_tool_calls": true,
                    "top_p": 1,
                    "stop": [],
                    "seed": null,
                    "max_completion_tokens": null,
                    "presence_penalty": 0,
                    "frequency_penalty": 0,
                    "logit_bias": null
                  },
                  "formData": {
                    "shortname": "GPT4o-mini",
                    "model": "gpt-4o-mini"
                  }
                }
              }
            },
            {
              "name": "Context Precision",
              "uid": "741cbd8e-fb42-4a14-a720-4dcd8541456d",
              "type": "llm",
              "state": {
                "prompt": "You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the precision of the retrieved context relative to the question\n\nContext Precision measures how efficiently the retrieved information is used to answer the question. High precision means most of the retrieved context was necessary and used in generating the answer, with minimal irrelevant information.\n\nEvaluation instructions:\n1. Identify which parts of the context were actually used to generate the answer.\n2. Calculate the proportion of the context that contains information relevant to and used in the answer.\n3. Consider both:\n   - Information density (how much of the context is relevant)\n   - Information utilization (how much of the relevant information was used)\n\n4. Rate the context precision on a scale from 0 to 1:\n   - 1.0: Perfect precision (all retrieved context is relevant and used efficiently)\n   - 0.5: Moderate precision (about half the context is relevant and used)\n   - 0.0: No precision (context is entirely irrelevant)\n\n```\nQuestion: {#query}\n```\nContext: {#context}\n\nAnswer:",
                "format": "num",
                "reasonBeforeScoring": true,
                "grader": {
                  "name": "GPT4o-mini",
                  "emoji": "ðŸ”¬",
                  "model": "gpt-4o-mini",
                  "base_model": "gpt-4",
                  "temp": 1,
                  "key": "8165b1c7-9e24-4f49-8e27-b2236032537d",
                  "settings": {
                    "system_msg": "You are a helpful assistant.",
                    "temperature": 1,
                    "response_format": {
                      "type": "text"
                    },
                    "tools": [],
                    "tool_choice": "",
                    "parallel_tool_calls": true,
                    "top_p": 1,
                    "stop": [],
                    "seed": null,
                    "max_completion_tokens": null,
                    "presence_penalty": 0,
                    "frequency_penalty": 0,
                    "logit_bias": null
                  },
                  "formData": {
                    "shortname": "GPT4o-mini",
                    "model": "gpt-4o-mini"
                  }
                }
              }
            },
            {
              "name": "Context Relevancy",
              "uid": "fb5bcf43-0ff1-4aa7-a325-cabd239ce297",
              "type": "llm",
              "state": {
                "prompt": "You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: {#query}\n```\n\nContext:",
                "format": "num",
                "reasonBeforeScoring": true,
                "grader": {
                  "name": "GPT4o-mini",
                  "emoji": "ðŸ”¬",
                  "model": "gpt-4o-mini",
                  "base_model": "gpt-4",
                  "temp": 1,
                  "key": "e6d323e4-5f2e-40c2-903e-fd0c0933d0e1",
                  "settings": {
                    "system_msg": "You are a helpful assistant.",
                    "temperature": 1,
                    "response_format": {
                      "type": "text"
                    },
                    "tools": [],
                    "tool_choice": "",
                    "parallel_tool_calls": true,
                    "top_p": 1,
                    "stop": [],
                    "seed": null,
                    "max_completion_tokens": null,
                    "presence_penalty": 0,
                    "frequency_penalty": 0,
                    "logit_bias": null
                  },
                  "formData": {
                    "shortname": "GPT4o-mini",
                    "model": "gpt-4o-mini"
                  }
                }
              }
            },
            {
              "name": "Answer Correctness",
              "uid": "1def5fa2-ec3e-4267-9e65-e350d5f97785",
              "type": "llm",
              "state": {
                "prompt": "You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the correctness of a generated answer based on a reference answer.\n\nAnswer Correctness measures how closely the generated answer matches the reference answer. An answer is considered correct if it contains all the necessary information from the reference answer and is free of errors.\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the reference answer (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the reference answer (assign 0 points)\n\n\nReturn a final answer correctness score between 0 and 1, where:\n- 1.0: Perfect correctness (all claims fully supported)\n- 0.0: Complete incorrectness (no claims supported by reference)\n\n```\nReference Answer: {#reference}\n```\n\nAnswer:",
                "format": "num",
                "reasonBeforeScoring": true,
                "grader": {
                  "name": "GPT4o-mini",
                  "emoji": "ðŸ”¬",
                  "model": "gpt-4o-mini",
                  "base_model": "gpt-4",
                  "temp": 1,
                  "key": "e5447968-8d09-4053-be05-d11e423f7d88",
                  "settings": {
                    "system_msg": "You are a helpful assistant.",
                    "temperature": 1,
                    "response_format": {
                      "type": "text"
                    },
                    "tools": [],
                    "tool_choice": "",
                    "parallel_tool_calls": true,
                    "top_p": 1,
                    "stop": [],
                    "seed": null,
                    "max_completion_tokens": null,
                    "presence_penalty": 0,
                    "frequency_penalty": 0,
                    "logit_bias": null
                  },
                  "formData": {
                    "shortname": "GPT4o-mini",
                    "model": "gpt-4o-mini"
                  }
                }
              }
            },
            {
              "name": "Chunk Overlap",
              "uid": "20989a16-087a-40aa-bf1d-3390e9dc2aab",
              "type": "python",
              "state": {
                "code": "def evaluate(response):\n  context = response.var['context']\n  gtruth = response.meta['answer_context']\n  chunks = context.split('\\n\\n')\n  return any(c in gtruth for c in chunks)",
                "sandbox": true
              }
            }
          ],
          "output": [
            {
              "llm": "gpt-4o-mini",
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 22,
                "baseMethod": 34,
                "chunkMethod": 5,
                "method": 36,
                "reference": 21,
                "signature": 38,
                "source_doc": 23
              },
              "prompt": 182,
              "responses": [
                17
              ],
              "uid": "fac8100e-19a6-4045-b612-2587c5fde08b",
              "vars": {
                "baseMethod": 34,
                "chunkLibrary": 4,
                "chunkMethod": 5,
                "context": 37,
                "methodId": 92,
                "query": 19,
                "retrievalMethod": 36
              },
              "eval_res": {
                "items": [
                  {
                    "Faithfulness": "1.0",
                    "Answer Relevancy": 1,
                    "Context Precision": "0.5",
                    "Context Relevancy": 1,
                    "Answer Correctness": "0.5",
                    "Chunk Overlap": true
                  }
                ],
                "dtype": "KeyValue_Mixed"
              }
            },
            {
              "llm": "gpt-4o-mini",
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 13,
                "baseMethod": 34,
                "chunkMethod": 5,
                "method": 36,
                "reference": 12,
                "signature": 38,
                "source_doc": 15
              },
              "prompt": 183,
              "responses": [
                2
              ],
              "uid": "07647594-a268-4772-8358-783edb8c47e2",
              "vars": {
                "baseMethod": 34,
                "chunkLibrary": 4,
                "chunkMethod": 5,
                "context": 43,
                "methodId": 92,
                "query": 9,
                "retrievalMethod": 36
              },
              "eval_res": {
                "items": [
                  {
                    "Faithfulness": "1.0",
                    "Answer Relevancy": 1,
                    "Context Precision": "To evaluate the context precision, let's first break down the context and the answer provided.\n\n### Identifying Relevant Parts of the Context\n\nThe relevant portion of the context is:\n- \"the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english.\"\n\nThe answer states:\n- \"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n\n### Analysis\n\n1. **Information Density**:\n   - The context does contain information explicitly stating the default checkpoint for the sentiment analysis pipeline, which is essential to answer the question.\n   - However, there is surrounding information in the context that does not pertain directly to the question (e.g., details about the tokenizer, methods from the class, and the note on telemetry). \n\n2. **Information Utilization**:\n   - The specific detail regarding the checkpoint is directly quoted in the answer. The answer used only the relevant part without needing any extraneous information provided in the context.\n   - Thus, the relevant information that was used to construct the answer is present and utilized effectively.\n\n### Calculation of Precision\n- The relevant context used for the answer was a very clear statement of the default checkpoint.\n- The entire context provided is somewhat distracting, with minor details about methods and telemetry that do not directly contribute to the answer.\n\nBased on this:\n- Out of the total retrieved context, only a small fraction (one distinct sentence) provides the required information to answer the question. The rest is irrelevant.\n- Therefore, the information density is low as the rest of the context is largely irrelevant. However, what was relevant was fully utilized.\n\n### Final Rating\nGiven the analysis:\n- Due to the low density of relevant context (only 1 sentence out of multiple sentences being relevant), I would rate the precision as moderate, but with the information used to answer effectively. Thus, I would land on the lower side of moderate precision.\n\nFinal score justification:\n- Information density: Low (due to irrelevant details) \n- Information utilization: High (the relevant information was used effectively).\n\nFinal score: **0.5**. \n\nThus, the output is:\n```\nSCORE: 0.5\n```",
                    "Context Relevancy": 1,
                    "Answer Correctness": "1.0",
                    "Chunk Overlap": true
                  }
                ],
                "dtype": "KeyValue_Mixed"
              }
            },
            {
              "llm": "gpt-4o-mini",
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 22,
                "baseMethod": 3,
                "chunkMethod": 5,
                "docTitle": 18,
                "embeddingModel": 7,
                "method": 10,
                "reference": 21,
                "signature": 14,
                "source_doc": 23
              },
              "prompt": 184,
              "responses": [
                17
              ],
              "uid": "2621bb44-b73e-40af-a553-3768d82cfc9a",
              "vars": {
                "baseMethod": 3,
                "chunkLibrary": 4,
                "chunkMethod": 5,
                "context": 20,
                "docTitle": 18,
                "embeddingModel": 7,
                "methodId": 93,
                "query": 19,
                "retrievalMethod": 10
              },
              "eval_res": {
                "items": [
                  {
                    "Faithfulness": "1.0",
                    "Answer Relevancy": 1,
                    "Context Precision": "0.5",
                    "Context Relevancy": 1,
                    "Answer Correctness": "1.0",
                    "Chunk Overlap": true
                  }
                ],
                "dtype": "KeyValue_Mixed"
              }
            },
            {
              "llm": "gpt-4o-mini",
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 49,
                "baseMethod": 34,
                "chunkMethod": 5,
                "method": 36,
                "reference": 48,
                "signature": 38,
                "source_doc": 50
              },
              "prompt": 185,
              "responses": [
                45
              ],
              "uid": "1846f4c6-4ed0-4505-9e24-002214fe6ed5",
              "vars": {
                "baseMethod": 34,
                "chunkLibrary": 4,
                "chunkMethod": 5,
                "context": 47,
                "methodId": 92,
                "query": 46,
                "retrievalMethod": 36
              },
              "eval_res": {
                "items": [
                  {
                    "Faithfulness": "0.8333",
                    "Answer Relevancy": 1,
                    "Context Precision": "0.5",
                    "Context Relevancy": 1,
                    "Answer Correctness": "0.25",
                    "Chunk Overlap": true
                  }
                ],
                "dtype": "KeyValue_Mixed"
              }
            },
            {
              "llm": "gpt-4o-mini",
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 30,
                "baseMethod": 34,
                "chunkMethod": 5,
                "method": 36,
                "reference": 29,
                "signature": 38,
                "source_doc": 32
              },
              "prompt": 186,
              "responses": [
                40
              ],
              "uid": "41ea9c17-ae54-417a-90b3-eef3320edcf8",
              "vars": {
                "baseMethod": 34,
                "chunkLibrary": 4,
                "chunkMethod": 5,
                "context": 41,
                "methodId": 92,
                "query": 27,
                "retrievalMethod": 36
              },
              "eval_res": {
                "items": [
                  {
                    "Faithfulness": "1.0",
                    "Answer Relevancy": 1,
                    "Context Precision": "1.0",
                    "Context Relevancy": 1,
                    "Answer Correctness": "1.0",
                    "Chunk Overlap": false
                  }
                ],
                "dtype": "KeyValue_Mixed"
              }
            },
            {
              "llm": "gpt-4o-mini",
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 63,
                "baseMethod": 34,
                "chunkMethod": 5,
                "method": 36,
                "reference": 62,
                "signature": 38,
                "source_doc": 64
              },
              "prompt": 187,
              "responses": [
                188
              ],
              "uid": "8442ebb0-fb6f-4b9f-ae0d-3bf0077e460f",
              "vars": {
                "baseMethod": 34,
                "chunkLibrary": 4,
                "chunkMethod": 5,
                "context": 61,
                "methodId": 92,
                "query": 60,
                "retrievalMethod": 36
              },
              "eval_res": {
                "items": [
                  {
                    "Faithfulness": "1.0",
                    "Answer Relevancy": 1,
                    "Context Precision": "0.8",
                    "Context Relevancy": 1,
                    "Answer Correctness": "0.67",
                    "Chunk Overlap": true
                  }
                ],
                "dtype": "KeyValue_Mixed"
              }
            },
            {
              "llm": "gpt-4o-mini",
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 13,
                "baseMethod": 3,
                "chunkMethod": 5,
                "docTitle": 6,
                "embeddingModel": 7,
                "method": 10,
                "reference": 12,
                "signature": 14,
                "source_doc": 15
              },
              "prompt": 189,
              "responses": [
                2
              ],
              "uid": "e5b2b334-688f-48a3-b9ba-22e095010318",
              "vars": {
                "baseMethod": 3,
                "chunkLibrary": 4,
                "chunkMethod": 5,
                "context": 11,
                "docTitle": 6,
                "embeddingModel": 7,
                "methodId": 93,
                "query": 9,
                "retrievalMethod": 10
              },
              "eval_res": {
                "items": [
                  {
                    "Faithfulness": "1.0",
                    "Answer Relevancy": 1,
                    "Context Precision": "0.5",
                    "Context Relevancy": 1,
                    "Answer Correctness": "0.5",
                    "Chunk Overlap": true
                  }
                ],
                "dtype": "KeyValue_Mixed"
              }
            },
            {
              "llm": "gpt-4o-mini",
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 30,
                "baseMethod": 3,
                "chunkMethod": 5,
                "embeddingModel": 7,
                "method": 10,
                "reference": 29,
                "signature": 14,
                "source_doc": 32
              },
              "prompt": 190,
              "responses": [
                191
              ],
              "uid": "b1a55fe8-d756-4029-a44a-6d1a60268943",
              "vars": {
                "baseMethod": 3,
                "chunkLibrary": 4,
                "chunkMethod": 5,
                "context": 69,
                "embeddingModel": 7,
                "methodId": 93,
                "query": 27,
                "retrievalMethod": 10
              },
              "eval_res": {
                "items": [
                  {
                    "Faithfulness": "0.75",
                    "Answer Relevancy": 1,
                    "Context Precision": "0.5",
                    "Context Relevancy": 1,
                    "Answer Correctness": "1.0",
                    "Chunk Overlap": false
                  }
                ],
                "dtype": "KeyValue_Mixed"
              }
            },
            {
              "llm": "gpt-4o-mini",
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 30,
                "baseMethod": 34,
                "chunkMethod": 26,
                "method": 36,
                "reference": 29,
                "signature": 53,
                "source_doc": 32
              },
              "prompt": 192,
              "responses": [
                25
              ],
              "uid": "7fc6868b-42f7-4ed7-8ddb-bb8aa6f0562d",
              "vars": {
                "baseMethod": 34,
                "chunkLibrary": 4,
                "chunkMethod": 26,
                "context": 52,
                "methodId": 92,
                "query": 27,
                "retrievalMethod": 36
              },
              "eval_res": {
                "items": [
                  {
                    "Faithfulness": "1.0",
                    "Answer Relevancy": 1,
                    "Context Precision": "0.5",
                    "Context Relevancy": 1,
                    "Answer Correctness": "0.83",
                    "Chunk Overlap": false
                  }
                ],
                "dtype": "KeyValue_Mixed"
              }
            },
            {
              "llm": "gpt-4o-mini",
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 63,
                "baseMethod": 3,
                "chunkMethod": 5,
                "embeddingModel": 7,
                "method": 10,
                "reference": 62,
                "signature": 14,
                "source_doc": 64
              },
              "prompt": 193,
              "responses": [
                194
              ],
              "uid": "fc04cf70-1464-499a-8a70-99c4077b8ec9",
              "vars": {
                "baseMethod": 3,
                "chunkLibrary": 4,
                "chunkMethod": 5,
                "context": 67,
                "embeddingModel": 7,
                "methodId": 93,
                "query": 60,
                "retrievalMethod": 10
              },
              "eval_res": {
                "items": [
                  {
                    "Faithfulness": "1.0",
                    "Answer Relevancy": 1,
                    "Context Precision": "0.85",
                    "Context Relevancy": 1,
                    "Answer Correctness": "Let's evaluate the generated answer against the reference answer.\n\n### Step 1: Identify factual claims in the generated answer.\n1. The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`.\n2. This method automatically retrieves the relevant pipeline based on the name or path to the pretrained weights.\n3. `from_pipe()` can be used to transfer components from the original pipeline to a new one.\n4. Using `from_pipe()` does not reallocate memory.\n\n### Step 2: Determine support for each claim against the reference answer.\n1. **Claim 1**: The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`.\n   - *Evaluation*: This claim is a direct statement that matches the reference answer. It is fully supported. \n   - **Score**: 1 point.\n   \n2. **Claim 2**: This method automatically retrieves the relevant pipeline based on the name or path to the pretrained weights.\n   - *Evaluation*: While this is a logical extrapolation based on the use of `from_pretrained()`, the specific functionality of automatic retrieval is not included in the reference answer. It suggests additional functionality beyond what is explicitly stated.\n   - **Score**: 0.5 points.\n   \n3. **Claim 3**: `from_pipe()` can be used to transfer components from the original pipeline to a new one.\n   - *Evaluation*: This claim introduces a method (`from_pipe()`) that is not mentioned in the reference answer. Hence, it is not supported.\n   - **Score**: 0 points.\n\n4. **Claim 4**: Using `from_pipe()` does not reallocate memory.\n   - *Evaluation*: This is also an assertion about the `from_pipe()` method that is not mentioned in the reference answer, making it unsupported.\n   - **Score**: 0 points.\n\n### Step 3: Calculate the final score.\nNow, let's total the points:\n- Claim 1: 1 point\n- Claim 2: 0.5 points\n- Claim 3: 0 points\n- Claim 4: 0 points\n\nTotal Score = 1 + 0.5 + 0 + 0 = 1.5 points.\n\nGiven that the maximum possible score is 4 points (1 for each claim), we can convert this into a score out of 1. \n\nFinal score = Total Score / Maximum Possible Score = 1.5 / 4 = 0.375.\n\nHowever, we will consider how to represent this score in the context of possible evaluation outputs which hold more relevance to the accuracy of the claims, primarily focusing on the correctness of the claims versus the reference answer directly.\n\nThus, I would assign a final answer correctness score based on the support for the correct claims.\n\nThe best representation of the final correctness that reflects the outcome accurately would be:\nSCORE: 0.375. \n\nGiven the requirement to fit strictly within a 0-1 score while reflecting the overall correctness appropriately, it should be conjectured or rounded depending on the usage of the evaluative context.\n\n##### Final output:\n```\nSCORE: 0.5\n```",
                    "Chunk Overlap": true
                  }
                ],
                "dtype": "KeyValue_Mixed"
              }
            },
            {
              "llm": "gpt-4o-mini",
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 30,
                "baseMethod": 3,
                "chunkMethod": 26,
                "embeddingModel": 7,
                "method": 10,
                "reference": 29,
                "signature": 31,
                "source_doc": 32
              },
              "prompt": 195,
              "responses": [
                196
              ],
              "uid": "ff70540b-64ca-4b91-969b-1ebc75c328eb",
              "vars": {
                "baseMethod": 3,
                "chunkLibrary": 4,
                "chunkMethod": 26,
                "context": 28,
                "embeddingModel": 7,
                "methodId": 93,
                "query": 27,
                "retrievalMethod": 10
              },
              "eval_res": {
                "items": [
                  {
                    "Faithfulness": "1.0",
                    "Answer Relevancy": 1,
                    "Context Precision": "To evaluate the context precision, let's analyze the context and its relevance to the question:\n\n1. **Identifying Relevant Parts Used in the Answer:**\n   - The context mentions: \"this is the *x86_64 - unknown - linux - musl* binary for `tokenizers`.\"\n   - The answer clearly states that the binary is designed for the \"x86_64 architecture on a Linux system using the musl C library.\"\n\n2. **Calculating Proportion of Relevant Information:**\n   - There are two main points in the relevant context: \n     - It identifies the architecture as \"x86_64\".\n     - It specifies the operating system as \"Linux\" and mentions the musl C library.\n   - The retrieved context primarily contains one clear and direct sentence relevant to the answer regarding the architecture and context for the `tokenizers-linux-x64-musl` binary.\n\n3. **Information Density and Utilization:**\n   - **Information Density:** The portion that contains relevant information is primarily the specific mention of \"x86_64 - linux - musl\", which accounts for a concise piece of relevant data.\n   - **Information Utilization:** The necessary informationâ€”architecture and systemâ€”was effectively utilized to answer the question accurately.\n\nTaking this into account:\n\n- All of the relevant context relates directly to the question at hand. Even though the entire context is longer and includes irrelevant information, the precise information used is highly relevant and necessary.\n- Since the utilized context only contains a single key phrase while the rest is irrelevant, the density is low, but utilization is fully high relative to the relevant information present.\n\nGiven the perfect relevance of the utilized information and its use in answering the question, but recognizing that the context includes additional inefficient text, I would rate the overall precision moderately high but not perfect.\n\nThus, I would assign a score of 0.8.\n\nFinal answer:\n```\nSCORE: 0.8\n```",
                    "Context Relevancy": 1,
                    "Answer Correctness": "0.75",
                    "Chunk Overlap": false
                  }
                ],
                "dtype": "KeyValue_Mixed"
              }
            },
            {
              "llm": "gpt-4o-mini",
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 13,
                "baseMethod": 3,
                "chunkMethod": 26,
                "docTitle": 6,
                "embeddingModel": 7,
                "method": 10,
                "reference": 12,
                "signature": 31,
                "source_doc": 15
              },
              "prompt": 197,
              "responses": [
                2
              ],
              "uid": "a7cfeb0a-d170-4fb2-80da-3e7561fd2e9b",
              "vars": {
                "baseMethod": 3,
                "chunkLibrary": 4,
                "chunkMethod": 26,
                "context": 55,
                "docTitle": 6,
                "embeddingModel": 7,
                "methodId": 93,
                "query": 9,
                "retrievalMethod": 10
              },
              "eval_res": {
                "items": [
                  {
                    "Faithfulness": "1.0",
                    "Answer Relevancy": 1,
                    "Context Precision": "0.7",
                    "Context Relevancy": 1,
                    "Answer Correctness": "0.75",
                    "Chunk Overlap": false
                  }
                ],
                "dtype": "KeyValue_Mixed"
              }
            },
            {
              "llm": "gpt-4o-mini",
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 22,
                "baseMethod": 34,
                "chunkMethod": 26,
                "docTitle": 18,
                "method": 36,
                "reference": 21,
                "signature": 53,
                "source_doc": 23
              },
              "prompt": 198,
              "responses": [
                199
              ],
              "uid": "c1255722-4c83-4cf2-9435-0f930f332397",
              "vars": {
                "baseMethod": 34,
                "chunkLibrary": 4,
                "chunkMethod": 26,
                "context": 76,
                "docTitle": 18,
                "methodId": 92,
                "query": 19,
                "retrievalMethod": 36
              },
              "eval_res": {
                "items": [
                  {
                    "Faithfulness": "Let's evaluate the claims in the provided answer based on the context.\n\n**Claims in the Answer:**\n1. \"In Transformers version 4.18.0, model checkpoints that exceed 10 GB in size are automatically sharded into smaller pieces.\"\n2. \"Each of these smaller pieces will be less than 10 GB.\"\n3. \"You can control the maximum size before sharding using the `max_shard_size` parameter.\"\n\n**Assessment of Claims:**\n1. **Claim 1:** This claim is directly supported by the context, which mentions \"sharded checkpoints since version 4.18.0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces.\" \n   - **Score:** 1 point (fully supported).\n\n2. **Claim 2:** This claim is also stated in the context where it indicates that \"model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces (each of which being of size < 10gb).\"\n   - **Score:** 1 point (fully supported).\n\n3. **Claim 3:** The context confirms this by stating, \"you can control the maximum size before sharding with the `max_shard_size` parameter.\"\n   - **Score:** 1 point (fully supported).\n\n**Total points scored:** 3 points  \n**Total number of claims:** 3\n\nTo calculate the faithfulness score:\n\\[ \\text{Faithfulness Score} = \\frac{\\text{Total Points}}{\\text{Total Number of Claims}} = \\frac{3}{3} = 1.0 \\]\n\nThus, the answer is completely faithful to the context provided, as all claims are fully supported.\n\nFinal output:\n```\nSCORE: 1.0\n```",
                    "Answer Relevancy": 1,
                    "Context Precision": "0.7",
                    "Context Relevancy": 1,
                    "Answer Correctness": "0.17",
                    "Chunk Overlap": false
                  }
                ],
                "dtype": "KeyValue_Mixed"
              }
            },
            {
              "llm": "gpt-4o-mini",
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 13,
                "baseMethod": 34,
                "chunkMethod": 26,
                "method": 36,
                "reference": 12,
                "signature": 53,
                "source_doc": 15
              },
              "prompt": 200,
              "responses": [
                201
              ],
              "uid": "f160b04e-581e-403b-8ab6-0eedd5ba80c1",
              "vars": {
                "baseMethod": 34,
                "chunkLibrary": 4,
                "chunkMethod": 26,
                "context": 57,
                "methodId": 92,
                "query": 9,
                "retrievalMethod": 36
              },
              "eval_res": {
                "items": [
                  {
                    "Faithfulness": "0.75",
                    "Answer Relevancy": 1,
                    "Context Precision": "To evaluate the precision of the retrieved context relative to the question, let's go through the process step by step.\n\n1. **Identify Used Parts of Context**: \n   The answer specifies that the default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst2-english.\" The context explicitly mentions this information when it states: \"here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english.\" This specific sentence directly answers the question.\n\n2. **Calculate Information Density and Utilization**: \n   - The part of the context that contains relevant information used to generate the answer consists of the specific sentence regarding the checkpoint.\n   - The context provided is long and includes a lot of information about tokenization and other aspects of the Transformers library pipeline that do not relate to the question. \n\n   Out of the entire provided context, only a small section (the one identifying the default checkpoint) is directly relevant to answering the question. The remainder of the context provides additional background information that is not necessary for answering the question about the default checkpoint. \n\n   If we evaluate the length:\n   - Relevant portion: about one sentence.\n   - Total context: multiple paragraphs (let's assume 7-10 sentences).\n  \n   Considering this ratio, the relevant information is approximately 1 out of a larger number of sentences.\n\n3. **Information Density & Utilization**: \n   - **Information Density**: Only a small fraction of the text is essential for answering the question. Therefore, the density of relevant information is low.\n   - **Information Utilization**: The relevant sentence was explicitly used in generating the answer, so utilization is high for that specific information.\n\nBased on this analysis:\n- The context includes one crucial piece of relevant information but is surrounded by a lot of irrelevant detail. Hence, I would rate this context's precision as below moderate but not entirely useless.\n\n**Final Score**: Given the low density of relevant information relative to the total context but the complete usage of the relevant detail, I would assign a precision score of 0.3.\n\nFinal answer:\n```\nSCORE: 0.3\n```",
                    "Context Relevancy": 1,
                    "Answer Correctness": "0.75",
                    "Chunk Overlap": false
                  }
                ],
                "dtype": "KeyValue_Mixed"
              }
            },
            {
              "llm": "gpt-4o-mini",
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 22,
                "baseMethod": 3,
                "chunkMethod": 26,
                "docTitle": 18,
                "embeddingModel": 7,
                "method": 10,
                "reference": 21,
                "signature": 31,
                "source_doc": 23
              },
              "prompt": 202,
              "responses": [
                203
              ],
              "uid": "d889ada5-20b4-4f5c-bee7-04ed3d0631f7",
              "vars": {
                "baseMethod": 3,
                "chunkLibrary": 4,
                "chunkMethod": 26,
                "context": 91,
                "docTitle": 18,
                "embeddingModel": 7,
                "methodId": 93,
                "query": 19,
                "retrievalMethod": 10
              },
              "eval_res": {
                "items": [
                  {
                    "Faithfulness": "1.0",
                    "Answer Relevancy": 1,
                    "Context Precision": "0.5",
                    "Context Relevancy": 1,
                    "Answer Correctness": "0.5",
                    "Chunk Overlap": false
                  }
                ],
                "dtype": "KeyValue_Mixed"
              }
            },
            {
              "llm": "gpt-4o-mini",
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 63,
                "baseMethod": 34,
                "chunkMethod": 26,
                "method": 36,
                "reference": 62,
                "signature": 53,
                "source_doc": 64
              },
              "prompt": 204,
              "responses": [
                205
              ],
              "uid": "a002fd40-1f88-45d2-a290-23c4dcf09fc8",
              "vars": {
                "baseMethod": 34,
                "chunkLibrary": 4,
                "chunkMethod": 26,
                "context": 79,
                "methodId": 92,
                "query": 60,
                "retrievalMethod": 36
              },
              "eval_res": {
                "items": [
                  {
                    "Faithfulness": "1.0",
                    "Answer Relevancy": 1,
                    "Context Precision": "0.5",
                    "Context Relevancy": 1,
                    "Answer Correctness": "0.5",
                    "Chunk Overlap": false
                  }
                ],
                "dtype": "KeyValue_Mixed"
              }
            },
            {
              "llm": "gpt-4o-mini",
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 63,
                "baseMethod": 3,
                "chunkMethod": 26,
                "embeddingModel": 7,
                "method": 10,
                "reference": 62,
                "signature": 31,
                "source_doc": 64
              },
              "prompt": 206,
              "responses": [
                207
              ],
              "uid": "4c72e706-4a97-4cbf-ac51-8dd2140e80f9",
              "vars": {
                "baseMethod": 3,
                "chunkLibrary": 4,
                "chunkMethod": 26,
                "context": 88,
                "embeddingModel": 7,
                "methodId": 93,
                "query": 60,
                "retrievalMethod": 10
              },
              "eval_res": {
                "items": [
                  {
                    "Faithfulness": "1.0",
                    "Answer Relevancy": 1,
                    "Context Precision": "0.7",
                    "Context Relevancy": 1,
                    "Answer Correctness": "0.5",
                    "Chunk Overlap": false
                  }
                ],
                "dtype": "KeyValue_Mixed"
              }
            },
            {
              "llm": "gpt-4o-mini",
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 49,
                "baseMethod": 3,
                "chunkMethod": 26,
                "embeddingModel": 7,
                "method": 10,
                "reference": 48,
                "signature": 31,
                "source_doc": 50
              },
              "prompt": 208,
              "responses": [
                97
              ],
              "uid": "4bc1d9ca-4e09-44bd-abff-5cfee5b00dee",
              "vars": {
                "baseMethod": 3,
                "chunkLibrary": 4,
                "chunkMethod": 26,
                "context": 85,
                "embeddingModel": 7,
                "methodId": 93,
                "query": 46,
                "retrievalMethod": 10
              },
              "eval_res": {
                "items": [
                  {
                    "Faithfulness": "0.875",
                    "Answer Relevancy": 1,
                    "Context Precision": "0.5",
                    "Context Relevancy": 1,
                    "Answer Correctness": "0.625",
                    "Chunk Overlap": false
                  }
                ],
                "dtype": "KeyValue_Mixed"
              }
            },
            {
              "llm": "gpt-4o-mini",
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 49,
                "baseMethod": 34,
                "chunkMethod": 26,
                "method": 36,
                "reference": 48,
                "signature": 53,
                "source_doc": 50
              },
              "prompt": 209,
              "responses": [
                210
              ],
              "uid": "2bb32e10-1e24-4b51-9f87-ed1cccbb90aa",
              "vars": {
                "baseMethod": 34,
                "chunkLibrary": 4,
                "chunkMethod": 26,
                "context": 82,
                "methodId": 92,
                "query": 46,
                "retrievalMethod": 36
              },
              "eval_res": {
                "items": [
                  {
                    "Faithfulness": "0.875",
                    "Answer Relevancy": 1,
                    "Context Precision": "1.0",
                    "Context Relevancy": 1,
                    "Answer Correctness": "0.375",
                    "Chunk Overlap": false
                  }
                ],
                "dtype": "KeyValue_Mixed"
              }
            },
            {
              "llm": "gpt-4o-mini",
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 49,
                "baseMethod": 3,
                "chunkMethod": 5,
                "docTitle": 72,
                "embeddingModel": 7,
                "method": 10,
                "reference": 48,
                "signature": 14,
                "source_doc": 50
              },
              "prompt": 211,
              "responses": [
                212
              ],
              "uid": "64bdd041-de19-4e64-a248-368138910621",
              "vars": {
                "baseMethod": 3,
                "chunkLibrary": 4,
                "chunkMethod": 5,
                "context": 73,
                "docTitle": 72,
                "embeddingModel": 7,
                "methodId": 93,
                "query": 46,
                "retrievalMethod": 10
              },
              "eval_res": {
                "items": [
                  {
                    "Faithfulness": "0.83",
                    "Answer Relevancy": 1,
                    "Context Precision": "1.0",
                    "Context Relevancy": 1,
                    "Answer Correctness": "To evaluate the correctness of the generated answer against the reference answer, let's break down the claims made in both texts.\n\n1. **Claim: \"The purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of a set of guidelines.\"**\n   - This phrase fails to align fully with the reference answer, which specifically mentions a \"set of hard-coded harmful concepts\" rather than \"a set of guidelines.\" The specific mention of \"harmful concepts\" is a crucial aspect of the Safety Checker's function, and using the term \"guidelines\" is less specific and potentially misleading.\n   - **Score: 0.5 (Partially supported with some extrapolation).**\n\n2. **Claim: \"It aims to mitigate issues associated with models like Stable Diffusion.\"**\n   - While the reference indicates the purpose is to \"mitigate the risk of generating harmful content,\" this claim about mitigating \"issues associated with models like Stable Diffusion\" is vague and does not directly correspond to the specific goal outlined in the reference answer about harmful content.\n   - **Score: 0 (Not supported or contradicted by reference).**\n\n3. **Claim: \"This contributes to the ethical development and use of the library.\"**\n   - This claim introduces a new concept (ethical development) that is not covered or supported by the reference answer. The original focus was specifically on content safety rather than the broader ethical implications, making this an unsupported addition.\n   - **Score: 0 (Not supported or contradicted by reference).**\n\nSummarizing the assessments leads to the following scores:\n- First claim: 0.5\n- Second claim: 0\n- Third claim: 0\n\nTotal points accumulated: \\(0.5 + 0 + 0 = 0.5\\).\n\nSince all claims were assessed, and given the total score of 0.5 out of a possible 3 points, the correctness score reflects how many claims were supported compared to the potential maximum.\n\nThus, the final answer correctness score is:\n```\nSCORE: 0.17\n```",
                    "Chunk Overlap": true
                  }
                ],
                "dtype": "KeyValue_Mixed"
              }
            }
          ]
        },
        "dragging": false,
        "id": "multieval-7beb361f-660e-4a12-a370-1c2bb8cb0818",
        "position": {
          "x": 464,
          "y": 624
        },
        "positionAbsolute": {
          "x": 464,
          "y": 624
        },
        "selected": false,
        "type": "multieval"
      },
      {
        "width": 334,
        "height": 216,
        "data": {
          "fields": [
            {
              "fill_history": {},
              "metavars": {
                "filename": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt",
                "id": "b740a482-c1e8-4a90-9595-2b16184cb623",
                "size": "1740",
                "type": "text/plain"
              },
              "prompt": "",
              "text": " Gradio Demo: blocks_multiple_event_triggers\r\n\r\n\r\n```\r\n!pip install -q gradio plotly pypistats\r\n```\r\n\r\n\r\n```\r\nimport gradio as gr\r\nimport pypistats\r\nfrom datetime import date\r\nfrom dateutil.relativedelta import relativedelta\r\nimport pandas as pd\r\n\r\ndef get_plot(lib, time):\r\n    data = pypistats.overall(lib, total=True, format=\"pandas\")\r\n    data = data.groupby(\"category\").get_group(\"with_mirrors\").sort_values(\"date\")\r\n    start_date = date.today() - relativedelta(months=int(time.split(\" \")[0]))\r\n    data = data[(data['date'] > str(start_date))]\r\n    data.date = pd.to_datetime(pd.to_datetime(data.date))\r\n    return gr.LinePlot(value=data, x=\"date\", y=\"downloads\",\r\n                              tooltip=['date', 'downloads'],\r\n                              title=f\"Pypi downloads of {lib} over last {time}\",\r\n                              overlay_point=True,\r\n                              height=400,\r\n                              width=900)\r\n\r\n\r\nwith gr.Blocks() as demo:\r\n    gr.Markdown(\r\n        \"\"\"\r\n        ## Pypi Download Stats ðŸ“ˆ\r\n        See live download stats for all of Hugging Face's open-source libraries ðŸ¤—\r\n        \"\"\")\r\n    with gr.Row():\r\n        lib = gr.Dropdown([\"transformers\", \"datasets\", \"huggingface-hub\", \"gradio\", \"accelerate\"],\r\n                          value=\"gradio\", label=\"Library\")\r\n        time = gr.Dropdown([\"3 months\", \"6 months\", \"9 months\", \"12 months\"],\r\n                           value=\"3 months\", label=\"Downloads over the last...\")\r\n\r\n    plt = gr.LinePlot()\r\n    # You can add multiple event triggers in 2 lines like this\r\n    for event in [lib.change, time.change, demo.load]:\r\n        event(get_plot, [lib, time], [plt])\r\n\r\nif __name__ == \"__main__\":\r\n    demo.launch()\r\n\r\n```\r\n"
            },
            {
              "fill_history": {},
              "metavars": {
                "filename": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt",
                "id": "2ac1ca0b-0011-40f3-b681-554f5a94fe0b",
                "size": "2037",
                "type": "text/plain"
              },
              "prompt": "",
              "text": " Contributing a Guide\r\n\r\nWant to help teach Gradio? Consider contributing a Guide! ðŸ¤—\r\n\r\nBroadly speaking, there are two types of guides:\r\n\r\n- **Use cases**: guides that cover step-by-step how to build a particular type of machine learning demo or app using Gradio. Here's an example: [_Creating a Chatbot_](https://github.com/gradio-app/gradio/blob/master/guides/creating_a_chatbot.md)\r\n- **Feature explanation**: guides that describe in detail a particular feature of Gradio. Here's an example: [_Using Flagging_](https://github.com/gradio-app/gradio/blob/master/guides/using_flagging.md)\r\n\r\nWe encourage you to submit either type of Guide! (Looking for ideas? We may also have open [issues](https://github.com/gradio-app/gradio/issues?q=is%3Aopen+is%3Aissue+label%3Aguides) where users have asked for guides on particular topics)\r\n\r\n## Guide Structure\r\n\r\nAs you can see with the previous examples, Guides are standard markdown documents. They usually:\r\n\r\n- start with an Introduction section describing the topic\r\n- include subheadings to make articles easy to navigate\r\n- include real code snippets that make it easy to follow along and implement the Guide\r\n- include embedded Gradio demos to make them more interactive and provide immediate demonstrations of the topic being discussed. These Gradio demos are hosted on [Hugging Face Spaces](https://huggingface.co/spaces) and are embedded using the standard \\<iframe\\> tag.\r\n\r\n## How to Contribute a Guide\r\n\r\n1. Clone or fork this `gradio` repo\r\n2. Add a new markdown document with a descriptive title to the `/guides` folder\r\n3. Write your Guide in standard markdown! Embed Gradio demos wherever helpful\r\n4. Add a list of `related_spaces` at the top of the markdown document (see the previously linked Guides for how to do this)\r\n5. Add 3 `tags` at the top of the markdown document to help users find your guide (again, see the previously linked Guides for how to do this)\r\n6. Open a PR to have your guide reviewed\r\n\r\nThat's it! We're looking forward to reading your Guide ðŸ¥³\r\n"
            },
            {
              "fill_history": {},
              "metavars": {
                "filename": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "904b2c99-2334-47ab-b7c9-6f3734e5e405",
                "size": "13641",
                "type": "text/plain"
              },
              "prompt": "",
              "text": " @gradio/gallery\r\n\r\n## 0.4.14\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`828fb9e`](https://github.com/gradio-app/gradio/commit/828fb9e6ce15b6ea08318675a2361117596a1b5d), [`73268ee`](https://github.com/gradio-app/gradio/commit/73268ee2e39f23ebdd1e927cb49b8d79c4b9a144)]:\r\n  - @gradio/client@0.9.3\r\n  - @gradio/statustracker@0.4.3\r\n  - @gradio/atoms@0.4.1\r\n  - @gradio/image@0.5.3\r\n  - @gradio/upload@0.5.6\r\n\r\n## 0.4.13\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`245d58e`](https://github.com/gradio-app/gradio/commit/245d58eff788e8d44a59d37a2d9b26d0f08a62b4)]:\r\n  - @gradio/client@0.9.2\r\n  - @gradio/image@0.5.2\r\n  - @gradio/upload@0.5.5\r\n\r\n## 0.4.12\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`5d51fbc`](https://github.com/gradio-app/gradio/commit/5d51fbce7826da840a2fd4940feb5d9ad6f1bc5a), [`34f9431`](https://github.com/gradio-app/gradio/commit/34f943101bf7dd6b8a8974a6131c1ed7c4a0dac0)]:\r\n  - @gradio/upload@0.5.4\r\n  - @gradio/client@0.9.1\r\n  - @gradio/image@0.5.1\r\n\r\n## 0.4.11\r\n\r\n### Fixes\r\n\r\n- [#6487](https://github.com/gradio-app/gradio/pull/6487) [`9a5811d`](https://github.com/gradio-app/gradio/commit/9a5811df9218b622af59ba243a937a9c36ba00f9) - Fix the download button of the `gr.Gallery()` component to work. Thanks [@whitphx](https://github.com/whitphx)!\r\n\r\n## 0.4.10\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`b639e04`](https://github.com/gradio-app/gradio/commit/b639e040741e6c0d9104271c81415d7befbd8cf3), [`206af31`](https://github.com/gradio-app/gradio/commit/206af31d7c1a31013364a44e9b40cf8df304ba50)]:\r\n  - @gradio/image@0.4.2\r\n  - @gradio/icons@0.3.1\r\n  - @gradio/atoms@0.3.1\r\n  - @gradio/statustracker@0.4.1\r\n  - @gradio/upload@0.5.2\r\n\r\n## 0.4.9\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`71f1a1f99`](https://github.com/gradio-app/gradio/commit/71f1a1f9931489d465c2c1302a5c8d768a3cd23a)]:\r\n  - @gradio/client@0.8.2\r\n  - @gradio/image@0.4.1\r\n  - @gradio/upload@0.5.1\r\n\r\n## 0.4.8\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`9caddc17b`](https://github.com/gradio-app/gradio/commit/9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8)]:\r\n  - @gradio/atoms@0.3.0\r\n  - @gradio/icons@0.3.0\r\n  - @gradio/image@0.4.0\r\n  - @gradio/statustracker@0.4.0\r\n  - @gradio/upload@0.5.0\r\n\r\n## 0.4.7\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`2f805a7dd`](https://github.com/gradio-app/gradio/commit/2f805a7dd3d2b64b098f659dadd5d01258290521), [`f816136a0`](https://github.com/gradio-app/gradio/commit/f816136a039fa6011be9c4fb14f573e4050a681a)]:\r\n  - @gradio/image@0.3.6\r\n  - @gradio/upload@0.4.2\r\n  - @gradio/atoms@0.2.2\r\n  - @gradio/icons@0.2.1\r\n  - @gradio/statustracker@0.3.2\r\n\r\n## 0.4.6\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`324867f63`](https://github.com/gradio-app/gradio/commit/324867f63c920113d89a565892aa596cf8b1e486)]:\r\n  - @gradio/client@0.8.1\r\n  - @gradio/image@0.3.5\r\n  - @gradio/upload@0.4.1\r\n\r\n## 0.4.5\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`6204ccac5`](https://github.com/gradio-app/gradio/commit/6204ccac5967763e0ebde550d04d12584243a120), [`4d3aad33a`](https://github.com/gradio-app/gradio/commit/4d3aad33a0b66639dbbb2928f305a79fb7789b2d), [`854b482f5`](https://github.com/gradio-app/gradio/commit/854b482f598e0dc47673846631643c079576da9c), [`f1409f95e`](https://github.com/gradio-app/gradio/commit/f1409f95ed39c5565bed6a601e41f94e30196a57)]:\r\n  - @gradio/image@0.3.4\r\n  - @gradio/upload@0.4.0\r\n  - @gradio/client@0.8.0\r\n\r\n## 0.4.4\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`bca6c2c80`](https://github.com/gradio-app/gradio/commit/bca6c2c80f7e5062427019de45c282238388af95), [`3cdeabc68`](https://github.com/gradio-app/gradio/commit/3cdeabc6843000310e1a9e1d17190ecbf3bbc780), [`fad92c29d`](https://github.com/gradio-app/gradio/commit/fad92c29dc1f5cd84341aae417c495b33e01245f)]:\r\n  - @gradio/client@0.7.2\r\n  - @gradio/atoms@0.2.1\r\n  - @gradio/upload@0.3.3\r\n  - @gradio/image@0.3.3\r\n  - @gradio/statustracker@0.3.1\r\n\r\n## 0.4.3\r\n\r\n### Fixes\r\n\r\n- [#6288](https://github.com/gradio-app/gradio/pull/6288) [`92278729e`](https://github.com/gradio-app/gradio/commit/92278729ee008126af15ffe6be399236211b2f34) - Gallery preview fix and optionally skip download of urls in postprcess. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\r\n\r\n## 0.4.2\r\n\r\n### Fixes\r\n\r\n- [#6277](https://github.com/gradio-app/gradio/pull/6277) [`5fe091367`](https://github.com/gradio-app/gradio/commit/5fe091367fbe0eecdd504aa734ca1c70b0621f52) - handle selected_index prop change for gallery. Thanks [@pngwn](https://github.com/pngwn)!\r\n\r\n## 0.4.1\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`2ba14b284`](https://github.com/gradio-app/gradio/commit/2ba14b284f908aa13859f4337167a157075a68eb)]:\r\n  - @gradio/client@0.7.1\r\n  - @gradio/image@0.3.1\r\n  - @gradio/upload@0.3.1\r\n\r\n## 0.4.0\r\n\r\n### Features\r\n\r\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - fix circular dependency with client + upload. Thanks [@pngwn](https://github.com/pngwn)!\r\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Clean root url. Thanks [@pngwn](https://github.com/pngwn)!\r\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Image v4. Thanks [@pngwn](https://github.com/pngwn)!\r\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Publish all components to npm. Thanks [@pngwn](https://github.com/pngwn)!\r\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Custom components. Thanks [@pngwn](https://github.com/pngwn)!\r\n\r\n## 0.4.0-beta.9\r\n\r\n### Features\r\n\r\n- [#6143](https://github.com/gradio-app/gradio/pull/6143) [`e4f7b4b40`](https://github.com/gradio-app/gradio/commit/e4f7b4b409323b01aa01b39e15ce6139e29aa073) - fix circular dependency with client + upload. Thanks [@pngwn](https://github.com/pngwn)!\r\n- [#6136](https://github.com/gradio-app/gradio/pull/6136) [`667802a6c`](https://github.com/gradio-app/gradio/commit/667802a6cdbfb2ce454a3be5a78e0990b194548a) - JS Component Documentation. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\r\n- [#6094](https://github.com/gradio-app/gradio/pull/6094) [`c476bd5a5`](https://github.com/gradio-app/gradio/commit/c476bd5a5b70836163b9c69bf4bfe068b17fbe13) - Image v4. Thanks [@pngwn](https://github.com/pngwn)!\r\n\r\n## 0.4.0-beta.8\r\n\r\n### Features\r\n\r\n- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676`](https://github.com/gradio-app/gradio/commit/83e947676d327ca2ab6ae2a2d710c78961c771a0) - Format js in v4 branch. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\r\n\r\n### Fixes\r\n\r\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)!\r\n\r\n## 0.4.0-beta.7\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies []:\r\n  - @gradio/image@0.3.0-beta.7\r\n\r\n## 0.4.0-beta.6\r\n\r\n### Features\r\n\r\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f`](https://github.com/gradio-app/gradio/commit/319c30f3fccf23bfe1da6c9b132a6a99d59652f7) - rererefactor frontend files. Thanks [@pngwn](https://github.com/pngwn)!\r\n- [#5938](https://github.com/gradio-app/gradio/pull/5938) [`13ed8a485`](https://github.com/gradio-app/gradio/commit/13ed8a485d5e31d7d75af87fe8654b661edcca93) - V4: Use beta release versions for '@gradio' packages. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\r\n\r\n## 0.5.1\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`b67115e8e`](https://github.com/gradio-app/gradio/commit/b67115e8e6e489fffd5271ea830211863241ddc5), [`8d909624f`](https://github.com/gradio-app/gradio/commit/8d909624f61a49536e3c0f71cb2d9efe91216219), [`e70805d54`](https://github.com/gradio-app/gradio/commit/e70805d54cc792452545f5d8eccc1aa0212a4695)]:\r\n  - @gradio/image@0.4.0\r\n  - @gradio/atoms@0.2.0\r\n  - @gradio/statustracker@0.2.3\r\n  - @gradio/upload@0.3.3\r\n\r\n## 0.5.0\r\n\r\n### Features\r\n\r\n- [#5780](https://github.com/gradio-app/gradio/pull/5780) [`ed0f9a21b`](https://github.com/gradio-app/gradio/commit/ed0f9a21b04ad6b941b63d2ce45100dbd1abd5c5) - Adds `change()` event to `gr.Gallery`. Thanks [@abidlabs](https://github.com/abidlabs)!\r\n- [#5783](https://github.com/gradio-app/gradio/pull/5783) [`4567788bd`](https://github.com/gradio-app/gradio/commit/4567788bd1fc25df9322902ba748012e392b520a) - Adds the ability to set the `selected_index` in a `gr.Gallery`. Thanks [@abidlabs](https://github.com/abidlabs)!\r\n\r\n### Fixes\r\n\r\n- [#5798](https://github.com/gradio-app/gradio/pull/5798) [`a0d3cc45c`](https://github.com/gradio-app/gradio/commit/a0d3cc45c6db48dc0db423c229b8fb285623cdc4) - Fix `gr.SelectData` so that the target attribute is correctly attached, and the filedata is included in the data attribute with `gr.Gallery`. Thanks [@abidlabs](https://github.com/abidlabs)!\r\n\r\n## 0.4.1\r\n\r\n### Fixes\r\n\r\n- [#5735](https://github.com/gradio-app/gradio/pull/5735) [`abb5e9df4`](https://github.com/gradio-app/gradio/commit/abb5e9df47989b2c56c2c312d74944678f9f2d4e) - Ensure images with no caption download in gallery. Thanks [@hannahblair](https://github.com/hannahblair)!\r\n- [#5754](https://github.com/gradio-app/gradio/pull/5754) [`502054848`](https://github.com/gradio-app/gradio/commit/502054848fdbe39fc03ec42445242b4e49b7affc) - Fix Gallery `columns` and `rows` params. Thanks [@abidlabs](https://github.com/abidlabs)!\r\n\r\n## 0.4.0\r\n\r\n### Features\r\n\r\n- [#5554](https://github.com/gradio-app/gradio/pull/5554) [`75ddeb390`](https://github.com/gradio-app/gradio/commit/75ddeb390d665d4484667390a97442081b49a423) - Accessibility Improvements. Thanks [@hannahblair](https://github.com/hannahblair)!\r\n\r\n## 0.3.4\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`e0d61b8ba`](https://github.com/gradio-app/gradio/commit/e0d61b8baa0f6293f53b9bdb1647d42f9ae2583a)]:\r\n  - @gradio/image@0.2.4\r\n\r\n## 0.3.3\r\n\r\n### Fixes\r\n\r\n- [#5528](https://github.com/gradio-app/gradio/pull/5528) [`dc86e4a7`](https://github.com/gradio-app/gradio/commit/dc86e4a7e1c40b910c74558e6f88fddf9b3292bc) - Lazy load all images. Thanks [@aliabid94](https://github.com/aliabid94)!\r\n\r\n## 0.3.2\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`afac0006`](https://github.com/gradio-app/gradio/commit/afac0006337ce2840cf497cd65691f2f60ee5912)]:\r\n  - @gradio/statustracker@0.2.0\r\n  - @gradio/image@0.2.2\r\n  - @gradio/utils@0.1.1\r\n  - @gradio/atoms@0.1.2\r\n  - @gradio/upload@0.2.1\r\n\r\n## 0.3.1\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`abf1c57d`](https://github.com/gradio-app/gradio/commit/abf1c57d7d85de0df233ee3b38aeb38b638477db), [`79d8f9d8`](https://github.com/gradio-app/gradio/commit/79d8f9d891901683c5a1b7486efb44eab2478c96)]:\r\n  - @gradio/icons@0.1.0\r\n  - @gradio/utils@0.1.0\r\n  - @gradio/upload@0.2.0\r\n  - @gradio/atoms@0.1.1\r\n  - @gradio/image@0.2.1\r\n  - @gradio/statustracker@0.1.1\r\n\r\n## 0.3.0\r\n\r\n### Highlights\r\n\r\n#### Improve startup performance and markdown support ([#5279](https://github.com/gradio-app/gradio/pull/5279) [`fe057300`](https://github.com/gradio-app/gradio/commit/fe057300f0672c62dab9d9b4501054ac5d45a4ec))\r\n\r\n##### Improved markdown support\r\n\r\nWe now have better support for markdown in `gr.Markdown` and `gr.Dataframe`. Including syntax highlighting and Github Flavoured Markdown. We also have more consistent markdown behaviour and styling.\r\n\r\n##### Various performance improvements\r\n\r\nThese improvements will be particularly beneficial to large applications.\r\n\r\n- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.\r\n- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).\r\n- Corrected an issue that was causing markdown to re-render infinitely.\r\n- Ensured that the `gr.3DModel` does re-render prematurely.\r\n\r\nThanks [@pngwn](https://github.com/pngwn)!\r\n\r\n### Features\r\n\r\n- [#5215](https://github.com/gradio-app/gradio/pull/5215) [`fbdad78a`](https://github.com/gradio-app/gradio/commit/fbdad78af4c47454cbb570f88cc14bf4479bbceb) - Lazy load interactive or static variants of a component individually, rather than loading both variants regardless. This change will improve performance for many applications. Thanks [@pngwn](https://github.com/pngwn)!\r\n- [#5216](https://github.com/gradio-app/gradio/pull/5216) [`4b58ea6d`](https://github.com/gradio-app/gradio/commit/4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8) - Update i18n tokens and locale files. Thanks [@hannahblair](https://github.com/hannahblair)!\r\n\r\n## 0.2.0\r\n\r\n### Features\r\n\r\n- [#5025](https://github.com/gradio-app/gradio/pull/5025) [`6693660a`](https://github.com/gradio-app/gradio/commit/6693660a790996f8f481feaf22a8c49130d52d89) - Add download button to selected images in `Gallery`. Thanks [@hannahblair](https://github.com/hannahblair)!\r\n\r\n## 0.1.0\r\n\r\n### Features\r\n\r\n- [#4995](https://github.com/gradio-app/gradio/pull/4995) [`3f8c210b`](https://github.com/gradio-app/gradio/commit/3f8c210b01ef1ceaaf8ee73be4bf246b5b745bbf) - Implement left and right click in `Gallery` component and show implicit images in `Gallery` grid. Thanks [@hannahblair](https://github.com/hannahblair)!\r\n"
            },
            {
              "fill_history": {},
              "metavars": {
                "filename": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "905de1be-07be-4467-ab4c-60fae840c6e6",
                "size": "4362",
                "type": "text/plain"
              },
              "prompt": "",
              "text": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs."
            },
            {
              "fill_history": {},
              "metavars": {
                "filename": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "id": "31875f02-4d4a-4f60-8069-b24d70795c99",
                "size": "2398",
                "type": "text/plain"
              },
              "prompt": "",
              "text": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\r\nthe License. You may obtain a copy of the License at\r\n\r\nhttp://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\r\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\r\nspecific language governing permissions and limitations under the License.\r\n-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\r\n2. use multiple pipelines in your workflow\r\n\r\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\r\n\r\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\r\n\r\n```py\r\nfrom diffusers import AutoPipelineForText2Image\r\nimport torch\r\n\r\npipeline = AutoPipelineForText2Image.from_pretrained(\r\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\r\n).to(\"cuda\")\r\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\r\n\r\nimage = pipeline(prompt, num_inference_steps=25).images[0]\r\n```\r\n\r\n<Tip>\r\n\r\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\r\n\r\n</Tip>\r\n\r\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\r\n\r\n- [Stable Diffusion](./stable_diffusion/overview)\r\n- [ControlNet](./controlnet)\r\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\r\n- [DeepFloyd IF](./deepfloyd_if)\r\n- [Kandinsky 2.1](./kandinsky)\r\n- [Kandinsky 2.2](./kandinsky_v22)\r\n\r\n\r\n## AutoPipelineForText2Image\r\n\r\n[[autodoc]] AutoPipelineForText2Image\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe\r\n\r\n## AutoPipelineForImage2Image\r\n\r\n[[autodoc]] AutoPipelineForImage2Image\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe\r\n\r\n## AutoPipelineForInpainting\r\n\r\n[[autodoc]] AutoPipelineForInpainting\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe\r\n"
            },
            {
              "fill_history": {},
              "metavars": {
                "filename": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "4be240a6-1797-4617-b2b7-3544c273c2f4",
                "size": "5262",
                "type": "text/plain"
              },
              "prompt": "",
              "text": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\r\nthe License. You may obtain a copy of the License at\r\n\r\nhttp://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\r\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\r\nspecific language governing permissions and limitations under the License.\r\n-->\r\n\r\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\r\n\r\n## Preamble\r\n\r\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\r\n\r\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\r\n\r\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\r\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\r\n\r\n\r\n## Scope\r\n\r\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\r\n\r\n\r\n## Ethical guidelines\r\n\r\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\r\n\r\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\r\n\r\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\r\n\r\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\r\n\r\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\r\n\r\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\r\n\r\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\r\n\r\n\r\n## Examples of implementations: Safety features and Mechanisms\r\n\r\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\r\n\r\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\r\n\r\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\r\n\r\n- **Encouraging safety in deployment**\r\n\r\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\r\n\r\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\r\n\r\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\r\n\r\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\r\n"
            },
            {
              "fill_history": {},
              "metavars": {
                "filename": "huggingface_diffusers_blob_main_src_diffusers_experimental_README.md.txt",
                "id": "8c572804-dd34-4a89-a896-bb874ed5a6ca",
                "size": "290",
                "type": "text/plain"
              },
              "prompt": "",
              "text": " ðŸ§¨ Diffusers Experimental\r\n\r\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\r\nCurrently, the following experiments are supported:\r\n* Reinforcement learning via an implementation of the [Diffuser](https://arxiv.org/abs/2205.09991) model."
            },
            {
              "fill_history": {},
              "metavars": {
                "filename": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "ee359072-5a6d-45c3-b8b4-66ef463b848a",
                "size": "9975",
                "type": "text/plain"
              },
              "prompt": "",
              "text": "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\r\nrendered properly in your Markdown viewer.\r\n-->\r\n\r\n# Environment variables\r\n\r\n`huggingface_hub` can be configured using environment variables.\r\n\r\nIf you are unfamiliar with environment variable, here are generic articles about them\r\n[on macOS and Linux](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/)\r\nand on [Windows](https://phoenixnap.com/kb/windows-set-environment-variable).\r\n\r\nThis page will guide you through all environment variables specific to `huggingface_hub`\r\nand their meaning.\r\n\r\n## Generic\r\n\r\n### HF_INFERENCE_ENDPOINT\r\n\r\nTo configure the inference api base url. You might want to set this variable if your organization\r\nis pointing at an API Gateway rather than directly at the inference api.\r\n\r\nDefaults to `\"https://api-inference.huggingface.com\"`.\r\n\r\n### HF_HOME\r\n\r\nTo configure where `huggingface_hub` will locally store data. In particular, your token\r\nand the cache will be stored in this folder.\r\n\r\nDefaults to `\"~/.cache/huggingface\"` unless [XDG_CACHE_HOME](#xdgcachehome) is set.\r\n\r\n### HF_HUB_CACHE\r\n\r\nTo configure where repositories from the Hub will be cached locally (models, datasets and\r\nspaces).\r\n\r\nDefaults to `\"$HF_HOME/hub\"` (e.g. `\"~/.cache/huggingface/hub\"` by default).\r\n\r\n### HF_ASSETS_CACHE\r\n\r\nTo configure where [assets](../guides/manage-cache#caching-assets) created by downstream libraries\r\nwill be cached locally. Those assets can be preprocessed data, files downloaded from GitHub,\r\nlogs,...\r\n\r\nDefaults to `\"$HF_HOME/assets\"` (e.g. `\"~/.cache/huggingface/assets\"` by default).\r\n\r\n### HF_TOKEN\r\n\r\nTo configure the User Access Token to authenticate to the Hub. If set, this value will\r\noverwrite the token stored on the machine (in `\"$HF_HOME/token\"`).\r\n\r\nFor more details about authentication, check out [this section](../quick-start#authentication).\r\n\r\n### HF_HUB_VERBOSITY\r\n\r\nSet the verbosity level of the `huggingface_hub`'s logger. Must be one of\r\n`{\"debug\", \"info\", \"warning\", \"error\", \"critical\"}`.\r\n\r\nDefaults to `\"warning\"`.\r\n\r\nFor more details, see [logging reference](../package_reference/utilities#huggingface_hub.utils.logging.get_verbosity).\r\n\r\n### HF_HUB_LOCAL_DIR_AUTO_SYMLINK_THRESHOLD\r\n\r\nInteger value to define under which size a file is considered as \"small\". When downloading files to a local directory,\r\nsmall files will be duplicated to ease user experience while bigger files are symlinked to save disk usage.\r\n\r\nFor more details, see the [download guide](../guides/download#download-files-to-local-folder).\r\n\r\n### HF_HUB_ETAG_TIMEOUT\r\n\r\nInteger value to define the number of seconds to wait for server response when fetching the latest metadata from a repo before downloading a file. If the request times out, `huggingface_hub` will default to the locally cached files. Setting a lower value speeds up the workflow for machines with a slow connection that have already cached files. A higher value guarantees the metadata call to succeed in more cases. Default to 10s.\r\n\r\n### HF_HUB_DOWNLOAD_TIMEOUT\r\n\r\nInteger value to define the number of seconds to wait for server response when downloading a file. If the request times out, a TimeoutError is raised. Setting a higher value is beneficial on machine with a slow connection. A smaller value makes the process fail quicker in case of complete network outage. Default to 10s.\r\n\r\n## Boolean values\r\n\r\nThe following environment variables expect a boolean value. The variable will be considered\r\nas `True` if its value is one of `{\"1\", \"ON\", \"YES\", \"TRUE\"}` (case-insensitive). Any other value\r\n(or undefined) will be considered as `False`.\r\n\r\n### HF_HUB_OFFLINE\r\n\r\nIf set, no HTTP calls will me made to the Hugging Face Hub. If you try to download files, only the cached files will be accessed. If no cache file is detected, an error is raised This is useful in case your network is slow and you don't care about having the latest version of a file.\r\n\r\nIf `HF_HUB_OFFLINE=1` is set as environment variable and you call any method of [`HfApi`], an [`~huggingface_hub.utils.OfflineModeIsEnabled`] exception will be raised.\r\n\r\n**Note:** even if the latest version of a file is cached, calling `hf_hub_download` still triggers a HTTP request to check that a new version is not available. Setting `HF_HUB_OFFLINE=1` will skip this call which speeds up your loading time.\r\n\r\n### HF_HUB_DISABLE_IMPLICIT_TOKEN\r\n\r\nAuthentication is not mandatory for every requests to the Hub. For instance, requesting\r\ndetails about `\"gpt2\"` model does not require to be authenticated. However, if a user is\r\n[logged in](../package_reference/login), the default behavior will be to always send the token\r\nin order to ease user experience (never get a HTTP 401 Unauthorized) when accessing private or gated repositories. For privacy, you can\r\ndisable this behavior by setting `HF_HUB_DISABLE_IMPLICIT_TOKEN=1`. In this case,\r\nthe token will be sent only for \"write-access\" calls (example: create a commit).\r\n\r\n**Note:** disabling implicit sending of token can have weird side effects. For example,\r\nif you want to list all models on the Hub, your private models will not be listed. You\r\nwould need to explicitly pass `token=True` argument in your script.\r\n\r\n### HF_HUB_DISABLE_PROGRESS_BARS\r\n\r\nFor time consuming tasks, `huggingface_hub` displays a progress bar by default (using tqdm).\r\nYou can disable all the progress bars at once by setting `HF_HUB_DISABLE_PROGRESS_BARS=1`.\r\n\r\n### HF_HUB_DISABLE_SYMLINKS_WARNING\r\n\r\nIf you are on a Windows machine, it is recommended to enable the developer mode or to run\r\n`huggingface_hub` in admin mode. If not, `huggingface_hub` will not be able to create\r\nsymlinks in your cache system. You will be able to execute any script but your user experience\r\nwill be degraded as some huge files might end-up duplicated on your hard-drive. A warning\r\nmessage is triggered to warn you about this behavior. Set `HF_HUB_DISABLE_SYMLINKS_WARNING=1`,\r\nto disable this warning.\r\n\r\nFor more details, see [cache limitations](../guides/manage-cache#limitations).\r\n\r\n### HF_HUB_DISABLE_EXPERIMENTAL_WARNING\r\n\r\nSome features of `huggingface_hub` are experimental. This means you can use them but we do not guarantee they will be\r\nmaintained in the future. In particular, we might update the API or behavior of such features without any deprecation\r\ncycle. A warning message is triggered when using an experimental feature to warn you about it. If you're comfortable debugging any potential issues using an experimental feature, you can set `HF_HUB_DISABLE_EXPERIMENTAL_WARNING=1` to disable the warning.\r\n\r\nIf you are using an experimental feature, please let us know! Your feedback can help us design and improve it.\r\n\r\n### HF_HUB_DISABLE_TELEMETRY\r\n\r\nBy default, some data is collected by HF libraries (`transformers`, `datasets`, `gradio`,..) to monitor usage, debug issues and help prioritize features.\r\nEach library defines its own policy (i.e. which usage to monitor) but the core implementation happens in `huggingface_hub` (see [`send_telemetry`]).\r\n\r\nYou can set `HF_HUB_DISABLE_TELEMETRY=1` as environment variable to globally disable telemetry.\r\n\r\n### HF_HUB_ENABLE_HF_TRANSFER\r\n\r\nSet to `True` for faster uploads and downloads from the Hub using `hf_transfer`.\r\n\r\nBy default, `huggingface_hub` uses the Python-based `requests.get` and `requests.post` functions. Although these are reliable and versatile, they may not be the most efficient choice for machines with high bandwidth. [`hf_transfer`](https://github.com/huggingface/hf_transfer) is a Rust-based package developed to maximize the bandwidth used by dividing large files into smaller parts and transferring them simultaneously using multiple threads. This approach can potentially double the transfer speed. To use `hf_transfer`, you need to install it separately [from PyPI](https://pypi.org/project/hf-transfer/) and set `HF_HUB_ENABLE_HF_TRANSFER=1` as an environment variable.\r\n\r\nPlease note that using `hf_transfer` comes with certain limitations. Since it is not purely Python-based, debugging errors may be challenging. Additionally, `hf_transfer` lacks several user-friendly features such as resumable downloads and proxies. These omissions are intentional to maintain the simplicity and speed of the Rust logic. Consequently, `hf_transfer` is not enabled by default in `huggingface_hub`.\r\n\r\n## Deprecated environment variables\r\n\r\nIn order to standardize all environment variables within the Hugging Face ecosystem, some variables have been marked as deprecated. Although they remain functional, they no longer take precedence over their replacements. The following table outlines the deprecated variables and their corresponding alternatives:\r\n\r\n\r\n| Deprecated Variable | Replacement |\r\n| --- | --- |\r\n| `HUGGINGFACE_HUB_CACHE` | `HF_HUB_CACHE` |\r\n| `HUGGINGFACE_ASSETS_CACHE` | `HF_ASSETS_CACHE` |\r\n| `HUGGING_FACE_HUB_TOKEN` | `HF_TOKEN` |\r\n| `HUGGINGFACE_HUB_VERBOSITY` | `HF_HUB_VERBOSITY` |\r\n\r\n## From external tools\r\n\r\nSome environment variables are not specific to `huggingface_hub` but are still taken into account when they are set.\r\n\r\n### DO_NOT_TRACK\r\n\r\nBoolean value. Equivalent to `HF_HUB_DISABLE_TELEMETRY`. When set to true, telemetry is globally disabled in the Hugging Face Python ecosystem (`transformers`, `diffusers`, `gradio`, etc.). See https://consoledonottrack.com/ for more details.\r\n\r\n### NO_COLOR\r\n\r\nBoolean value. When set, `huggingface-cli` tool will not print any ANSI color.\r\nSee [no-color.org](https://no-color.org/).\r\n\r\n### XDG_CACHE_HOME\r\n\r\nUsed only when `HF_HOME` is not set!\r\n\r\nThis is the default way to configure where [user-specific non-essential (cached) data should be written](https://wiki.archlinux.org/title/XDG_Base_Directory)\r\non linux machines. \r\n\r\nIf `HF_HOME` is not set, the default home will be `\"$XDG_CACHE_HOME/huggingface\"` instead\r\nof `\"~/.cache/huggingface\"`.\r\n"
            },
            {
              "fill_history": {},
              "metavars": {
                "filename": "huggingface_tokenizers_blob_main_bindings_node_npm_linux-x64-musl_README.md.txt",
                "id": "b21458d5-2e88-4b77-b7b6-7cd22215edb2",
                "size": "99",
                "type": "text/plain"
              },
              "prompt": "",
              "text": " `tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\r\n"
            },
            {
              "fill_history": {},
              "metavars": {
                "filename": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "945bf623-e3a7-4e05-bf48-66eb169db6f8",
                "size": "6187",
                "type": "text/plain"
              },
              "prompt": "",
              "text": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\r\nthe License. You may obtain a copy of the License at\r\n\r\nhttp://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\r\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\r\nspecific language governing permissions and limitations under the License.\r\n\r\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\r\nrendered properly in your Markdown viewer.\r\n\r\n-->\r\n\r\n# Instantiating a big model\r\n\r\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\r\nfrom PyTorch is:\r\n\r\n1. Create your model with random weights.\r\n2. Load your pretrained weights.\r\n3. Put those pretrained weights in your random model.\r\n\r\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\r\n\r\n<Tip>\r\n\r\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \r\n\r\n</Tip>\r\n\r\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\r\n\r\n## Sharded checkpoints\r\n\r\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\r\n\r\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\r\n\r\n```py\r\nfrom transformers import AutoModel\r\n\r\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\r\n```\r\n\r\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\r\n\r\n```py\r\n>>> import os\r\n>>> import tempfile\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir)\r\n...     print(sorted(os.listdir(tmp_dir)))\r\n['config.json', 'pytorch_model.bin']\r\n```\r\n\r\nNow let's use a maximum shard size of 200MB:\r\n\r\n```py\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\r\n...     print(sorted(os.listdir(tmp_dir)))\r\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\r\n```\r\n\r\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\r\n\r\n```py\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\r\n...     new_model = AutoModel.from_pretrained(tmp_dir)\r\n```\r\n\r\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\r\n\r\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\r\n\r\n```py\r\n>>> import json\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\r\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\r\n...         index = json.load(f)\r\n\r\n>>> print(index.keys())\r\ndict_keys(['metadata', 'weight_map'])\r\n```\r\n\r\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\r\n\r\n```py\r\n>>> index[\"metadata\"]\r\n{'total_size': 433245184}\r\n```\r\n\r\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\r\n\r\n```py\r\n>>> index[\"weight_map\"]\r\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\r\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\r\n ...\r\n```\r\n\r\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\r\n\r\n```py\r\n>>> from transformers.modeling_utils import load_sharded_checkpoint\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\r\n...     load_sharded_checkpoint(model, tmp_dir)\r\n```\r\n\r\n## Low memory loading\r\n\r\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\r\n\r\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\r\n"
            }
          ],
          "output": [
            {
              "fill_history": {},
              "metavars": {
                "filename": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt",
                "id": "b740a482-c1e8-4a90-9595-2b16184cb623",
                "size": "1740",
                "type": "text/plain"
              },
              "prompt": "",
              "text": " Gradio Demo: blocks_multiple_event_triggers\r\n\r\n\r\n```\r\n!pip install -q gradio plotly pypistats\r\n```\r\n\r\n\r\n```\r\nimport gradio as gr\r\nimport pypistats\r\nfrom datetime import date\r\nfrom dateutil.relativedelta import relativedelta\r\nimport pandas as pd\r\n\r\ndef get_plot(lib, time):\r\n    data = pypistats.overall(lib, total=True, format=\"pandas\")\r\n    data = data.groupby(\"category\").get_group(\"with_mirrors\").sort_values(\"date\")\r\n    start_date = date.today() - relativedelta(months=int(time.split(\" \")[0]))\r\n    data = data[(data['date'] > str(start_date))]\r\n    data.date = pd.to_datetime(pd.to_datetime(data.date))\r\n    return gr.LinePlot(value=data, x=\"date\", y=\"downloads\",\r\n                              tooltip=['date', 'downloads'],\r\n                              title=f\"Pypi downloads of {lib} over last {time}\",\r\n                              overlay_point=True,\r\n                              height=400,\r\n                              width=900)\r\n\r\n\r\nwith gr.Blocks() as demo:\r\n    gr.Markdown(\r\n        \"\"\"\r\n        ## Pypi Download Stats ðŸ“ˆ\r\n        See live download stats for all of Hugging Face's open-source libraries ðŸ¤—\r\n        \"\"\")\r\n    with gr.Row():\r\n        lib = gr.Dropdown([\"transformers\", \"datasets\", \"huggingface-hub\", \"gradio\", \"accelerate\"],\r\n                          value=\"gradio\", label=\"Library\")\r\n        time = gr.Dropdown([\"3 months\", \"6 months\", \"9 months\", \"12 months\"],\r\n                           value=\"3 months\", label=\"Downloads over the last...\")\r\n\r\n    plt = gr.LinePlot()\r\n    # You can add multiple event triggers in 2 lines like this\r\n    for event in [lib.change, time.change, demo.load]:\r\n        event(get_plot, [lib, time], [plt])\r\n\r\nif __name__ == \"__main__\":\r\n    demo.launch()\r\n\r\n```\r\n"
            },
            {
              "fill_history": {},
              "metavars": {
                "filename": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt",
                "id": "2ac1ca0b-0011-40f3-b681-554f5a94fe0b",
                "size": "2037",
                "type": "text/plain"
              },
              "prompt": "",
              "text": " Contributing a Guide\r\n\r\nWant to help teach Gradio? Consider contributing a Guide! ðŸ¤—\r\n\r\nBroadly speaking, there are two types of guides:\r\n\r\n- **Use cases**: guides that cover step-by-step how to build a particular type of machine learning demo or app using Gradio. Here's an example: [_Creating a Chatbot_](https://github.com/gradio-app/gradio/blob/master/guides/creating_a_chatbot.md)\r\n- **Feature explanation**: guides that describe in detail a particular feature of Gradio. Here's an example: [_Using Flagging_](https://github.com/gradio-app/gradio/blob/master/guides/using_flagging.md)\r\n\r\nWe encourage you to submit either type of Guide! (Looking for ideas? We may also have open [issues](https://github.com/gradio-app/gradio/issues?q=is%3Aopen+is%3Aissue+label%3Aguides) where users have asked for guides on particular topics)\r\n\r\n## Guide Structure\r\n\r\nAs you can see with the previous examples, Guides are standard markdown documents. They usually:\r\n\r\n- start with an Introduction section describing the topic\r\n- include subheadings to make articles easy to navigate\r\n- include real code snippets that make it easy to follow along and implement the Guide\r\n- include embedded Gradio demos to make them more interactive and provide immediate demonstrations of the topic being discussed. These Gradio demos are hosted on [Hugging Face Spaces](https://huggingface.co/spaces) and are embedded using the standard \\<iframe\\> tag.\r\n\r\n## How to Contribute a Guide\r\n\r\n1. Clone or fork this `gradio` repo\r\n2. Add a new markdown document with a descriptive title to the `/guides` folder\r\n3. Write your Guide in standard markdown! Embed Gradio demos wherever helpful\r\n4. Add a list of `related_spaces` at the top of the markdown document (see the previously linked Guides for how to do this)\r\n5. Add 3 `tags` at the top of the markdown document to help users find your guide (again, see the previously linked Guides for how to do this)\r\n6. Open a PR to have your guide reviewed\r\n\r\nThat's it! We're looking forward to reading your Guide ðŸ¥³\r\n"
            },
            {
              "fill_history": {},
              "metavars": {
                "filename": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "904b2c99-2334-47ab-b7c9-6f3734e5e405",
                "size": "13641",
                "type": "text/plain"
              },
              "prompt": "",
              "text": " @gradio/gallery\r\n\r\n## 0.4.14\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`828fb9e`](https://github.com/gradio-app/gradio/commit/828fb9e6ce15b6ea08318675a2361117596a1b5d), [`73268ee`](https://github.com/gradio-app/gradio/commit/73268ee2e39f23ebdd1e927cb49b8d79c4b9a144)]:\r\n  - @gradio/client@0.9.3\r\n  - @gradio/statustracker@0.4.3\r\n  - @gradio/atoms@0.4.1\r\n  - @gradio/image@0.5.3\r\n  - @gradio/upload@0.5.6\r\n\r\n## 0.4.13\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`245d58e`](https://github.com/gradio-app/gradio/commit/245d58eff788e8d44a59d37a2d9b26d0f08a62b4)]:\r\n  - @gradio/client@0.9.2\r\n  - @gradio/image@0.5.2\r\n  - @gradio/upload@0.5.5\r\n\r\n## 0.4.12\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`5d51fbc`](https://github.com/gradio-app/gradio/commit/5d51fbce7826da840a2fd4940feb5d9ad6f1bc5a), [`34f9431`](https://github.com/gradio-app/gradio/commit/34f943101bf7dd6b8a8974a6131c1ed7c4a0dac0)]:\r\n  - @gradio/upload@0.5.4\r\n  - @gradio/client@0.9.1\r\n  - @gradio/image@0.5.1\r\n\r\n## 0.4.11\r\n\r\n### Fixes\r\n\r\n- [#6487](https://github.com/gradio-app/gradio/pull/6487) [`9a5811d`](https://github.com/gradio-app/gradio/commit/9a5811df9218b622af59ba243a937a9c36ba00f9) - Fix the download button of the `gr.Gallery()` component to work. Thanks [@whitphx](https://github.com/whitphx)!\r\n\r\n## 0.4.10\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`b639e04`](https://github.com/gradio-app/gradio/commit/b639e040741e6c0d9104271c81415d7befbd8cf3), [`206af31`](https://github.com/gradio-app/gradio/commit/206af31d7c1a31013364a44e9b40cf8df304ba50)]:\r\n  - @gradio/image@0.4.2\r\n  - @gradio/icons@0.3.1\r\n  - @gradio/atoms@0.3.1\r\n  - @gradio/statustracker@0.4.1\r\n  - @gradio/upload@0.5.2\r\n\r\n## 0.4.9\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`71f1a1f99`](https://github.com/gradio-app/gradio/commit/71f1a1f9931489d465c2c1302a5c8d768a3cd23a)]:\r\n  - @gradio/client@0.8.2\r\n  - @gradio/image@0.4.1\r\n  - @gradio/upload@0.5.1\r\n\r\n## 0.4.8\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`9caddc17b`](https://github.com/gradio-app/gradio/commit/9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8)]:\r\n  - @gradio/atoms@0.3.0\r\n  - @gradio/icons@0.3.0\r\n  - @gradio/image@0.4.0\r\n  - @gradio/statustracker@0.4.0\r\n  - @gradio/upload@0.5.0\r\n\r\n## 0.4.7\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`2f805a7dd`](https://github.com/gradio-app/gradio/commit/2f805a7dd3d2b64b098f659dadd5d01258290521), [`f816136a0`](https://github.com/gradio-app/gradio/commit/f816136a039fa6011be9c4fb14f573e4050a681a)]:\r\n  - @gradio/image@0.3.6\r\n  - @gradio/upload@0.4.2\r\n  - @gradio/atoms@0.2.2\r\n  - @gradio/icons@0.2.1\r\n  - @gradio/statustracker@0.3.2\r\n\r\n## 0.4.6\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`324867f63`](https://github.com/gradio-app/gradio/commit/324867f63c920113d89a565892aa596cf8b1e486)]:\r\n  - @gradio/client@0.8.1\r\n  - @gradio/image@0.3.5\r\n  - @gradio/upload@0.4.1\r\n\r\n## 0.4.5\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`6204ccac5`](https://github.com/gradio-app/gradio/commit/6204ccac5967763e0ebde550d04d12584243a120), [`4d3aad33a`](https://github.com/gradio-app/gradio/commit/4d3aad33a0b66639dbbb2928f305a79fb7789b2d), [`854b482f5`](https://github.com/gradio-app/gradio/commit/854b482f598e0dc47673846631643c079576da9c), [`f1409f95e`](https://github.com/gradio-app/gradio/commit/f1409f95ed39c5565bed6a601e41f94e30196a57)]:\r\n  - @gradio/image@0.3.4\r\n  - @gradio/upload@0.4.0\r\n  - @gradio/client@0.8.0\r\n\r\n## 0.4.4\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`bca6c2c80`](https://github.com/gradio-app/gradio/commit/bca6c2c80f7e5062427019de45c282238388af95), [`3cdeabc68`](https://github.com/gradio-app/gradio/commit/3cdeabc6843000310e1a9e1d17190ecbf3bbc780), [`fad92c29d`](https://github.com/gradio-app/gradio/commit/fad92c29dc1f5cd84341aae417c495b33e01245f)]:\r\n  - @gradio/client@0.7.2\r\n  - @gradio/atoms@0.2.1\r\n  - @gradio/upload@0.3.3\r\n  - @gradio/image@0.3.3\r\n  - @gradio/statustracker@0.3.1\r\n\r\n## 0.4.3\r\n\r\n### Fixes\r\n\r\n- [#6288](https://github.com/gradio-app/gradio/pull/6288) [`92278729e`](https://github.com/gradio-app/gradio/commit/92278729ee008126af15ffe6be399236211b2f34) - Gallery preview fix and optionally skip download of urls in postprcess. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\r\n\r\n## 0.4.2\r\n\r\n### Fixes\r\n\r\n- [#6277](https://github.com/gradio-app/gradio/pull/6277) [`5fe091367`](https://github.com/gradio-app/gradio/commit/5fe091367fbe0eecdd504aa734ca1c70b0621f52) - handle selected_index prop change for gallery. Thanks [@pngwn](https://github.com/pngwn)!\r\n\r\n## 0.4.1\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`2ba14b284`](https://github.com/gradio-app/gradio/commit/2ba14b284f908aa13859f4337167a157075a68eb)]:\r\n  - @gradio/client@0.7.1\r\n  - @gradio/image@0.3.1\r\n  - @gradio/upload@0.3.1\r\n\r\n## 0.4.0\r\n\r\n### Features\r\n\r\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - fix circular dependency with client + upload. Thanks [@pngwn](https://github.com/pngwn)!\r\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Clean root url. Thanks [@pngwn](https://github.com/pngwn)!\r\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Image v4. Thanks [@pngwn](https://github.com/pngwn)!\r\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Publish all components to npm. Thanks [@pngwn](https://github.com/pngwn)!\r\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Custom components. Thanks [@pngwn](https://github.com/pngwn)!\r\n\r\n## 0.4.0-beta.9\r\n\r\n### Features\r\n\r\n- [#6143](https://github.com/gradio-app/gradio/pull/6143) [`e4f7b4b40`](https://github.com/gradio-app/gradio/commit/e4f7b4b409323b01aa01b39e15ce6139e29aa073) - fix circular dependency with client + upload. Thanks [@pngwn](https://github.com/pngwn)!\r\n- [#6136](https://github.com/gradio-app/gradio/pull/6136) [`667802a6c`](https://github.com/gradio-app/gradio/commit/667802a6cdbfb2ce454a3be5a78e0990b194548a) - JS Component Documentation. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\r\n- [#6094](https://github.com/gradio-app/gradio/pull/6094) [`c476bd5a5`](https://github.com/gradio-app/gradio/commit/c476bd5a5b70836163b9c69bf4bfe068b17fbe13) - Image v4. Thanks [@pngwn](https://github.com/pngwn)!\r\n\r\n## 0.4.0-beta.8\r\n\r\n### Features\r\n\r\n- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676`](https://github.com/gradio-app/gradio/commit/83e947676d327ca2ab6ae2a2d710c78961c771a0) - Format js in v4 branch. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\r\n\r\n### Fixes\r\n\r\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)!\r\n\r\n## 0.4.0-beta.7\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies []:\r\n  - @gradio/image@0.3.0-beta.7\r\n\r\n## 0.4.0-beta.6\r\n\r\n### Features\r\n\r\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f`](https://github.com/gradio-app/gradio/commit/319c30f3fccf23bfe1da6c9b132a6a99d59652f7) - rererefactor frontend files. Thanks [@pngwn](https://github.com/pngwn)!\r\n- [#5938](https://github.com/gradio-app/gradio/pull/5938) [`13ed8a485`](https://github.com/gradio-app/gradio/commit/13ed8a485d5e31d7d75af87fe8654b661edcca93) - V4: Use beta release versions for '@gradio' packages. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\r\n\r\n## 0.5.1\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`b67115e8e`](https://github.com/gradio-app/gradio/commit/b67115e8e6e489fffd5271ea830211863241ddc5), [`8d909624f`](https://github.com/gradio-app/gradio/commit/8d909624f61a49536e3c0f71cb2d9efe91216219), [`e70805d54`](https://github.com/gradio-app/gradio/commit/e70805d54cc792452545f5d8eccc1aa0212a4695)]:\r\n  - @gradio/image@0.4.0\r\n  - @gradio/atoms@0.2.0\r\n  - @gradio/statustracker@0.2.3\r\n  - @gradio/upload@0.3.3\r\n\r\n## 0.5.0\r\n\r\n### Features\r\n\r\n- [#5780](https://github.com/gradio-app/gradio/pull/5780) [`ed0f9a21b`](https://github.com/gradio-app/gradio/commit/ed0f9a21b04ad6b941b63d2ce45100dbd1abd5c5) - Adds `change()` event to `gr.Gallery`. Thanks [@abidlabs](https://github.com/abidlabs)!\r\n- [#5783](https://github.com/gradio-app/gradio/pull/5783) [`4567788bd`](https://github.com/gradio-app/gradio/commit/4567788bd1fc25df9322902ba748012e392b520a) - Adds the ability to set the `selected_index` in a `gr.Gallery`. Thanks [@abidlabs](https://github.com/abidlabs)!\r\n\r\n### Fixes\r\n\r\n- [#5798](https://github.com/gradio-app/gradio/pull/5798) [`a0d3cc45c`](https://github.com/gradio-app/gradio/commit/a0d3cc45c6db48dc0db423c229b8fb285623cdc4) - Fix `gr.SelectData` so that the target attribute is correctly attached, and the filedata is included in the data attribute with `gr.Gallery`. Thanks [@abidlabs](https://github.com/abidlabs)!\r\n\r\n## 0.4.1\r\n\r\n### Fixes\r\n\r\n- [#5735](https://github.com/gradio-app/gradio/pull/5735) [`abb5e9df4`](https://github.com/gradio-app/gradio/commit/abb5e9df47989b2c56c2c312d74944678f9f2d4e) - Ensure images with no caption download in gallery. Thanks [@hannahblair](https://github.com/hannahblair)!\r\n- [#5754](https://github.com/gradio-app/gradio/pull/5754) [`502054848`](https://github.com/gradio-app/gradio/commit/502054848fdbe39fc03ec42445242b4e49b7affc) - Fix Gallery `columns` and `rows` params. Thanks [@abidlabs](https://github.com/abidlabs)!\r\n\r\n## 0.4.0\r\n\r\n### Features\r\n\r\n- [#5554](https://github.com/gradio-app/gradio/pull/5554) [`75ddeb390`](https://github.com/gradio-app/gradio/commit/75ddeb390d665d4484667390a97442081b49a423) - Accessibility Improvements. Thanks [@hannahblair](https://github.com/hannahblair)!\r\n\r\n## 0.3.4\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`e0d61b8ba`](https://github.com/gradio-app/gradio/commit/e0d61b8baa0f6293f53b9bdb1647d42f9ae2583a)]:\r\n  - @gradio/image@0.2.4\r\n\r\n## 0.3.3\r\n\r\n### Fixes\r\n\r\n- [#5528](https://github.com/gradio-app/gradio/pull/5528) [`dc86e4a7`](https://github.com/gradio-app/gradio/commit/dc86e4a7e1c40b910c74558e6f88fddf9b3292bc) - Lazy load all images. Thanks [@aliabid94](https://github.com/aliabid94)!\r\n\r\n## 0.3.2\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`afac0006`](https://github.com/gradio-app/gradio/commit/afac0006337ce2840cf497cd65691f2f60ee5912)]:\r\n  - @gradio/statustracker@0.2.0\r\n  - @gradio/image@0.2.2\r\n  - @gradio/utils@0.1.1\r\n  - @gradio/atoms@0.1.2\r\n  - @gradio/upload@0.2.1\r\n\r\n## 0.3.1\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`abf1c57d`](https://github.com/gradio-app/gradio/commit/abf1c57d7d85de0df233ee3b38aeb38b638477db), [`79d8f9d8`](https://github.com/gradio-app/gradio/commit/79d8f9d891901683c5a1b7486efb44eab2478c96)]:\r\n  - @gradio/icons@0.1.0\r\n  - @gradio/utils@0.1.0\r\n  - @gradio/upload@0.2.0\r\n  - @gradio/atoms@0.1.1\r\n  - @gradio/image@0.2.1\r\n  - @gradio/statustracker@0.1.1\r\n\r\n## 0.3.0\r\n\r\n### Highlights\r\n\r\n#### Improve startup performance and markdown support ([#5279](https://github.com/gradio-app/gradio/pull/5279) [`fe057300`](https://github.com/gradio-app/gradio/commit/fe057300f0672c62dab9d9b4501054ac5d45a4ec))\r\n\r\n##### Improved markdown support\r\n\r\nWe now have better support for markdown in `gr.Markdown` and `gr.Dataframe`. Including syntax highlighting and Github Flavoured Markdown. We also have more consistent markdown behaviour and styling.\r\n\r\n##### Various performance improvements\r\n\r\nThese improvements will be particularly beneficial to large applications.\r\n\r\n- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.\r\n- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).\r\n- Corrected an issue that was causing markdown to re-render infinitely.\r\n- Ensured that the `gr.3DModel` does re-render prematurely.\r\n\r\nThanks [@pngwn](https://github.com/pngwn)!\r\n\r\n### Features\r\n\r\n- [#5215](https://github.com/gradio-app/gradio/pull/5215) [`fbdad78a`](https://github.com/gradio-app/gradio/commit/fbdad78af4c47454cbb570f88cc14bf4479bbceb) - Lazy load interactive or static variants of a component individually, rather than loading both variants regardless. This change will improve performance for many applications. Thanks [@pngwn](https://github.com/pngwn)!\r\n- [#5216](https://github.com/gradio-app/gradio/pull/5216) [`4b58ea6d`](https://github.com/gradio-app/gradio/commit/4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8) - Update i18n tokens and locale files. Thanks [@hannahblair](https://github.com/hannahblair)!\r\n\r\n## 0.2.0\r\n\r\n### Features\r\n\r\n- [#5025](https://github.com/gradio-app/gradio/pull/5025) [`6693660a`](https://github.com/gradio-app/gradio/commit/6693660a790996f8f481feaf22a8c49130d52d89) - Add download button to selected images in `Gallery`. Thanks [@hannahblair](https://github.com/hannahblair)!\r\n\r\n## 0.1.0\r\n\r\n### Features\r\n\r\n- [#4995](https://github.com/gradio-app/gradio/pull/4995) [`3f8c210b`](https://github.com/gradio-app/gradio/commit/3f8c210b01ef1ceaaf8ee73be4bf246b5b745bbf) - Implement left and right click in `Gallery` component and show implicit images in `Gallery` grid. Thanks [@hannahblair](https://github.com/hannahblair)!\r\n"
            },
            {
              "fill_history": {},
              "metavars": {
                "filename": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "905de1be-07be-4467-ab4c-60fae840c6e6",
                "size": "4362",
                "type": "text/plain"
              },
              "prompt": "",
              "text": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs."
            },
            {
              "fill_history": {},
              "metavars": {
                "filename": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "id": "31875f02-4d4a-4f60-8069-b24d70795c99",
                "size": "2398",
                "type": "text/plain"
              },
              "prompt": "",
              "text": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\r\nthe License. You may obtain a copy of the License at\r\n\r\nhttp://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\r\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\r\nspecific language governing permissions and limitations under the License.\r\n-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\r\n2. use multiple pipelines in your workflow\r\n\r\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\r\n\r\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\r\n\r\n```py\r\nfrom diffusers import AutoPipelineForText2Image\r\nimport torch\r\n\r\npipeline = AutoPipelineForText2Image.from_pretrained(\r\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\r\n).to(\"cuda\")\r\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\r\n\r\nimage = pipeline(prompt, num_inference_steps=25).images[0]\r\n```\r\n\r\n<Tip>\r\n\r\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\r\n\r\n</Tip>\r\n\r\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\r\n\r\n- [Stable Diffusion](./stable_diffusion/overview)\r\n- [ControlNet](./controlnet)\r\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\r\n- [DeepFloyd IF](./deepfloyd_if)\r\n- [Kandinsky 2.1](./kandinsky)\r\n- [Kandinsky 2.2](./kandinsky_v22)\r\n\r\n\r\n## AutoPipelineForText2Image\r\n\r\n[[autodoc]] AutoPipelineForText2Image\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe\r\n\r\n## AutoPipelineForImage2Image\r\n\r\n[[autodoc]] AutoPipelineForImage2Image\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe\r\n\r\n## AutoPipelineForInpainting\r\n\r\n[[autodoc]] AutoPipelineForInpainting\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe\r\n"
            },
            {
              "fill_history": {},
              "metavars": {
                "filename": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "4be240a6-1797-4617-b2b7-3544c273c2f4",
                "size": "5262",
                "type": "text/plain"
              },
              "prompt": "",
              "text": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\r\nthe License. You may obtain a copy of the License at\r\n\r\nhttp://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\r\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\r\nspecific language governing permissions and limitations under the License.\r\n-->\r\n\r\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\r\n\r\n## Preamble\r\n\r\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\r\n\r\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\r\n\r\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\r\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\r\n\r\n\r\n## Scope\r\n\r\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\r\n\r\n\r\n## Ethical guidelines\r\n\r\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\r\n\r\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\r\n\r\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\r\n\r\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\r\n\r\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\r\n\r\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\r\n\r\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\r\n\r\n\r\n## Examples of implementations: Safety features and Mechanisms\r\n\r\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\r\n\r\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\r\n\r\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\r\n\r\n- **Encouraging safety in deployment**\r\n\r\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\r\n\r\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\r\n\r\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\r\n\r\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\r\n"
            },
            {
              "fill_history": {},
              "metavars": {
                "filename": "huggingface_diffusers_blob_main_src_diffusers_experimental_README.md.txt",
                "id": "8c572804-dd34-4a89-a896-bb874ed5a6ca",
                "size": "290",
                "type": "text/plain"
              },
              "prompt": "",
              "text": " ðŸ§¨ Diffusers Experimental\r\n\r\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\r\nCurrently, the following experiments are supported:\r\n* Reinforcement learning via an implementation of the [Diffuser](https://arxiv.org/abs/2205.09991) model."
            },
            {
              "fill_history": {},
              "metavars": {
                "filename": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "ee359072-5a6d-45c3-b8b4-66ef463b848a",
                "size": "9975",
                "type": "text/plain"
              },
              "prompt": "",
              "text": "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\r\nrendered properly in your Markdown viewer.\r\n-->\r\n\r\n# Environment variables\r\n\r\n`huggingface_hub` can be configured using environment variables.\r\n\r\nIf you are unfamiliar with environment variable, here are generic articles about them\r\n[on macOS and Linux](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/)\r\nand on [Windows](https://phoenixnap.com/kb/windows-set-environment-variable).\r\n\r\nThis page will guide you through all environment variables specific to `huggingface_hub`\r\nand their meaning.\r\n\r\n## Generic\r\n\r\n### HF_INFERENCE_ENDPOINT\r\n\r\nTo configure the inference api base url. You might want to set this variable if your organization\r\nis pointing at an API Gateway rather than directly at the inference api.\r\n\r\nDefaults to `\"https://api-inference.huggingface.com\"`.\r\n\r\n### HF_HOME\r\n\r\nTo configure where `huggingface_hub` will locally store data. In particular, your token\r\nand the cache will be stored in this folder.\r\n\r\nDefaults to `\"~/.cache/huggingface\"` unless [XDG_CACHE_HOME](#xdgcachehome) is set.\r\n\r\n### HF_HUB_CACHE\r\n\r\nTo configure where repositories from the Hub will be cached locally (models, datasets and\r\nspaces).\r\n\r\nDefaults to `\"$HF_HOME/hub\"` (e.g. `\"~/.cache/huggingface/hub\"` by default).\r\n\r\n### HF_ASSETS_CACHE\r\n\r\nTo configure where [assets](../guides/manage-cache#caching-assets) created by downstream libraries\r\nwill be cached locally. Those assets can be preprocessed data, files downloaded from GitHub,\r\nlogs,...\r\n\r\nDefaults to `\"$HF_HOME/assets\"` (e.g. `\"~/.cache/huggingface/assets\"` by default).\r\n\r\n### HF_TOKEN\r\n\r\nTo configure the User Access Token to authenticate to the Hub. If set, this value will\r\noverwrite the token stored on the machine (in `\"$HF_HOME/token\"`).\r\n\r\nFor more details about authentication, check out [this section](../quick-start#authentication).\r\n\r\n### HF_HUB_VERBOSITY\r\n\r\nSet the verbosity level of the `huggingface_hub`'s logger. Must be one of\r\n`{\"debug\", \"info\", \"warning\", \"error\", \"critical\"}`.\r\n\r\nDefaults to `\"warning\"`.\r\n\r\nFor more details, see [logging reference](../package_reference/utilities#huggingface_hub.utils.logging.get_verbosity).\r\n\r\n### HF_HUB_LOCAL_DIR_AUTO_SYMLINK_THRESHOLD\r\n\r\nInteger value to define under which size a file is considered as \"small\". When downloading files to a local directory,\r\nsmall files will be duplicated to ease user experience while bigger files are symlinked to save disk usage.\r\n\r\nFor more details, see the [download guide](../guides/download#download-files-to-local-folder).\r\n\r\n### HF_HUB_ETAG_TIMEOUT\r\n\r\nInteger value to define the number of seconds to wait for server response when fetching the latest metadata from a repo before downloading a file. If the request times out, `huggingface_hub` will default to the locally cached files. Setting a lower value speeds up the workflow for machines with a slow connection that have already cached files. A higher value guarantees the metadata call to succeed in more cases. Default to 10s.\r\n\r\n### HF_HUB_DOWNLOAD_TIMEOUT\r\n\r\nInteger value to define the number of seconds to wait for server response when downloading a file. If the request times out, a TimeoutError is raised. Setting a higher value is beneficial on machine with a slow connection. A smaller value makes the process fail quicker in case of complete network outage. Default to 10s.\r\n\r\n## Boolean values\r\n\r\nThe following environment variables expect a boolean value. The variable will be considered\r\nas `True` if its value is one of `{\"1\", \"ON\", \"YES\", \"TRUE\"}` (case-insensitive). Any other value\r\n(or undefined) will be considered as `False`.\r\n\r\n### HF_HUB_OFFLINE\r\n\r\nIf set, no HTTP calls will me made to the Hugging Face Hub. If you try to download files, only the cached files will be accessed. If no cache file is detected, an error is raised This is useful in case your network is slow and you don't care about having the latest version of a file.\r\n\r\nIf `HF_HUB_OFFLINE=1` is set as environment variable and you call any method of [`HfApi`], an [`~huggingface_hub.utils.OfflineModeIsEnabled`] exception will be raised.\r\n\r\n**Note:** even if the latest version of a file is cached, calling `hf_hub_download` still triggers a HTTP request to check that a new version is not available. Setting `HF_HUB_OFFLINE=1` will skip this call which speeds up your loading time.\r\n\r\n### HF_HUB_DISABLE_IMPLICIT_TOKEN\r\n\r\nAuthentication is not mandatory for every requests to the Hub. For instance, requesting\r\ndetails about `\"gpt2\"` model does not require to be authenticated. However, if a user is\r\n[logged in](../package_reference/login), the default behavior will be to always send the token\r\nin order to ease user experience (never get a HTTP 401 Unauthorized) when accessing private or gated repositories. For privacy, you can\r\ndisable this behavior by setting `HF_HUB_DISABLE_IMPLICIT_TOKEN=1`. In this case,\r\nthe token will be sent only for \"write-access\" calls (example: create a commit).\r\n\r\n**Note:** disabling implicit sending of token can have weird side effects. For example,\r\nif you want to list all models on the Hub, your private models will not be listed. You\r\nwould need to explicitly pass `token=True` argument in your script.\r\n\r\n### HF_HUB_DISABLE_PROGRESS_BARS\r\n\r\nFor time consuming tasks, `huggingface_hub` displays a progress bar by default (using tqdm).\r\nYou can disable all the progress bars at once by setting `HF_HUB_DISABLE_PROGRESS_BARS=1`.\r\n\r\n### HF_HUB_DISABLE_SYMLINKS_WARNING\r\n\r\nIf you are on a Windows machine, it is recommended to enable the developer mode or to run\r\n`huggingface_hub` in admin mode. If not, `huggingface_hub` will not be able to create\r\nsymlinks in your cache system. You will be able to execute any script but your user experience\r\nwill be degraded as some huge files might end-up duplicated on your hard-drive. A warning\r\nmessage is triggered to warn you about this behavior. Set `HF_HUB_DISABLE_SYMLINKS_WARNING=1`,\r\nto disable this warning.\r\n\r\nFor more details, see [cache limitations](../guides/manage-cache#limitations).\r\n\r\n### HF_HUB_DISABLE_EXPERIMENTAL_WARNING\r\n\r\nSome features of `huggingface_hub` are experimental. This means you can use them but we do not guarantee they will be\r\nmaintained in the future. In particular, we might update the API or behavior of such features without any deprecation\r\ncycle. A warning message is triggered when using an experimental feature to warn you about it. If you're comfortable debugging any potential issues using an experimental feature, you can set `HF_HUB_DISABLE_EXPERIMENTAL_WARNING=1` to disable the warning.\r\n\r\nIf you are using an experimental feature, please let us know! Your feedback can help us design and improve it.\r\n\r\n### HF_HUB_DISABLE_TELEMETRY\r\n\r\nBy default, some data is collected by HF libraries (`transformers`, `datasets`, `gradio`,..) to monitor usage, debug issues and help prioritize features.\r\nEach library defines its own policy (i.e. which usage to monitor) but the core implementation happens in `huggingface_hub` (see [`send_telemetry`]).\r\n\r\nYou can set `HF_HUB_DISABLE_TELEMETRY=1` as environment variable to globally disable telemetry.\r\n\r\n### HF_HUB_ENABLE_HF_TRANSFER\r\n\r\nSet to `True` for faster uploads and downloads from the Hub using `hf_transfer`.\r\n\r\nBy default, `huggingface_hub` uses the Python-based `requests.get` and `requests.post` functions. Although these are reliable and versatile, they may not be the most efficient choice for machines with high bandwidth. [`hf_transfer`](https://github.com/huggingface/hf_transfer) is a Rust-based package developed to maximize the bandwidth used by dividing large files into smaller parts and transferring them simultaneously using multiple threads. This approach can potentially double the transfer speed. To use `hf_transfer`, you need to install it separately [from PyPI](https://pypi.org/project/hf-transfer/) and set `HF_HUB_ENABLE_HF_TRANSFER=1` as an environment variable.\r\n\r\nPlease note that using `hf_transfer` comes with certain limitations. Since it is not purely Python-based, debugging errors may be challenging. Additionally, `hf_transfer` lacks several user-friendly features such as resumable downloads and proxies. These omissions are intentional to maintain the simplicity and speed of the Rust logic. Consequently, `hf_transfer` is not enabled by default in `huggingface_hub`.\r\n\r\n## Deprecated environment variables\r\n\r\nIn order to standardize all environment variables within the Hugging Face ecosystem, some variables have been marked as deprecated. Although they remain functional, they no longer take precedence over their replacements. The following table outlines the deprecated variables and their corresponding alternatives:\r\n\r\n\r\n| Deprecated Variable | Replacement |\r\n| --- | --- |\r\n| `HUGGINGFACE_HUB_CACHE` | `HF_HUB_CACHE` |\r\n| `HUGGINGFACE_ASSETS_CACHE` | `HF_ASSETS_CACHE` |\r\n| `HUGGING_FACE_HUB_TOKEN` | `HF_TOKEN` |\r\n| `HUGGINGFACE_HUB_VERBOSITY` | `HF_HUB_VERBOSITY` |\r\n\r\n## From external tools\r\n\r\nSome environment variables are not specific to `huggingface_hub` but are still taken into account when they are set.\r\n\r\n### DO_NOT_TRACK\r\n\r\nBoolean value. Equivalent to `HF_HUB_DISABLE_TELEMETRY`. When set to true, telemetry is globally disabled in the Hugging Face Python ecosystem (`transformers`, `diffusers`, `gradio`, etc.). See https://consoledonottrack.com/ for more details.\r\n\r\n### NO_COLOR\r\n\r\nBoolean value. When set, `huggingface-cli` tool will not print any ANSI color.\r\nSee [no-color.org](https://no-color.org/).\r\n\r\n### XDG_CACHE_HOME\r\n\r\nUsed only when `HF_HOME` is not set!\r\n\r\nThis is the default way to configure where [user-specific non-essential (cached) data should be written](https://wiki.archlinux.org/title/XDG_Base_Directory)\r\non linux machines. \r\n\r\nIf `HF_HOME` is not set, the default home will be `\"$XDG_CACHE_HOME/huggingface\"` instead\r\nof `\"~/.cache/huggingface\"`.\r\n"
            },
            {
              "fill_history": {},
              "metavars": {
                "filename": "huggingface_tokenizers_blob_main_bindings_node_npm_linux-x64-musl_README.md.txt",
                "id": "b21458d5-2e88-4b77-b7b6-7cd22215edb2",
                "size": "99",
                "type": "text/plain"
              },
              "prompt": "",
              "text": " `tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\r\n"
            },
            {
              "fill_history": {},
              "metavars": {
                "filename": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "945bf623-e3a7-4e05-bf48-66eb169db6f8",
                "size": "6187",
                "type": "text/plain"
              },
              "prompt": "",
              "text": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\r\nthe License. You may obtain a copy of the License at\r\n\r\nhttp://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\r\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\r\nspecific language governing permissions and limitations under the License.\r\n\r\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\r\nrendered properly in your Markdown viewer.\r\n\r\n-->\r\n\r\n# Instantiating a big model\r\n\r\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\r\nfrom PyTorch is:\r\n\r\n1. Create your model with random weights.\r\n2. Load your pretrained weights.\r\n3. Put those pretrained weights in your random model.\r\n\r\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\r\n\r\n<Tip>\r\n\r\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \r\n\r\n</Tip>\r\n\r\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\r\n\r\n## Sharded checkpoints\r\n\r\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\r\n\r\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\r\n\r\n```py\r\nfrom transformers import AutoModel\r\n\r\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\r\n```\r\n\r\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\r\n\r\n```py\r\n>>> import os\r\n>>> import tempfile\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir)\r\n...     print(sorted(os.listdir(tmp_dir)))\r\n['config.json', 'pytorch_model.bin']\r\n```\r\n\r\nNow let's use a maximum shard size of 200MB:\r\n\r\n```py\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\r\n...     print(sorted(os.listdir(tmp_dir)))\r\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\r\n```\r\n\r\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\r\n\r\n```py\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\r\n...     new_model = AutoModel.from_pretrained(tmp_dir)\r\n```\r\n\r\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\r\n\r\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\r\n\r\n```py\r\n>>> import json\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\r\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\r\n...         index = json.load(f)\r\n\r\n>>> print(index.keys())\r\ndict_keys(['metadata', 'weight_map'])\r\n```\r\n\r\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\r\n\r\n```py\r\n>>> index[\"metadata\"]\r\n{'total_size': 433245184}\r\n```\r\n\r\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\r\n\r\n```py\r\n>>> index[\"weight_map\"]\r\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\r\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\r\n ...\r\n```\r\n\r\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\r\n\r\n```py\r\n>>> from transformers.modeling_utils import load_sharded_checkpoint\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\r\n...     load_sharded_checkpoint(model, tmp_dir)\r\n```\r\n\r\n## Low memory loading\r\n\r\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\r\n\r\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\r\n"
            }
          ]
        },
        "dragging": false,
        "id": "upload-e9d11bb2-f4f5-43a2-8d28-4c54f345659e",
        "position": {
          "x": -2080,
          "y": 848
        },
        "positionAbsolute": {
          "x": -2080,
          "y": 848
        },
        "selected": false,
        "type": "upload"
      },
      {
        "width": 422,
        "height": 475,
        "data": {
          "columns": [
            {
              "header": "question",
              "key": "c0"
            },
            {
              "header": "reference",
              "key": "c1"
            },
            {
              "header": "answer_context",
              "key": "c2"
            },
            {
              "header": "source_doc",
              "key": "c3"
            }
          ],
          "rows": [
            {
              "__uid": "60c00763-94a2-459e-b5fc-be9f19680e56",
              "c0": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
              "c1": "distilbert base uncased finetuned sst2 english",
              "c2": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
              "c3": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
            },
            {
              "__uid": "a32c0842-4f6c-4d37-ac1e-35a4581f332c",
              "c0": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
              "c1": "10GB",
              "c2": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
              "c3": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
            },
            {
              "__uid": "3bc91950-3627-4357-b733-d070039f7763",
              "c0": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
              "c1": "x86_64-unknown-linux-musl",
              "c2": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
              "c3": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
            },
            {
              "__uid": "8fd6885c-a2bf-448a-b588-a0fe4cb6666f",
              "c0": "What is the purpose of the Safety Checker in the Diffusers library?\n",
              "c1": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
              "c2": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
              "c3": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
            },
            {
              "__uid": "a0879f3a-aa04-45b9-9410-aa0dcafc5f37",
              "c0": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
              "c1": "from_pretrained()",
              "c2": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
              "c3": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
            },
            {
              "__uid": "c1287d05-8d11-46cf-b700-f5478e581592",
              "c0": ""
            }
          ],
          "sel_rows": null
        },
        "dragging": false,
        "id": "table-ab424041-6fa2-46b0-9121-517805638f94",
        "position": {
          "x": -1744,
          "y": -80
        },
        "positionAbsolute": {
          "x": -1744,
          "y": -80
        },
        "selected": false,
        "type": "table"
      },
      {
        "width": 312,
        "height": 352,
        "data": {
          "fields": [
            {
              "fill_history": {
                "baseMethod": 34,
                "chunkLibrary": 4,
                "chunkMethod": 5,
                "context": 37,
                "methodId": 92,
                "query": 19,
                "retrievalMethod": 36
              },
              "llm": {
                "base_model": "gpt-4",
                "emoji": "ðŸ¤–",
                "formData": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "model": "gpt-4o-mini",
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": "text",
                  "seed": null,
                  "shortname": "gpt-4o-mini",
                  "stop": "",
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": "",
                  "top_p": 1
                },
                "key": "5fce3844-27b9-41f9-9b81-447b08f10b23",
                "model": "gpt-4o-mini",
                "name": "gpt-4o-mini",
                "progress": {
                  "error": 0,
                  "success": 95
                },
                "settings": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": {
                    "type": "text"
                  },
                  "seed": null,
                  "stop": [],
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": [],
                  "top_p": 1
                },
                "temp": 1
              },
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 22,
                "baseMethod": 34,
                "chunkMethod": 5,
                "method": 36,
                "reference": 21,
                "signature": 38,
                "source_doc": 23
              },
              "prompt": 182,
              "text": 17,
              "uid": "fac8100e-19a6-4045-b612-2587c5fde08b"
            },
            {
              "fill_history": {
                "baseMethod": 34,
                "chunkLibrary": 4,
                "chunkMethod": 5,
                "context": 43,
                "methodId": 92,
                "query": 9,
                "retrievalMethod": 36
              },
              "llm": {
                "base_model": "gpt-4",
                "emoji": "ðŸ¤–",
                "formData": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "model": "gpt-4o-mini",
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": "text",
                  "seed": null,
                  "shortname": "gpt-4o-mini",
                  "stop": "",
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": "",
                  "top_p": 1
                },
                "key": "5fce3844-27b9-41f9-9b81-447b08f10b23",
                "model": "gpt-4o-mini",
                "name": "gpt-4o-mini",
                "progress": {
                  "error": 0,
                  "success": 95
                },
                "settings": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": {
                    "type": "text"
                  },
                  "seed": null,
                  "stop": [],
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": [],
                  "top_p": 1
                },
                "temp": 1
              },
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 13,
                "baseMethod": 34,
                "chunkMethod": 5,
                "method": 36,
                "reference": 12,
                "signature": 38,
                "source_doc": 15
              },
              "prompt": 183,
              "text": 2,
              "uid": "07647594-a268-4772-8358-783edb8c47e2"
            },
            {
              "fill_history": {
                "baseMethod": 3,
                "chunkLibrary": 4,
                "chunkMethod": 5,
                "context": 20,
                "docTitle": 18,
                "embeddingModel": 7,
                "methodId": 93,
                "query": 19,
                "retrievalMethod": 10
              },
              "llm": {
                "base_model": "gpt-4",
                "emoji": "ðŸ¤–",
                "formData": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "model": "gpt-4o-mini",
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": "text",
                  "seed": null,
                  "shortname": "gpt-4o-mini",
                  "stop": "",
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": "",
                  "top_p": 1
                },
                "key": "5fce3844-27b9-41f9-9b81-447b08f10b23",
                "model": "gpt-4o-mini",
                "name": "gpt-4o-mini",
                "progress": {
                  "error": 0,
                  "success": 95
                },
                "settings": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": {
                    "type": "text"
                  },
                  "seed": null,
                  "stop": [],
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": [],
                  "top_p": 1
                },
                "temp": 1
              },
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 22,
                "baseMethod": 3,
                "chunkMethod": 5,
                "docTitle": 18,
                "embeddingModel": 7,
                "method": 10,
                "reference": 21,
                "signature": 14,
                "source_doc": 23
              },
              "prompt": 184,
              "text": 17,
              "uid": "2621bb44-b73e-40af-a553-3768d82cfc9a"
            },
            {
              "fill_history": {
                "baseMethod": 34,
                "chunkLibrary": 4,
                "chunkMethod": 5,
                "context": 47,
                "methodId": 92,
                "query": 46,
                "retrievalMethod": 36
              },
              "llm": {
                "base_model": "gpt-4",
                "emoji": "ðŸ¤–",
                "formData": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "model": "gpt-4o-mini",
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": "text",
                  "seed": null,
                  "shortname": "gpt-4o-mini",
                  "stop": "",
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": "",
                  "top_p": 1
                },
                "key": "5fce3844-27b9-41f9-9b81-447b08f10b23",
                "model": "gpt-4o-mini",
                "name": "gpt-4o-mini",
                "progress": {
                  "error": 0,
                  "success": 95
                },
                "settings": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": {
                    "type": "text"
                  },
                  "seed": null,
                  "stop": [],
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": [],
                  "top_p": 1
                },
                "temp": 1
              },
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 49,
                "baseMethod": 34,
                "chunkMethod": 5,
                "method": 36,
                "reference": 48,
                "signature": 38,
                "source_doc": 50
              },
              "prompt": 185,
              "text": 45,
              "uid": "1846f4c6-4ed0-4505-9e24-002214fe6ed5"
            },
            {
              "fill_history": {
                "baseMethod": 34,
                "chunkLibrary": 4,
                "chunkMethod": 5,
                "context": 41,
                "methodId": 92,
                "query": 27,
                "retrievalMethod": 36
              },
              "llm": {
                "base_model": "gpt-4",
                "emoji": "ðŸ¤–",
                "formData": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "model": "gpt-4o-mini",
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": "text",
                  "seed": null,
                  "shortname": "gpt-4o-mini",
                  "stop": "",
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": "",
                  "top_p": 1
                },
                "key": "5fce3844-27b9-41f9-9b81-447b08f10b23",
                "model": "gpt-4o-mini",
                "name": "gpt-4o-mini",
                "progress": {
                  "error": 0,
                  "success": 95
                },
                "settings": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": {
                    "type": "text"
                  },
                  "seed": null,
                  "stop": [],
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": [],
                  "top_p": 1
                },
                "temp": 1
              },
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 30,
                "baseMethod": 34,
                "chunkMethod": 5,
                "method": 36,
                "reference": 29,
                "signature": 38,
                "source_doc": 32
              },
              "prompt": 186,
              "text": 40,
              "uid": "41ea9c17-ae54-417a-90b3-eef3320edcf8"
            },
            {
              "fill_history": {
                "baseMethod": 34,
                "chunkLibrary": 4,
                "chunkMethod": 5,
                "context": 61,
                "methodId": 92,
                "query": 60,
                "retrievalMethod": 36
              },
              "llm": {
                "base_model": "gpt-4",
                "emoji": "ðŸ¤–",
                "formData": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "model": "gpt-4o-mini",
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": "text",
                  "seed": null,
                  "shortname": "gpt-4o-mini",
                  "stop": "",
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": "",
                  "top_p": 1
                },
                "key": "5fce3844-27b9-41f9-9b81-447b08f10b23",
                "model": "gpt-4o-mini",
                "name": "gpt-4o-mini",
                "progress": {
                  "error": 0,
                  "success": 95
                },
                "settings": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": {
                    "type": "text"
                  },
                  "seed": null,
                  "stop": [],
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": [],
                  "top_p": 1
                },
                "temp": 1
              },
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 63,
                "baseMethod": 34,
                "chunkMethod": 5,
                "method": 36,
                "reference": 62,
                "signature": 38,
                "source_doc": 64
              },
              "prompt": 187,
              "text": 188,
              "uid": "8442ebb0-fb6f-4b9f-ae0d-3bf0077e460f"
            },
            {
              "fill_history": {
                "baseMethod": 3,
                "chunkLibrary": 4,
                "chunkMethod": 5,
                "context": 11,
                "docTitle": 6,
                "embeddingModel": 7,
                "methodId": 93,
                "query": 9,
                "retrievalMethod": 10
              },
              "llm": {
                "base_model": "gpt-4",
                "emoji": "ðŸ¤–",
                "formData": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "model": "gpt-4o-mini",
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": "text",
                  "seed": null,
                  "shortname": "gpt-4o-mini",
                  "stop": "",
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": "",
                  "top_p": 1
                },
                "key": "5fce3844-27b9-41f9-9b81-447b08f10b23",
                "model": "gpt-4o-mini",
                "name": "gpt-4o-mini",
                "progress": {
                  "error": 0,
                  "success": 95
                },
                "settings": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": {
                    "type": "text"
                  },
                  "seed": null,
                  "stop": [],
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": [],
                  "top_p": 1
                },
                "temp": 1
              },
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 13,
                "baseMethod": 3,
                "chunkMethod": 5,
                "docTitle": 6,
                "embeddingModel": 7,
                "method": 10,
                "reference": 12,
                "signature": 14,
                "source_doc": 15
              },
              "prompt": 189,
              "text": 2,
              "uid": "e5b2b334-688f-48a3-b9ba-22e095010318"
            },
            {
              "fill_history": {
                "baseMethod": 3,
                "chunkLibrary": 4,
                "chunkMethod": 5,
                "context": 69,
                "embeddingModel": 7,
                "methodId": 93,
                "query": 27,
                "retrievalMethod": 10
              },
              "llm": {
                "base_model": "gpt-4",
                "emoji": "ðŸ¤–",
                "formData": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "model": "gpt-4o-mini",
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": "text",
                  "seed": null,
                  "shortname": "gpt-4o-mini",
                  "stop": "",
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": "",
                  "top_p": 1
                },
                "key": "5fce3844-27b9-41f9-9b81-447b08f10b23",
                "model": "gpt-4o-mini",
                "name": "gpt-4o-mini",
                "progress": {
                  "error": 0,
                  "success": 95
                },
                "settings": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": {
                    "type": "text"
                  },
                  "seed": null,
                  "stop": [],
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": [],
                  "top_p": 1
                },
                "temp": 1
              },
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 30,
                "baseMethod": 3,
                "chunkMethod": 5,
                "embeddingModel": 7,
                "method": 10,
                "reference": 29,
                "signature": 14,
                "source_doc": 32
              },
              "prompt": 190,
              "text": 191,
              "uid": "b1a55fe8-d756-4029-a44a-6d1a60268943"
            },
            {
              "fill_history": {
                "baseMethod": 34,
                "chunkLibrary": 4,
                "chunkMethod": 26,
                "context": 52,
                "methodId": 92,
                "query": 27,
                "retrievalMethod": 36
              },
              "llm": {
                "base_model": "gpt-4",
                "emoji": "ðŸ¤–",
                "formData": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "model": "gpt-4o-mini",
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": "text",
                  "seed": null,
                  "shortname": "gpt-4o-mini",
                  "stop": "",
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": "",
                  "top_p": 1
                },
                "key": "5fce3844-27b9-41f9-9b81-447b08f10b23",
                "model": "gpt-4o-mini",
                "name": "gpt-4o-mini",
                "progress": {
                  "error": 0,
                  "success": 95
                },
                "settings": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": {
                    "type": "text"
                  },
                  "seed": null,
                  "stop": [],
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": [],
                  "top_p": 1
                },
                "temp": 1
              },
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 30,
                "baseMethod": 34,
                "chunkMethod": 26,
                "method": 36,
                "reference": 29,
                "signature": 53,
                "source_doc": 32
              },
              "prompt": 192,
              "text": 25,
              "uid": "7fc6868b-42f7-4ed7-8ddb-bb8aa6f0562d"
            },
            {
              "fill_history": {
                "baseMethod": 3,
                "chunkLibrary": 4,
                "chunkMethod": 5,
                "context": 67,
                "embeddingModel": 7,
                "methodId": 93,
                "query": 60,
                "retrievalMethod": 10
              },
              "llm": {
                "base_model": "gpt-4",
                "emoji": "ðŸ¤–",
                "formData": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "model": "gpt-4o-mini",
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": "text",
                  "seed": null,
                  "shortname": "gpt-4o-mini",
                  "stop": "",
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": "",
                  "top_p": 1
                },
                "key": "5fce3844-27b9-41f9-9b81-447b08f10b23",
                "model": "gpt-4o-mini",
                "name": "gpt-4o-mini",
                "progress": {
                  "error": 0,
                  "success": 95
                },
                "settings": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": {
                    "type": "text"
                  },
                  "seed": null,
                  "stop": [],
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": [],
                  "top_p": 1
                },
                "temp": 1
              },
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 63,
                "baseMethod": 3,
                "chunkMethod": 5,
                "embeddingModel": 7,
                "method": 10,
                "reference": 62,
                "signature": 14,
                "source_doc": 64
              },
              "prompt": 193,
              "text": 194,
              "uid": "fc04cf70-1464-499a-8a70-99c4077b8ec9"
            },
            {
              "fill_history": {
                "baseMethod": 3,
                "chunkLibrary": 4,
                "chunkMethod": 26,
                "context": 28,
                "embeddingModel": 7,
                "methodId": 93,
                "query": 27,
                "retrievalMethod": 10
              },
              "llm": {
                "base_model": "gpt-4",
                "emoji": "ðŸ¤–",
                "formData": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "model": "gpt-4o-mini",
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": "text",
                  "seed": null,
                  "shortname": "gpt-4o-mini",
                  "stop": "",
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": "",
                  "top_p": 1
                },
                "key": "5fce3844-27b9-41f9-9b81-447b08f10b23",
                "model": "gpt-4o-mini",
                "name": "gpt-4o-mini",
                "progress": {
                  "error": 0,
                  "success": 95
                },
                "settings": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": {
                    "type": "text"
                  },
                  "seed": null,
                  "stop": [],
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": [],
                  "top_p": 1
                },
                "temp": 1
              },
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 30,
                "baseMethod": 3,
                "chunkMethod": 26,
                "embeddingModel": 7,
                "method": 10,
                "reference": 29,
                "signature": 31,
                "source_doc": 32
              },
              "prompt": 195,
              "text": 196,
              "uid": "ff70540b-64ca-4b91-969b-1ebc75c328eb"
            },
            {
              "fill_history": {
                "baseMethod": 3,
                "chunkLibrary": 4,
                "chunkMethod": 26,
                "context": 55,
                "docTitle": 6,
                "embeddingModel": 7,
                "methodId": 93,
                "query": 9,
                "retrievalMethod": 10
              },
              "llm": {
                "base_model": "gpt-4",
                "emoji": "ðŸ¤–",
                "formData": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "model": "gpt-4o-mini",
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": "text",
                  "seed": null,
                  "shortname": "gpt-4o-mini",
                  "stop": "",
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": "",
                  "top_p": 1
                },
                "key": "5fce3844-27b9-41f9-9b81-447b08f10b23",
                "model": "gpt-4o-mini",
                "name": "gpt-4o-mini",
                "progress": {
                  "error": 0,
                  "success": 95
                },
                "settings": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": {
                    "type": "text"
                  },
                  "seed": null,
                  "stop": [],
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": [],
                  "top_p": 1
                },
                "temp": 1
              },
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 13,
                "baseMethod": 3,
                "chunkMethod": 26,
                "docTitle": 6,
                "embeddingModel": 7,
                "method": 10,
                "reference": 12,
                "signature": 31,
                "source_doc": 15
              },
              "prompt": 197,
              "text": 2,
              "uid": "a7cfeb0a-d170-4fb2-80da-3e7561fd2e9b"
            },
            {
              "fill_history": {
                "baseMethod": 34,
                "chunkLibrary": 4,
                "chunkMethod": 26,
                "context": 76,
                "docTitle": 18,
                "methodId": 92,
                "query": 19,
                "retrievalMethod": 36
              },
              "llm": {
                "base_model": "gpt-4",
                "emoji": "ðŸ¤–",
                "formData": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "model": "gpt-4o-mini",
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": "text",
                  "seed": null,
                  "shortname": "gpt-4o-mini",
                  "stop": "",
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": "",
                  "top_p": 1
                },
                "key": "5fce3844-27b9-41f9-9b81-447b08f10b23",
                "model": "gpt-4o-mini",
                "name": "gpt-4o-mini",
                "progress": {
                  "error": 0,
                  "success": 95
                },
                "settings": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": {
                    "type": "text"
                  },
                  "seed": null,
                  "stop": [],
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": [],
                  "top_p": 1
                },
                "temp": 1
              },
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 22,
                "baseMethod": 34,
                "chunkMethod": 26,
                "docTitle": 18,
                "method": 36,
                "reference": 21,
                "signature": 53,
                "source_doc": 23
              },
              "prompt": 198,
              "text": 199,
              "uid": "c1255722-4c83-4cf2-9435-0f930f332397"
            },
            {
              "fill_history": {
                "baseMethod": 34,
                "chunkLibrary": 4,
                "chunkMethod": 26,
                "context": 57,
                "methodId": 92,
                "query": 9,
                "retrievalMethod": 36
              },
              "llm": {
                "base_model": "gpt-4",
                "emoji": "ðŸ¤–",
                "formData": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "model": "gpt-4o-mini",
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": "text",
                  "seed": null,
                  "shortname": "gpt-4o-mini",
                  "stop": "",
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": "",
                  "top_p": 1
                },
                "key": "5fce3844-27b9-41f9-9b81-447b08f10b23",
                "model": "gpt-4o-mini",
                "name": "gpt-4o-mini",
                "progress": {
                  "error": 0,
                  "success": 95
                },
                "settings": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": {
                    "type": "text"
                  },
                  "seed": null,
                  "stop": [],
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": [],
                  "top_p": 1
                },
                "temp": 1
              },
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 13,
                "baseMethod": 34,
                "chunkMethod": 26,
                "method": 36,
                "reference": 12,
                "signature": 53,
                "source_doc": 15
              },
              "prompt": 200,
              "text": 201,
              "uid": "f160b04e-581e-403b-8ab6-0eedd5ba80c1"
            },
            {
              "fill_history": {
                "baseMethod": 3,
                "chunkLibrary": 4,
                "chunkMethod": 26,
                "context": 91,
                "docTitle": 18,
                "embeddingModel": 7,
                "methodId": 93,
                "query": 19,
                "retrievalMethod": 10
              },
              "llm": {
                "base_model": "gpt-4",
                "emoji": "ðŸ¤–",
                "formData": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "model": "gpt-4o-mini",
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": "text",
                  "seed": null,
                  "shortname": "gpt-4o-mini",
                  "stop": "",
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": "",
                  "top_p": 1
                },
                "key": "5fce3844-27b9-41f9-9b81-447b08f10b23",
                "model": "gpt-4o-mini",
                "name": "gpt-4o-mini",
                "progress": {
                  "error": 0,
                  "success": 95
                },
                "settings": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": {
                    "type": "text"
                  },
                  "seed": null,
                  "stop": [],
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": [],
                  "top_p": 1
                },
                "temp": 1
              },
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 22,
                "baseMethod": 3,
                "chunkMethod": 26,
                "docTitle": 18,
                "embeddingModel": 7,
                "method": 10,
                "reference": 21,
                "signature": 31,
                "source_doc": 23
              },
              "prompt": 202,
              "text": 203,
              "uid": "d889ada5-20b4-4f5c-bee7-04ed3d0631f7"
            },
            {
              "fill_history": {
                "baseMethod": 34,
                "chunkLibrary": 4,
                "chunkMethod": 26,
                "context": 79,
                "methodId": 92,
                "query": 60,
                "retrievalMethod": 36
              },
              "llm": {
                "base_model": "gpt-4",
                "emoji": "ðŸ¤–",
                "formData": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "model": "gpt-4o-mini",
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": "text",
                  "seed": null,
                  "shortname": "gpt-4o-mini",
                  "stop": "",
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": "",
                  "top_p": 1
                },
                "key": "5fce3844-27b9-41f9-9b81-447b08f10b23",
                "model": "gpt-4o-mini",
                "name": "gpt-4o-mini",
                "progress": {
                  "error": 0,
                  "success": 95
                },
                "settings": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": {
                    "type": "text"
                  },
                  "seed": null,
                  "stop": [],
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": [],
                  "top_p": 1
                },
                "temp": 1
              },
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 63,
                "baseMethod": 34,
                "chunkMethod": 26,
                "method": 36,
                "reference": 62,
                "signature": 53,
                "source_doc": 64
              },
              "prompt": 204,
              "text": 205,
              "uid": "a002fd40-1f88-45d2-a290-23c4dcf09fc8"
            },
            {
              "fill_history": {
                "baseMethod": 3,
                "chunkLibrary": 4,
                "chunkMethod": 26,
                "context": 88,
                "embeddingModel": 7,
                "methodId": 93,
                "query": 60,
                "retrievalMethod": 10
              },
              "llm": {
                "base_model": "gpt-4",
                "emoji": "ðŸ¤–",
                "formData": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "model": "gpt-4o-mini",
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": "text",
                  "seed": null,
                  "shortname": "gpt-4o-mini",
                  "stop": "",
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": "",
                  "top_p": 1
                },
                "key": "5fce3844-27b9-41f9-9b81-447b08f10b23",
                "model": "gpt-4o-mini",
                "name": "gpt-4o-mini",
                "progress": {
                  "error": 0,
                  "success": 95
                },
                "settings": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": {
                    "type": "text"
                  },
                  "seed": null,
                  "stop": [],
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": [],
                  "top_p": 1
                },
                "temp": 1
              },
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 63,
                "baseMethod": 3,
                "chunkMethod": 26,
                "embeddingModel": 7,
                "method": 10,
                "reference": 62,
                "signature": 31,
                "source_doc": 64
              },
              "prompt": 206,
              "text": 207,
              "uid": "4c72e706-4a97-4cbf-ac51-8dd2140e80f9"
            },
            {
              "fill_history": {
                "baseMethod": 3,
                "chunkLibrary": 4,
                "chunkMethod": 26,
                "context": 85,
                "embeddingModel": 7,
                "methodId": 93,
                "query": 46,
                "retrievalMethod": 10
              },
              "llm": {
                "base_model": "gpt-4",
                "emoji": "ðŸ¤–",
                "formData": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "model": "gpt-4o-mini",
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": "text",
                  "seed": null,
                  "shortname": "gpt-4o-mini",
                  "stop": "",
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": "",
                  "top_p": 1
                },
                "key": "5fce3844-27b9-41f9-9b81-447b08f10b23",
                "model": "gpt-4o-mini",
                "name": "gpt-4o-mini",
                "progress": {
                  "error": 0,
                  "success": 95
                },
                "settings": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": {
                    "type": "text"
                  },
                  "seed": null,
                  "stop": [],
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": [],
                  "top_p": 1
                },
                "temp": 1
              },
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 49,
                "baseMethod": 3,
                "chunkMethod": 26,
                "embeddingModel": 7,
                "method": 10,
                "reference": 48,
                "signature": 31,
                "source_doc": 50
              },
              "prompt": 208,
              "text": 97,
              "uid": "4bc1d9ca-4e09-44bd-abff-5cfee5b00dee"
            },
            {
              "fill_history": {
                "baseMethod": 34,
                "chunkLibrary": 4,
                "chunkMethod": 26,
                "context": 82,
                "methodId": 92,
                "query": 46,
                "retrievalMethod": 36
              },
              "llm": {
                "base_model": "gpt-4",
                "emoji": "ðŸ¤–",
                "formData": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "model": "gpt-4o-mini",
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": "text",
                  "seed": null,
                  "shortname": "gpt-4o-mini",
                  "stop": "",
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": "",
                  "top_p": 1
                },
                "key": "5fce3844-27b9-41f9-9b81-447b08f10b23",
                "model": "gpt-4o-mini",
                "name": "gpt-4o-mini",
                "progress": {
                  "error": 0,
                  "success": 95
                },
                "settings": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": {
                    "type": "text"
                  },
                  "seed": null,
                  "stop": [],
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": [],
                  "top_p": 1
                },
                "temp": 1
              },
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 49,
                "baseMethod": 34,
                "chunkMethod": 26,
                "method": 36,
                "reference": 48,
                "signature": 53,
                "source_doc": 50
              },
              "prompt": 209,
              "text": 210,
              "uid": "2bb32e10-1e24-4b51-9f87-ed1cccbb90aa"
            },
            {
              "fill_history": {
                "baseMethod": 3,
                "chunkLibrary": 4,
                "chunkMethod": 5,
                "context": 73,
                "docTitle": 72,
                "embeddingModel": 7,
                "methodId": 93,
                "query": 46,
                "retrievalMethod": 10
              },
              "llm": {
                "base_model": "gpt-4",
                "emoji": "ðŸ¤–",
                "formData": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "model": "gpt-4o-mini",
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": "text",
                  "seed": null,
                  "shortname": "gpt-4o-mini",
                  "stop": "",
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": "",
                  "top_p": 1
                },
                "key": "5fce3844-27b9-41f9-9b81-447b08f10b23",
                "model": "gpt-4o-mini",
                "name": "gpt-4o-mini",
                "progress": {
                  "error": 0,
                  "success": 95
                },
                "settings": {
                  "frequency_penalty": 0,
                  "logit_bias": null,
                  "max_completion_tokens": null,
                  "parallel_tool_calls": true,
                  "presence_penalty": 0,
                  "response_format": {
                    "type": "text"
                  },
                  "seed": null,
                  "stop": [],
                  "system_msg": "You are a helpful assistant.",
                  "temperature": 0,
                  "tool_choice": "",
                  "tools": [],
                  "top_p": 1
                },
                "temp": 1
              },
              "metavars": {
                "LLM_0": "gpt-4o-mini",
                "answer_context": 49,
                "baseMethod": 3,
                "chunkMethod": 5,
                "docTitle": 72,
                "embeddingModel": 7,
                "method": 10,
                "reference": 48,
                "signature": 14,
                "source_doc": 50
              },
              "prompt": 211,
              "text": 212,
              "uid": "64bdd041-de19-4e64-a248-368138910621"
            }
          ],
          "llms": [
            {
              "base_model": "gpt-4",
              "emoji": "ðŸ¤–",
              "formData": {
                "frequency_penalty": 0,
                "logit_bias": null,
                "max_completion_tokens": null,
                "model": "gpt-4o-mini",
                "parallel_tool_calls": true,
                "presence_penalty": 0,
                "response_format": "text",
                "seed": null,
                "shortname": "gpt-4o-mini",
                "stop": "",
                "system_msg": "You are a helpful assistant.",
                "temperature": 0,
                "tool_choice": "",
                "tools": "",
                "top_p": 1
              },
              "key": "5fce3844-27b9-41f9-9b81-447b08f10b23",
              "model": "gpt-4o-mini",
              "name": "gpt-4o-mini",
              "settings": {
                "frequency_penalty": 0,
                "logit_bias": null,
                "max_completion_tokens": null,
                "parallel_tool_calls": true,
                "presence_penalty": 0,
                "response_format": {
                  "type": "text"
                },
                "seed": null,
                "stop": [],
                "system_msg": "You are a helpful assistant.",
                "temperature": 0,
                "tool_choice": "",
                "tools": [],
                "top_p": 1
              },
              "temp": 1
            }
          ],
          "prompt": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: {#query}\nContext: {context}\nAnswer: ",
          "refresh": false
        },
        "dragging": false,
        "id": "promptNode-d95a71d4-5df6-4689-9bb1-f18b2f9049d3",
        "position": {
          "x": 16,
          "y": 592
        },
        "positionAbsolute": {
          "x": 16,
          "y": 592
        },
        "selected": false,
        "type": "prompt"
      },
      {
        "width": 722,
        "height": 608,
        "data": {
          "input": "multieval-7beb361f-660e-4a12-a370-1c2bb8cb0818",
          "llm_groups": [
            {
              "label": "LLMs (last)",
              "value": "LLM"
            },
            {
              "label": "LLMs #NaN",
              "value": "answer_context"
            },
            {
              "label": "LLMs #NaN",
              "value": "baseMethod"
            },
            {
              "label": "LLMs #NaN",
              "value": "chunkMethod"
            },
            {
              "label": "LLMs #NaN",
              "value": "method"
            },
            {
              "label": "LLMs #NaN",
              "value": "reference"
            },
            {
              "label": "LLMs #NaN",
              "value": "signature"
            },
            {
              "label": "LLMs #NaN",
              "value": "source_doc"
            },
            {
              "label": "LLMs #NaN",
              "value": "docTitle"
            },
            {
              "label": "LLMs #NaN",
              "value": "embeddingModel"
            }
          ],
          "refresh": false,
          "selected_llm_group": "LLM",
          "selected_vars": [
            "__meta_signature"
          ],
          "vars": [
            {
              "label": "LLM (default)",
              "value": "LLM (default)"
            },
            {
              "label": "baseMethod",
              "value": "baseMethod"
            },
            {
              "label": "chunkLibrary",
              "value": "chunkLibrary"
            },
            {
              "label": "chunkMethod",
              "value": "chunkMethod"
            },
            {
              "label": "methodId",
              "value": "methodId"
            },
            {
              "label": "query",
              "value": "query"
            },
            {
              "label": "retrievalMethod",
              "value": "retrievalMethod"
            },
            {
              "label": "context",
              "value": "context"
            },
            {
              "label": "docTitle",
              "value": "docTitle"
            },
            {
              "label": "embeddingModel",
              "value": "embeddingModel"
            },
            {
              "label": "answer_context (meta)",
              "value": "__meta_answer_context"
            },
            {
              "label": "baseMethod (meta)",
              "value": "__meta_baseMethod"
            },
            {
              "label": "chunkMethod (meta)",
              "value": "__meta_chunkMethod"
            },
            {
              "label": "method (meta)",
              "value": "__meta_method"
            },
            {
              "label": "reference (meta)",
              "value": "__meta_reference"
            },
            {
              "label": "signature (meta)",
              "value": "__meta_signature"
            },
            {
              "label": "source_doc (meta)",
              "value": "__meta_source_doc"
            },
            {
              "label": "docTitle (meta)",
              "value": "__meta_docTitle"
            },
            {
              "label": "embeddingModel (meta)",
              "value": "__meta_embeddingModel"
            }
          ]
        },
        "dragging": false,
        "id": "visNode-33c183d7-6baf-4954-a2a4-e06ce50add74",
        "position": {
          "x": 1104,
          "y": 512
        },
        "positionAbsolute": {
          "x": 1104,
          "y": 512
        },
        "selected": false,
        "type": "vis"
      },
      {
        "width": 480,
        "height": 296,
        "data": {
          "fields": [
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "prompt": "",
              "text": "Gradio Demo: blocks_multiple_event_triggers\r\n\r\n\r\n```\r\n!pip install -q gradio plotly pypistats\r\n```\r\n\r\n\r\n```\r\nimport gradio as gr\r\nimport pypistats\r\nfrom datetime import date"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "prompt": "",
              "text": "import pypistats\r\nfrom datetime import date\r\nfrom dateutil.relativedelta import relativedelta\r\nimport pandas as pd\r\n\r\ndef get_plot(lib, time):"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "prompt": "",
              "text": "import pandas as pd\r\n\r\ndef get_plot(lib, time):\r\n    data = pypistats.overall(lib, total=True, format=\"pandas\")\r\n    data = data.groupby(\"category\").get_group(\"with_mirrors\").sort_values(\"date\")"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_3_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_3_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "prompt": "",
              "text": "start_date = date.today() - relativedelta(months=int(time.split(\" \")[0]))\r\n    data = data[(data['date'] > str(start_date))]\r\n    data.date = pd.to_datetime(pd.to_datetime(data.date))"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "prompt": "",
              "text": "return gr.LinePlot(value=data, x=\"date\", y=\"downloads\",\r\n                              tooltip=['date', 'downloads'],"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "prompt": "",
              "text": "title=f\"Pypi downloads of {lib} over last {time}\",\r\n                              overlay_point=True,\r\n                              height=400,"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "prompt": "",
              "text": "height=400,\r\n                              width=900)\r\n\r\n\r\nwith gr.Blocks() as demo:\r\n    gr.Markdown(\r\n        \"\"\"\r\n        ## Pypi Download Stats ðŸ“ˆ"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "prompt": "",
              "text": "\"\"\"\r\n        ## Pypi Download Stats ðŸ“ˆ\r\n        See live download stats for all of Hugging Face's open-source libraries ðŸ¤—\r\n        \"\"\")\r\n    with gr.Row():"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "prompt": "",
              "text": "\"\"\")\r\n    with gr.Row():\r\n        lib = gr.Dropdown([\"transformers\", \"datasets\", \"huggingface-hub\", \"gradio\", \"accelerate\"],\r\n                          value=\"gradio\", label=\"Library\")"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "prompt": "",
              "text": "time = gr.Dropdown([\"3 months\", \"6 months\", \"9 months\", \"12 months\"],\r\n                           value=\"3 months\", label=\"Downloads over the last...\")\r\n\r\n    plt = gr.LinePlot()"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "prompt": "",
              "text": "plt = gr.LinePlot()\r\n    # You can add multiple event triggers in 2 lines like this\r\n    for event in [lib.change, time.change, demo.load]:\r\n        event(get_plot, [lib, time], [plt])"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_11_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_11_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "prompt": "",
              "text": "event(get_plot, [lib, time], [plt])\r\n\r\nif __name__ == \"__main__\":\r\n    demo.launch()\r\n\r\n```"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "prompt": "",
              "text": "Contributing a Guide\r\n\r\nWant to help teach Gradio? Consider contributing a Guide! ðŸ¤—\r\n\r\nBroadly speaking, there are two types of guides:"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "prompt": "",
              "text": "- **Use cases**: guides that cover step-by-step how to build a particular type of machine learning demo or app using Gradio. Here's an example: [_Creating a"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "prompt": "",
              "text": "app using Gradio. Here's an example: [_Creating a Chatbot_](https://github.com/gradio-app/gradio/blob/master/guides/creating_a_chatbot.md)"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_3_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_3_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "prompt": "",
              "text": "- **Feature explanation**: guides that describe in detail a particular feature of Gradio. Here's an example: [_Using"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "prompt": "",
              "text": "feature of Gradio. Here's an example: [_Using Flagging_](https://github.com/gradio-app/gradio/blob/master/guides/using_flagging.md)"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "prompt": "",
              "text": "We encourage you to submit either type of Guide! (Looking for ideas? We may also have open [issues](https://github.com/gradio-app/gradio/issues?q=is%3Aopen+is%3Aissue+label%3Aguides) where users have"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "prompt": "",
              "text": "where users have asked for guides on particular topics)"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "prompt": "",
              "text": "## Guide Structure\r\n\r\nAs you can see with the previous examples, Guides are standard markdown documents. They usually:\r\n\r\n- start with an Introduction section describing the topic"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "prompt": "",
              "text": "- include subheadings to make articles easy to navigate\r\n- include real code snippets that make it easy to follow along and implement the Guide"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "prompt": "",
              "text": "- include embedded Gradio demos to make them more interactive and provide immediate demonstrations of the topic being discussed. These Gradio demos are hosted on [Hugging Face"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "prompt": "",
              "text": "These Gradio demos are hosted on [Hugging Face Spaces](https://huggingface.co/spaces) and are embedded using the standard \\<iframe\\> tag."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_11_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_11_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "prompt": "",
              "text": "## How to Contribute a Guide\r\n\r\n1. Clone or fork this `gradio` repo\r\n2. Add a new markdown document with a descriptive title to the `/guides` folder"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_12_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_12_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "prompt": "",
              "text": "3. Write your Guide in standard markdown! Embed Gradio demos wherever helpful"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_13_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_13_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "prompt": "",
              "text": "4. Add a list of `related_spaces` at the top of the markdown document (see the previously linked Guides for how to do this)"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_14_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_14_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "prompt": "",
              "text": "5. Add 3 `tags` at the top of the markdown document to help users find your guide (again, see the previously linked Guides for how to do this)\r\n6. Open a PR to have your guide reviewed"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "prompt": "",
              "text": "6. Open a PR to have your guide reviewed\r\n\r\nThat's it! We're looking forward to reading your Guide ðŸ¥³"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "@gradio/gallery\r\n\r\n## 0.4.14\r\n\r\n### Patch Changes"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Updated dependencies [[`828fb9e`](https://github.com/gradio-app/gradio/commit/828fb9e6ce15b6ea08318675a2361117596a1b5d),"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "[`73268ee`](https://github.com/gradio-app/gradio/commit/73268ee2e39f23ebdd1e927cb49b8d79c4b9a144)]:"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_3_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_3_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- @gradio/client@0.9.3\r\n  - @gradio/statustracker@0.4.3\r\n  - @gradio/atoms@0.4.1\r\n  - @gradio/image@0.5.3\r\n  - @gradio/upload@0.5.6\r\n\r\n## 0.4.13\r\n\r\n### Patch Changes"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "## 0.4.13\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`245d58e`](https://github.com/gradio-app/gradio/commit/245d58eff788e8d44a59d37a2d9b26d0f08a62b4)]:\r\n  - @gradio/client@0.9.2"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- @gradio/client@0.9.2\r\n  - @gradio/image@0.5.2\r\n  - @gradio/upload@0.5.5\r\n\r\n## 0.4.12\r\n\r\n### Patch Changes"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Updated dependencies [[`5d51fbc`](https://github.com/gradio-app/gradio/commit/5d51fbce7826da840a2fd4940feb5d9ad6f1bc5a),"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "[`34f9431`](https://github.com/gradio-app/gradio/commit/34f943101bf7dd6b8a8974a6131c1ed7c4a0dac0)]:"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- @gradio/upload@0.5.4\r\n  - @gradio/client@0.9.1\r\n  - @gradio/image@0.5.1\r\n\r\n## 0.4.11\r\n\r\n### Fixes"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#6487](https://github.com/gradio-app/gradio/pull/6487) [`9a5811d`](https://github.com/gradio-app/gradio/commit/9a5811df9218b622af59ba243a937a9c36ba00f9) - Fix the download button of the"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Fix the download button of the `gr.Gallery()` component to work. Thanks [@whitphx](https://github.com/whitphx)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_11_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_11_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "## 0.4.10\r\n\r\n### Patch Changes"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_12_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_12_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Updated dependencies [[`b639e04`](https://github.com/gradio-app/gradio/commit/b639e040741e6c0d9104271c81415d7befbd8cf3),"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_13_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_13_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "[`206af31`](https://github.com/gradio-app/gradio/commit/206af31d7c1a31013364a44e9b40cf8df304ba50)]:"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_14_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_14_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- @gradio/image@0.4.2\r\n  - @gradio/icons@0.3.1\r\n  - @gradio/atoms@0.3.1\r\n  - @gradio/statustracker@0.4.1\r\n  - @gradio/upload@0.5.2\r\n\r\n## 0.4.9\r\n\r\n### Patch Changes"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "## 0.4.9\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`71f1a1f99`](https://github.com/gradio-app/gradio/commit/71f1a1f9931489d465c2c1302a5c8d768a3cd23a)]:\r\n  - @gradio/client@0.8.2"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_16_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_16_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- @gradio/client@0.8.2\r\n  - @gradio/image@0.4.1\r\n  - @gradio/upload@0.5.1\r\n\r\n## 0.4.8\r\n\r\n### Patch Changes"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_17_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_17_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "## 0.4.8\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`9caddc17b`](https://github.com/gradio-app/gradio/commit/9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8)]:\r\n  - @gradio/atoms@0.3.0"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_18_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_18_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- @gradio/atoms@0.3.0\r\n  - @gradio/icons@0.3.0\r\n  - @gradio/image@0.4.0\r\n  - @gradio/statustracker@0.4.0\r\n  - @gradio/upload@0.5.0\r\n\r\n## 0.4.7\r\n\r\n### Patch Changes"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_19_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_19_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Updated dependencies [[`2f805a7dd`](https://github.com/gradio-app/gradio/commit/2f805a7dd3d2b64b098f659dadd5d01258290521),"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_20_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_20_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "[`f816136a0`](https://github.com/gradio-app/gradio/commit/f816136a039fa6011be9c4fb14f573e4050a681a)]:"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_21_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_21_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- @gradio/image@0.3.6\r\n  - @gradio/upload@0.4.2\r\n  - @gradio/atoms@0.2.2\r\n  - @gradio/icons@0.2.1\r\n  - @gradio/statustracker@0.3.2\r\n\r\n## 0.4.6\r\n\r\n### Patch Changes"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_22_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_22_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "## 0.4.6\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`324867f63`](https://github.com/gradio-app/gradio/commit/324867f63c920113d89a565892aa596cf8b1e486)]:\r\n  - @gradio/client@0.8.1"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_23_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_23_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- @gradio/client@0.8.1\r\n  - @gradio/image@0.3.5\r\n  - @gradio/upload@0.4.1\r\n\r\n## 0.4.5\r\n\r\n### Patch Changes"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_24_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_24_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Updated dependencies [[`6204ccac5`](https://github.com/gradio-app/gradio/commit/6204ccac5967763e0ebde550d04d12584243a120),"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_25_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_25_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "[`4d3aad33a`](https://github.com/gradio-app/gradio/commit/4d3aad33a0b66639dbbb2928f305a79fb7789b2d),"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_26_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_26_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "[`854b482f5`](https://github.com/gradio-app/gradio/commit/854b482f598e0dc47673846631643c079576da9c),"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_27_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_27_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "[`f1409f95e`](https://github.com/gradio-app/gradio/commit/f1409f95ed39c5565bed6a601e41f94e30196a57)]:"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_28_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_28_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- @gradio/image@0.3.4\r\n  - @gradio/upload@0.4.0\r\n  - @gradio/client@0.8.0\r\n\r\n## 0.4.4\r\n\r\n### Patch Changes"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_29_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_29_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Updated dependencies [[`bca6c2c80`](https://github.com/gradio-app/gradio/commit/bca6c2c80f7e5062427019de45c282238388af95),"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_30_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_30_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "[`3cdeabc68`](https://github.com/gradio-app/gradio/commit/3cdeabc6843000310e1a9e1d17190ecbf3bbc780),"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_31_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_31_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "[`fad92c29d`](https://github.com/gradio-app/gradio/commit/fad92c29dc1f5cd84341aae417c495b33e01245f)]:"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_32_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_32_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- @gradio/client@0.7.2\r\n  - @gradio/atoms@0.2.1\r\n  - @gradio/upload@0.3.3\r\n  - @gradio/image@0.3.3\r\n  - @gradio/statustracker@0.3.1\r\n\r\n## 0.4.3\r\n\r\n### Fixes"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_33_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_33_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#6288](https://github.com/gradio-app/gradio/pull/6288) [`92278729e`](https://github.com/gradio-app/gradio/commit/92278729ee008126af15ffe6be399236211b2f34) - Gallery preview fix and optionally skip"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_34_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_34_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Gallery preview fix and optionally skip download of urls in postprcess. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_35_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_35_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "## 0.4.2\r\n\r\n### Fixes"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_36_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_36_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#6277](https://github.com/gradio-app/gradio/pull/6277) [`5fe091367`](https://github.com/gradio-app/gradio/commit/5fe091367fbe0eecdd504aa734ca1c70b0621f52) - handle selected_index prop change for"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_37_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_37_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- handle selected_index prop change for gallery. Thanks [@pngwn](https://github.com/pngwn)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_38_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_38_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "## 0.4.1\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`2ba14b284`](https://github.com/gradio-app/gradio/commit/2ba14b284f908aa13859f4337167a157075a68eb)]:\r\n  - @gradio/client@0.7.1"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_39_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_39_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- @gradio/client@0.7.1\r\n  - @gradio/image@0.3.1\r\n  - @gradio/upload@0.3.1\r\n\r\n## 0.4.0\r\n\r\n### Features"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_40_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_40_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - fix circular dependency with client +"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_41_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_41_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- fix circular dependency with client + upload. Thanks [@pngwn](https://github.com/pngwn)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_42_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_42_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Clean root url. Thanks"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_43_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_43_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Clean root url. Thanks [@pngwn](https://github.com/pngwn)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_44_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_44_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Image v4. Thanks"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_45_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_45_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Image v4. Thanks [@pngwn](https://github.com/pngwn)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_46_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_46_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Publish all components to npm. Thanks"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_47_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_47_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Publish all components to npm. Thanks [@pngwn](https://github.com/pngwn)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_48_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_48_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Custom components. Thanks"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_49_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_49_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Custom components. Thanks [@pngwn](https://github.com/pngwn)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_50_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_50_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "## 0.4.0-beta.9\r\n\r\n### Features"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_51_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_51_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#6143](https://github.com/gradio-app/gradio/pull/6143) [`e4f7b4b40`](https://github.com/gradio-app/gradio/commit/e4f7b4b409323b01aa01b39e15ce6139e29aa073) - fix circular dependency with client +"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_52_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_52_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- fix circular dependency with client + upload. Thanks [@pngwn](https://github.com/pngwn)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_53_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_53_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#6136](https://github.com/gradio-app/gradio/pull/6136) [`667802a6c`](https://github.com/gradio-app/gradio/commit/667802a6cdbfb2ce454a3be5a78e0990b194548a) - JS Component Documentation. Thanks"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_54_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_54_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- JS Component Documentation. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_55_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_55_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#6094](https://github.com/gradio-app/gradio/pull/6094) [`c476bd5a5`](https://github.com/gradio-app/gradio/commit/c476bd5a5b70836163b9c69bf4bfe068b17fbe13) - Image v4. Thanks"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_56_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_56_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Image v4. Thanks [@pngwn](https://github.com/pngwn)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_57_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_57_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "## 0.4.0-beta.8\r\n\r\n### Features"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_58_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_58_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676`](https://github.com/gradio-app/gradio/commit/83e947676d327ca2ab6ae2a2d710c78961c771a0) - Format js in v4 branch. Thanks"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_59_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_59_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Format js in v4 branch. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_60_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_60_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "### Fixes"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_61_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_61_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_62_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_62_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- fix tests. Thanks [@pngwn](https://github.com/pngwn)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_63_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_63_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "## 0.4.0-beta.7\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies []:\r\n  - @gradio/image@0.3.0-beta.7\r\n\r\n## 0.4.0-beta.6\r\n\r\n### Features"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_64_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_64_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f`](https://github.com/gradio-app/gradio/commit/319c30f3fccf23bfe1da6c9b132a6a99d59652f7) - rererefactor frontend files. Thanks"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_65_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_65_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- rererefactor frontend files. Thanks [@pngwn](https://github.com/pngwn)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_66_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_66_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#5938](https://github.com/gradio-app/gradio/pull/5938) [`13ed8a485`](https://github.com/gradio-app/gradio/commit/13ed8a485d5e31d7d75af87fe8654b661edcca93) - V4: Use beta release versions for"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_67_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_67_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- V4: Use beta release versions for '@gradio' packages. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_68_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_68_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "## 0.5.1\r\n\r\n### Patch Changes"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_69_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_69_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Updated dependencies [[`b67115e8e`](https://github.com/gradio-app/gradio/commit/b67115e8e6e489fffd5271ea830211863241ddc5),"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_70_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_70_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "[`8d909624f`](https://github.com/gradio-app/gradio/commit/8d909624f61a49536e3c0f71cb2d9efe91216219),"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_71_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_71_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "[`e70805d54`](https://github.com/gradio-app/gradio/commit/e70805d54cc792452545f5d8eccc1aa0212a4695)]:"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_72_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_72_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- @gradio/image@0.4.0\r\n  - @gradio/atoms@0.2.0\r\n  - @gradio/statustracker@0.2.3\r\n  - @gradio/upload@0.3.3\r\n\r\n## 0.5.0\r\n\r\n### Features"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_73_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_73_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#5780](https://github.com/gradio-app/gradio/pull/5780) [`ed0f9a21b`](https://github.com/gradio-app/gradio/commit/ed0f9a21b04ad6b941b63d2ce45100dbd1abd5c5) - Adds `change()` event to `gr.Gallery`."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_74_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_74_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Adds `change()` event to `gr.Gallery`. Thanks [@abidlabs](https://github.com/abidlabs)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_75_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_75_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#5783](https://github.com/gradio-app/gradio/pull/5783) [`4567788bd`](https://github.com/gradio-app/gradio/commit/4567788bd1fc25df9322902ba748012e392b520a) - Adds the ability to set the"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_76_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_76_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Adds the ability to set the `selected_index` in a `gr.Gallery`. Thanks [@abidlabs](https://github.com/abidlabs)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_77_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_77_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "### Fixes"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_78_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_78_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#5798](https://github.com/gradio-app/gradio/pull/5798) [`a0d3cc45c`](https://github.com/gradio-app/gradio/commit/a0d3cc45c6db48dc0db423c229b8fb285623cdc4) - Fix `gr.SelectData` so that the target"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_79_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_79_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Fix `gr.SelectData` so that the target attribute is correctly attached, and the filedata is included in the data attribute with `gr.Gallery`. Thanks [@abidlabs](https://github.com/abidlabs)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_80_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_80_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "## 0.4.1\r\n\r\n### Fixes"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_81_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_81_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#5735](https://github.com/gradio-app/gradio/pull/5735) [`abb5e9df4`](https://github.com/gradio-app/gradio/commit/abb5e9df47989b2c56c2c312d74944678f9f2d4e) - Ensure images with no caption download"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_82_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_82_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Ensure images with no caption download in gallery. Thanks [@hannahblair](https://github.com/hannahblair)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_83_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_83_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#5754](https://github.com/gradio-app/gradio/pull/5754) [`502054848`](https://github.com/gradio-app/gradio/commit/502054848fdbe39fc03ec42445242b4e49b7affc) - Fix Gallery `columns` and `rows`"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_84_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_84_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Fix Gallery `columns` and `rows` params. Thanks [@abidlabs](https://github.com/abidlabs)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_85_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_85_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "## 0.4.0\r\n\r\n### Features"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_86_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_86_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#5554](https://github.com/gradio-app/gradio/pull/5554) [`75ddeb390`](https://github.com/gradio-app/gradio/commit/75ddeb390d665d4484667390a97442081b49a423) - Accessibility Improvements. Thanks"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_87_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_87_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Accessibility Improvements. Thanks [@hannahblair](https://github.com/hannahblair)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_88_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_88_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "## 0.3.4\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`e0d61b8ba`](https://github.com/gradio-app/gradio/commit/e0d61b8baa0f6293f53b9bdb1647d42f9ae2583a)]:\r\n  - @gradio/image@0.2.4\r\n\r\n## 0.3.3"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_89_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_89_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- @gradio/image@0.2.4\r\n\r\n## 0.3.3\r\n\r\n### Fixes"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_90_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_90_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#5528](https://github.com/gradio-app/gradio/pull/5528) [`dc86e4a7`](https://github.com/gradio-app/gradio/commit/dc86e4a7e1c40b910c74558e6f88fddf9b3292bc) - Lazy load all images. Thanks"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_91_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_91_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Lazy load all images. Thanks [@aliabid94](https://github.com/aliabid94)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_92_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_92_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "## 0.3.2\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`afac0006`](https://github.com/gradio-app/gradio/commit/afac0006337ce2840cf497cd65691f2f60ee5912)]:\r\n  - @gradio/statustracker@0.2.0"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_93_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_93_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- @gradio/statustracker@0.2.0\r\n  - @gradio/image@0.2.2\r\n  - @gradio/utils@0.1.1\r\n  - @gradio/atoms@0.1.2\r\n  - @gradio/upload@0.2.1\r\n\r\n## 0.3.1\r\n\r\n### Patch Changes"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_94_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_94_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Updated dependencies [[`abf1c57d`](https://github.com/gradio-app/gradio/commit/abf1c57d7d85de0df233ee3b38aeb38b638477db),"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_95_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_95_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "[`79d8f9d8`](https://github.com/gradio-app/gradio/commit/79d8f9d891901683c5a1b7486efb44eab2478c96)]:"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_96_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_96_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- @gradio/icons@0.1.0\r\n  - @gradio/utils@0.1.0\r\n  - @gradio/upload@0.2.0\r\n  - @gradio/atoms@0.1.1\r\n  - @gradio/image@0.2.1\r\n  - @gradio/statustracker@0.1.1\r\n\r\n## 0.3.0\r\n\r\n### Highlights"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_97_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_97_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "#### Improve startup performance and markdown support ([#5279](https://github.com/gradio-app/gradio/pull/5279)"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_98_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_98_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "[`fe057300`](https://github.com/gradio-app/gradio/commit/fe057300f0672c62dab9d9b4501054ac5d45a4ec))"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_99_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_99_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "##### Improved markdown support"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_100_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_100_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "We now have better support for markdown in `gr.Markdown` and `gr.Dataframe`. Including syntax highlighting and Github Flavoured Markdown. We also have more consistent markdown behaviour and styling."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_101_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_101_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "##### Various performance improvements\r\n\r\nThese improvements will be particularly beneficial to large applications."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_102_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_102_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_103_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_103_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "introduced in a recent version of Gradio. App startup for large applications is now around twice as fast."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_104_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_104_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).\r\n- Corrected an issue that was causing markdown to re-render infinitely."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_105_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_105_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Ensured that the `gr.3DModel` does re-render prematurely.\r\n\r\nThanks [@pngwn](https://github.com/pngwn)!\r\n\r\n### Features"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_106_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_106_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#5215](https://github.com/gradio-app/gradio/pull/5215) [`fbdad78a`](https://github.com/gradio-app/gradio/commit/fbdad78af4c47454cbb570f88cc14bf4479bbceb) - Lazy load interactive or static variants"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_107_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_107_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Lazy load interactive or static variants of a component individually, rather than loading both variants regardless. This change will improve performance for many applications. Thanks"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_108_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_108_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "improve performance for many applications. Thanks [@pngwn](https://github.com/pngwn)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_109_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_109_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#5216](https://github.com/gradio-app/gradio/pull/5216) [`4b58ea6d`](https://github.com/gradio-app/gradio/commit/4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8) - Update i18n tokens and locale files."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_110_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_110_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Update i18n tokens and locale files. Thanks [@hannahblair](https://github.com/hannahblair)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_111_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_111_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "## 0.2.0\r\n\r\n### Features"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_112_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_112_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#5025](https://github.com/gradio-app/gradio/pull/5025) [`6693660a`](https://github.com/gradio-app/gradio/commit/6693660a790996f8f481feaf22a8c49130d52d89) - Add download button to selected images"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_113_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_113_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Add download button to selected images in `Gallery`. Thanks [@hannahblair](https://github.com/hannahblair)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_114_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_114_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "## 0.1.0\r\n\r\n### Features"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_115_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_115_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- [#4995](https://github.com/gradio-app/gradio/pull/4995) [`3f8c210b`](https://github.com/gradio-app/gradio/commit/3f8c210b01ef1ceaaf8ee73be4bf246b5b745bbf) - Implement left and right click in"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_116_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_116_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- Implement left and right click in `Gallery` component and show implicit images in `Gallery` grid. Thanks [@hannahblair](https://github.com/hannahblair)!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_3_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_3_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage,"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_11_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_11_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_12_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_12_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_13_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_13_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_14_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_14_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_16_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_16_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_17_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_17_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_18_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_18_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_19_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_19_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_20_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_20_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_21_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_21_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_22_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_22_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_23_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_23_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_24_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_24_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_25_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_25_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_26_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_26_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_27_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_27_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "(index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_28_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_28_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "prompt": "",
              "text": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "prompt": "",
              "text": "the License. You may obtain a copy of the License at\r\n\r\nhttp://www.apache.org/licenses/LICENSE-2.0"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "prompt": "",
              "text": "http://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_3_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_3_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "prompt": "",
              "text": "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\r\nspecific language governing permissions and limitations under the License.\r\n-->"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "prompt": "",
              "text": "-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "prompt": "",
              "text": "2. use multiple pipelines in your workflow"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "prompt": "",
              "text": "Based on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "prompt": "",
              "text": "To seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "prompt": "",
              "text": "```py\r\nfrom diffusers import AutoPipelineForText2Image\r\nimport torch\r\n\r\npipeline = AutoPipelineForText2Image.from_pretrained("
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "prompt": "",
              "text": "\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\r\n).to(\"cuda\")\r\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\""
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "prompt": "",
              "text": "image = pipeline(prompt, num_inference_steps=25).images[0]\r\n```\r\n\r\n<Tip>\r\n\r\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\r\n\r\n</Tip>"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_11_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_11_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "prompt": "",
              "text": "</Tip>\r\n\r\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\r\n\r\n- [Stable Diffusion](./stable_diffusion/overview)"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_12_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_12_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "prompt": "",
              "text": "- [ControlNet](./controlnet)\r\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\r\n- [DeepFloyd IF](./deepfloyd_if)\r\n- [Kandinsky 2.1](./kandinsky)"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_13_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_13_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "prompt": "",
              "text": "- [Kandinsky 2.1](./kandinsky)\r\n- [Kandinsky 2.2](./kandinsky_v22)\r\n\r\n\r\n## AutoPipelineForText2Image\r\n\r\n[[autodoc]] AutoPipelineForText2Image\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_14_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_14_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "prompt": "",
              "text": "- all\r\n\t- from_pretrained\r\n\t- from_pipe\r\n\r\n## AutoPipelineForImage2Image\r\n\r\n[[autodoc]] AutoPipelineForImage2Image\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe\r\n\r\n## AutoPipelineForInpainting"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "prompt": "",
              "text": "- from_pipe\r\n\r\n## AutoPipelineForInpainting\r\n\r\n[[autodoc]] AutoPipelineForInpainting\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "the License. You may obtain a copy of the License at\r\n\r\nhttp://www.apache.org/licenses/LICENSE-2.0"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "http://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_3_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_3_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\r\nspecific language governing permissions and limitations under the License.\r\n-->"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "-->\r\n\r\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\r\n\r\n## Preamble"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "## Preamble\r\n\r\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "Given its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "The risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "We will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\r\n\r\n\r\n## Scope"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_11_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_11_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "The Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_12_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_12_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "the contributions, especially concerning sensitive topics related to ethical concerns."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_13_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_13_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "## Ethical guidelines"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_14_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_14_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "The following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_16_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_16_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_17_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_17_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_18_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_18_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_19_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_19_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_20_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_20_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "makes research artifacts more accessible to the community."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_21_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_21_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_22_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_22_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_23_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_23_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "## Examples of implementations: Safety features and Mechanisms"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_24_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_24_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "The team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_25_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_25_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_26_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_26_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_27_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_27_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_28_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_28_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_29_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_29_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "- **Encouraging safety in deployment**"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_30_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_30_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "- [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_31_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_31_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion:"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_32_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_32_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105)."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_33_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_33_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "- [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_34_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_34_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_35_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_35_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "The harmful concepts are intentionally hidden to prevent reverse engineering of the checker."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_36_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_36_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_37_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_37_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "an intermediary step that allows the repositoryâ€™s authors to have more control over its use."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_38_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_38_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_src_diffusers_experimental_README.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_src_diffusers_experimental_README.md.txt"
              },
              "prompt": "",
              "text": "ðŸ§¨ Diffusers Experimental\r\n\r\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\r\nCurrently, the following experiments are supported:"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_src_diffusers_experimental_README.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_diffusers_blob_main_src_diffusers_experimental_README.md.txt"
              },
              "prompt": "",
              "text": "* Reinforcement learning via an implementation of the [Diffuser](https://arxiv.org/abs/2205.09991) model."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\r\nrendered properly in your Markdown viewer.\r\n-->\r\n\r\n# Environment variables"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "-->\r\n\r\n# Environment variables\r\n\r\n`huggingface_hub` can be configured using environment variables.\r\n\r\nIf you are unfamiliar with environment variable, here are generic articles about them"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "[on macOS and Linux](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/)\r\nand on [Windows](https://phoenixnap.com/kb/windows-set-environment-variable)."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_3_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_3_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "This page will guide you through all environment variables specific to `huggingface_hub`\r\nand their meaning.\r\n\r\n## Generic\r\n\r\n### HF_INFERENCE_ENDPOINT"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "## Generic\r\n\r\n### HF_INFERENCE_ENDPOINT\r\n\r\nTo configure the inference api base url. You might want to set this variable if your organization"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "is pointing at an API Gateway rather than directly at the inference api.\r\n\r\nDefaults to `\"https://api-inference.huggingface.com\"`.\r\n\r\n### HF_HOME"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "### HF_HOME\r\n\r\nTo configure where `huggingface_hub` will locally store data. In particular, your token\r\nand the cache will be stored in this folder."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "and the cache will be stored in this folder.\r\n\r\nDefaults to `\"~/.cache/huggingface\"` unless [XDG_CACHE_HOME](#xdgcachehome) is set.\r\n\r\n### HF_HUB_CACHE"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "### HF_HUB_CACHE\r\n\r\nTo configure where repositories from the Hub will be cached locally (models, datasets and\r\nspaces)."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "spaces).\r\n\r\nDefaults to `\"$HF_HOME/hub\"` (e.g. `\"~/.cache/huggingface/hub\"` by default).\r\n\r\n### HF_ASSETS_CACHE"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "### HF_ASSETS_CACHE\r\n\r\nTo configure where [assets](../guides/manage-cache#caching-assets) created by downstream libraries"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_11_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_11_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "will be cached locally. Those assets can be preprocessed data, files downloaded from GitHub,\r\nlogs,...\r\n\r\nDefaults to `\"$HF_HOME/assets\"` (e.g. `\"~/.cache/huggingface/assets\"` by default)."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_12_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_12_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "### HF_TOKEN\r\n\r\nTo configure the User Access Token to authenticate to the Hub. If set, this value will\r\noverwrite the token stored on the machine (in `\"$HF_HOME/token\"`)."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_13_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_13_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "For more details about authentication, check out [this section](../quick-start#authentication).\r\n\r\n### HF_HUB_VERBOSITY\r\n\r\nSet the verbosity level of the `huggingface_hub`'s logger. Must be one of"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_14_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_14_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "`{\"debug\", \"info\", \"warning\", \"error\", \"critical\"}`.\r\n\r\nDefaults to `\"warning\"`."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "Defaults to `\"warning\"`.\r\n\r\nFor more details, see [logging reference](../package_reference/utilities#huggingface_hub.utils.logging.get_verbosity).\r\n\r\n### HF_HUB_LOCAL_DIR_AUTO_SYMLINK_THRESHOLD"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_16_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_16_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "### HF_HUB_LOCAL_DIR_AUTO_SYMLINK_THRESHOLD\r\n\r\nInteger value to define under which size a file is considered as \"small\". When downloading files to a local directory,"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_17_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_17_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "small files will be duplicated to ease user experience while bigger files are symlinked to save disk usage."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_18_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_18_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "For more details, see the [download guide](../guides/download#download-files-to-local-folder).\r\n\r\n### HF_HUB_ETAG_TIMEOUT"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_19_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_19_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "Integer value to define the number of seconds to wait for server response when fetching the latest metadata from a repo before downloading a file. If the request times out, `huggingface_hub` will"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_20_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_20_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "If the request times out, `huggingface_hub` will default to the locally cached files. Setting a lower value speeds up the workflow for machines with a slow connection that have already cached files."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_21_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_21_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "a slow connection that have already cached files. A higher value guarantees the metadata call to succeed in more cases. Default to 10s."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_22_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_22_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "### HF_HUB_DOWNLOAD_TIMEOUT"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_23_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_23_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "Integer value to define the number of seconds to wait for server response when downloading a file. If the request times out, a TimeoutError is raised. Setting a higher value is beneficial on machine"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_24_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_24_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "Setting a higher value is beneficial on machine with a slow connection. A smaller value makes the process fail quicker in case of complete network outage. Default to 10s."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_25_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_25_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "## Boolean values\r\n\r\nThe following environment variables expect a boolean value. The variable will be considered"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_26_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_26_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "as `True` if its value is one of `{\"1\", \"ON\", \"YES\", \"TRUE\"}` (case-insensitive). Any other value\r\n(or undefined) will be considered as `False`.\r\n\r\n### HF_HUB_OFFLINE"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_27_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_27_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "If set, no HTTP calls will me made to the Hugging Face Hub. If you try to download files, only the cached files will be accessed. If no cache file is detected, an error is raised This is useful in"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_28_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_28_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "is detected, an error is raised This is useful in case your network is slow and you don't care about having the latest version of a file."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_29_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_29_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "If `HF_HUB_OFFLINE=1` is set as environment variable and you call any method of [`HfApi`], an [`~huggingface_hub.utils.OfflineModeIsEnabled`] exception will be raised."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_30_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_30_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "**Note:** even if the latest version of a file is cached, calling `hf_hub_download` still triggers a HTTP request to check that a new version is not available. Setting `HF_HUB_OFFLINE=1` will skip"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_31_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_31_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "available. Setting `HF_HUB_OFFLINE=1` will skip this call which speeds up your loading time."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_32_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_32_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "### HF_HUB_DISABLE_IMPLICIT_TOKEN\r\n\r\nAuthentication is not mandatory for every requests to the Hub. For instance, requesting"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_33_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_33_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "details about `\"gpt2\"` model does not require to be authenticated. However, if a user is\r\n[logged in](../package_reference/login), the default behavior will be to always send the token"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_34_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_34_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "in order to ease user experience (never get a HTTP 401 Unauthorized) when accessing private or gated repositories. For privacy, you can"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_35_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_35_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "disable this behavior by setting `HF_HUB_DISABLE_IMPLICIT_TOKEN=1`. In this case,\r\nthe token will be sent only for \"write-access\" calls (example: create a commit)."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_36_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_36_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "**Note:** disabling implicit sending of token can have weird side effects. For example,\r\nif you want to list all models on the Hub, your private models will not be listed. You"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_37_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_37_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "would need to explicitly pass `token=True` argument in your script.\r\n\r\n### HF_HUB_DISABLE_PROGRESS_BARS"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_38_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_38_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "### HF_HUB_DISABLE_PROGRESS_BARS\r\n\r\nFor time consuming tasks, `huggingface_hub` displays a progress bar by default (using tqdm)."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_39_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_39_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "You can disable all the progress bars at once by setting `HF_HUB_DISABLE_PROGRESS_BARS=1`.\r\n\r\n### HF_HUB_DISABLE_SYMLINKS_WARNING"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_40_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_40_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "### HF_HUB_DISABLE_SYMLINKS_WARNING\r\n\r\nIf you are on a Windows machine, it is recommended to enable the developer mode or to run"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_41_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_41_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "`huggingface_hub` in admin mode. If not, `huggingface_hub` will not be able to create\r\nsymlinks in your cache system. You will be able to execute any script but your user experience"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_42_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_42_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "will be degraded as some huge files might end-up duplicated on your hard-drive. A warning\r\nmessage is triggered to warn you about this behavior. Set `HF_HUB_DISABLE_SYMLINKS_WARNING=1`,"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_43_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_43_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "to disable this warning.\r\n\r\nFor more details, see [cache limitations](../guides/manage-cache#limitations).\r\n\r\n### HF_HUB_DISABLE_EXPERIMENTAL_WARNING"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_44_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_44_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "### HF_HUB_DISABLE_EXPERIMENTAL_WARNING\r\n\r\nSome features of `huggingface_hub` are experimental. This means you can use them but we do not guarantee they will be"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_45_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_45_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "maintained in the future. In particular, we might update the API or behavior of such features without any deprecation"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_46_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_46_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "cycle. A warning message is triggered when using an experimental feature to warn you about it. If you're comfortable debugging any potential issues using an experimental feature, you can set"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_47_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_47_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "issues using an experimental feature, you can set `HF_HUB_DISABLE_EXPERIMENTAL_WARNING=1` to disable the warning."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_48_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_48_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "If you are using an experimental feature, please let us know! Your feedback can help us design and improve it.\r\n\r\n### HF_HUB_DISABLE_TELEMETRY"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_49_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_49_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "### HF_HUB_DISABLE_TELEMETRY\r\n\r\nBy default, some data is collected by HF libraries (`transformers`, `datasets`, `gradio`,..) to monitor usage, debug issues and help prioritize features."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_50_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_50_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "Each library defines its own policy (i.e. which usage to monitor) but the core implementation happens in `huggingface_hub` (see [`send_telemetry`])."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_51_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_51_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "You can set `HF_HUB_DISABLE_TELEMETRY=1` as environment variable to globally disable telemetry.\r\n\r\n### HF_HUB_ENABLE_HF_TRANSFER"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_52_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_52_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "### HF_HUB_ENABLE_HF_TRANSFER\r\n\r\nSet to `True` for faster uploads and downloads from the Hub using `hf_transfer`."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_53_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_53_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "By default, `huggingface_hub` uses the Python-based `requests.get` and `requests.post` functions. Although these are reliable and versatile, they may not be the most efficient choice for machines"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_54_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_54_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "may not be the most efficient choice for machines with high bandwidth. [`hf_transfer`](https://github.com/huggingface/hf_transfer) is a Rust-based package developed to maximize the bandwidth used by"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_55_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_55_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "developed to maximize the bandwidth used by dividing large files into smaller parts and transferring them simultaneously using multiple threads. This approach can potentially double the transfer"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_56_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_56_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "This approach can potentially double the transfer speed. To use `hf_transfer`, you need to install it separately [from PyPI](https://pypi.org/project/hf-transfer/) and set"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_57_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_57_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "and set `HF_HUB_ENABLE_HF_TRANSFER=1` as an environment variable."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_58_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_58_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "Please note that using `hf_transfer` comes with certain limitations. Since it is not purely Python-based, debugging errors may be challenging. Additionally, `hf_transfer` lacks several user-friendly"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_59_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_59_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "`hf_transfer` lacks several user-friendly features such as resumable downloads and proxies. These omissions are intentional to maintain the simplicity and speed of the Rust logic. Consequently,"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_60_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_60_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "and speed of the Rust logic. Consequently, `hf_transfer` is not enabled by default in `huggingface_hub`."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_61_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_61_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "## Deprecated environment variables"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_62_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_62_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "In order to standardize all environment variables within the Hugging Face ecosystem, some variables have been marked as deprecated. Although they remain functional, they no longer take precedence"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_63_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_63_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "remain functional, they no longer take precedence over their replacements. The following table outlines the deprecated variables and their corresponding alternatives:"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_64_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_64_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "| Deprecated Variable | Replacement |\r\n| --- | --- |\r\n| `HUGGINGFACE_HUB_CACHE` | `HF_HUB_CACHE` |\r\n| `HUGGINGFACE_ASSETS_CACHE` | `HF_ASSETS_CACHE` |\r\n| `HUGGING_FACE_HUB_TOKEN` | `HF_TOKEN` |"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_65_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_65_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "| `HUGGING_FACE_HUB_TOKEN` | `HF_TOKEN` |\r\n| `HUGGINGFACE_HUB_VERBOSITY` | `HF_HUB_VERBOSITY` |\r\n\r\n## From external tools"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_66_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_66_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "## From external tools\r\n\r\nSome environment variables are not specific to `huggingface_hub` but are still taken into account when they are set.\r\n\r\n### DO_NOT_TRACK"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_67_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_67_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "Boolean value. Equivalent to `HF_HUB_DISABLE_TELEMETRY`. When set to true, telemetry is globally disabled in the Hugging Face Python ecosystem (`transformers`, `diffusers`, `gradio`, etc.). See"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_68_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_68_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "`diffusers`, `gradio`, etc.). See https://consoledonottrack.com/ for more details."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_69_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_69_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "### NO_COLOR\r\n\r\nBoolean value. When set, `huggingface-cli` tool will not print any ANSI color.\r\nSee [no-color.org](https://no-color.org/).\r\n\r\n### XDG_CACHE_HOME"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_70_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_70_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "### XDG_CACHE_HOME\r\n\r\nUsed only when `HF_HOME` is not set!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_71_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_71_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "Used only when `HF_HOME` is not set!\r\n\r\nThis is the default way to configure where [user-specific non-essential (cached) data should be written](https://wiki.archlinux.org/title/XDG_Base_Directory)"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_72_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_72_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "on linux machines. \r\n\r\nIf `HF_HOME` is not set, the default home will be `\"$XDG_CACHE_HOME/huggingface\"` instead\r\nof `\"~/.cache/huggingface\"`."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_tokenizers_blob_main_bindings_node_npm_linux-x64-musl_README.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_tokenizers_blob_main_bindings_node_npm_linux-x64-musl_README.md.txt"
              },
              "prompt": "",
              "text": "`tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "the License. You may obtain a copy of the License at\r\n\r\nhttp://www.apache.org/licenses/LICENSE-2.0"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "http://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_3_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_3_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\r\nspecific language governing permissions and limitations under the License."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\r\nrendered properly in your Markdown viewer.\r\n\r\n-->"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "-->\r\n\r\n# Instantiating a big model\r\n\r\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\r\nfrom PyTorch is:"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "from PyTorch is:\r\n\r\n1. Create your model with random weights.\r\n2. Load your pretrained weights.\r\n3. Put those pretrained weights in your random model."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "Step 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "the pretrained model and store these two copies in RAM."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "<Tip>"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_11_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_11_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "Note that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_12_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_12_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_13_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_13_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_14_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_14_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "</Tip>"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "In this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_16_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_16_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "## Sharded checkpoints"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_17_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_17_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_18_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_18_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_19_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_19_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "being of size < 10GB) and an index that maps parameter names to the files they are stored in."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_20_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_20_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "You can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_21_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_21_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "with a small shard size: let's take a traditional BERT model."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_22_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_22_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "```py\r\nfrom transformers import AutoModel\r\n\r\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\r\n```"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_23_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_23_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "```\r\n\r\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\r\n\r\n```py\r\n>>> import os\r\n>>> import tempfile"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_24_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_24_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "```py\r\n>>> import os\r\n>>> import tempfile\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir)\r\n...     print(sorted(os.listdir(tmp_dir)))"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_25_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_25_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "...     print(sorted(os.listdir(tmp_dir)))\r\n['config.json', 'pytorch_model.bin']\r\n```\r\n\r\nNow let's use a maximum shard size of 200MB:\r\n\r\n```py\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_26_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_26_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\r\n...     print(sorted(os.listdir(tmp_dir)))"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_27_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_27_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\r\n```"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_28_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_28_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "On top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_29_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_29_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_30_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_30_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "```py\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\r\n...     new_model = AutoModel.from_pretrained(tmp_dir)\r\n```"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_31_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_31_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "The main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_32_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_32_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_33_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_33_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "Behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_34_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_34_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "```py\r\n>>> import json\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_35_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_35_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\r\n...         index = json.load(f)\r\n\r\n>>> print(index.keys())\r\ndict_keys(['metadata', 'weight_map'])\r\n```"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_36_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_36_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "dict_keys(['metadata', 'weight_map'])\r\n```\r\n\r\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\r\n\r\n```py\r\n>>> index[\"metadata\"]"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_37_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_37_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "```py\r\n>>> index[\"metadata\"]\r\n{'total_size': 433245184}\r\n```"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_38_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_38_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "{'total_size': 433245184}\r\n```\r\n\r\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_39_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_39_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "```py\r\n>>> index[\"weight_map\"]\r\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\r\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\r\n ...\r\n```"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_40_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_40_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "If you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_41_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_41_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_42_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_42_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "```py\r\n>>> from transformers.modeling_utils import load_sharded_checkpoint\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_43_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_43_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "...     load_sharded_checkpoint(model, tmp_dir)\r\n```\r\n\r\n## Low memory loading"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_44_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_44_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "Sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_45_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_45_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "we recommend leveraging our tools based on the Accelerate library."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_46_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_46_LangChain_TextSplitter",
                "chunkLibrary": "LangChain TextSplitter",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "Please read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "prompt": "",
              "text": "gradio demo : blocks _ multiple _ event _ triggers ` ` `! pip install - q gradio plotly pypistats ` ` ` ` ` ` import gradio as gr import pypistats from datetime import date from dateutil. relativedelta import relativedelta import pandas as pd def get _ plot ( lib, time ) : data = pypistats. overall ( lib, total = true, format = \" pandas \" ) data = data. groupby ( \" category \" ). get _ group ( \" with _ mirrors \" ). sort _ values ( \" date \" ) start _ date = date. today ( ) - relativedelta ( months = int ( time. split ( \" \" ) [ 0 ] ) ) data = data [ ( data [ ' date ' ] > str ( start _ date ) ) ] data. date = pd. to _ datetime ( pd. to _ date"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "prompt": "",
              "text": "( time. split ( \" \" ) [ 0 ] ) ) data = data [ ( data [ ' date ' ] > str ( start _ date ) ) ] data. date = pd. to _ datetime ( pd. to _ datetime ( data. date ) ) return gr. lineplot ( value = data, x = \" date \", y = \" downloads \", tooltip = [ ' date ', ' downloads ' ], title = f \" pypi downloads of { lib } over last { time } \", overlay _ point = true, height = 400, width = 900 ) with gr. blocks ( ) as demo : gr. markdown ( \" \" \" # # pypi download stats see live download stats for all of hugging face ' s open - source libraries \" \" \" ) with gr. row ( ) : lib = gr. dropdown ( [ \" transformers \", \" datasets \", \""
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "prompt": "",
              "text": "download stats see live download stats for all of hugging face ' s open - source libraries \" \" \" ) with gr. row ( ) : lib = gr. dropdown ( [ \" transformers \", \" datasets \", \" huggingface - hub \", \" gradio \", \" accelerate \" ], value = \" gradio \", label = \" library \" ) time = gr. dropdown ( [ \" 3 months \", \" 6 months \", \" 9 months \", \" 12 months \" ], value = \" 3 months \", label = \" downloads over the last... \" ) plt = gr. lineplot ( ) # you can add multiple event triggers in 2 lines like this for event in [ lib. change, time. change, demo. load ] : event ( get _ plot, [ lib, time ], [ plt ] ) if _ _ name _ _ = = \" _ _ main _"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt"
              },
              "prompt": "",
              "text": "this for event in [ lib. change, time. change, demo. load ] : event ( get _ plot, [ lib, time ], [ plt ] ) if _ _ name _ _ = = \" _ _ main _ _ \" : demo. launch ( ) ` ` `"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "prompt": "",
              "text": "contributing a guide want to help teach gradio? consider contributing a guide! broadly speaking, there are two types of guides : - * * use cases * * : guides that cover step - by - step how to build a particular type of machine learning demo or app using gradio. here ' s an example : [ _ creating a chatbot _ ] ( https : / / github. com / gradio - app / gradio / blob / master / guides / creating _ a _ chatbot. md ) - * * feature explanation * * : guides that describe in detail a particular feature of gradio. here ' s an example : [ _ using flagging _ ] ( https : / / github. com / gradio - app / gradio / blob / master / guides / using _ flagging. md ) we encourage you to submit either type of guide! ( looking for ideas? we may also"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "prompt": "",
              "text": "/ github. com / gradio - app / gradio / blob / master / guides / using _ flagging. md ) we encourage you to submit either type of guide! ( looking for ideas? we may also have open [ issues ] ( https : / / github. com / gradio - app / gradio / issues? q = is % 3aopen + is % 3aissue + label % 3aguides ) where users have asked for guides on particular topics ) # # guide structure as you can see with the previous examples, guides are standard markdown documents. they usually : - start with an introduction section describing the topic - include subheadings to make articles easy to navigate - include real code snippets that make it easy to follow along and implement the guide - include embedded gradio demos to make them more interactive and provide immediate demonstrations of the topic being discussed. these gradio demos are hosted on ["
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "prompt": "",
              "text": "- include real code snippets that make it easy to follow along and implement the guide - include embedded gradio demos to make them more interactive and provide immediate demonstrations of the topic being discussed. these gradio demos are hosted on [ hugging face spaces ] ( https : / / huggingface. co / spaces ) and are embedded using the standard \\ < iframe \\ > tag. # # how to contribute a guide 1. clone or fork this ` gradio ` repo 2. add a new markdown document with a descriptive title to the ` / guides ` folder 3. write your guide in standard markdown! embed gradio demos wherever helpful 4. add a list of ` related _ spaces ` at the top of the markdown document ( see the previously linked guides for how to do this ) 5. add 3 ` tags ` at the top of the markdown document to help users find your guide ( again, see the previously linked guides for how"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt"
              },
              "prompt": "",
              "text": "the top of the markdown document ( see the previously linked guides for how to do this ) 5. add 3 ` tags ` at the top of the markdown document to help users find your guide ( again, see the previously linked guides for how to do this ) 6. open a pr to have your guide reviewed that ' s it! we ' re looking forward to reading your guide"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "@ gradio / gallery # # 0. 4. 14 # # # patch changes - updated dependencies [ [ ` 828fb9e ` ] ( https : / / github. com / gradio - app / gradio / commit / 828fb9e6ce15b6ea08318675a2361117596a1b5d ), [ ` 73268ee ` ] ( https : / / github. com / gradio - app / gradio / commit / 73268ee2e39f23ebdd1e927cb49b8d79c4b9a144 ) ] : - @ gradio / client @ 0. 9. 3 - @ gradio / statustracker @ 0. 4. 3 - @ gradio / atoms @ 0. 4. 1 - @ gradio / image @ 0. 5. 3"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "##o / client @ 0. 9. 3 - @ gradio / statustracker @ 0. 4. 3 - @ gradio / atoms @ 0. 4. 1 - @ gradio / image @ 0. 5. 3 - @ gradio / upload @ 0. 5. 6 # # 0. 4. 13 # # # patch changes - updated dependencies [ [ ` 245d58e ` ] ( https : / / github. com / gradio - app / gradio / commit / 245d58eff788e8d44a59d37a2d9b26d0f08a62b4 ) ] : - @ gradio / client @ 0. 9. 2 - @ gradio / image @ 0. 5. 2 - @ gradio / upload @ 0. 5. 5 # # 0. 4. 12 # # # patch changes -"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "##adio / client @ 0. 9. 2 - @ gradio / image @ 0. 5. 2 - @ gradio / upload @ 0. 5. 5 # # 0. 4. 12 # # # patch changes - updated dependencies [ [ ` 5d51fbc ` ] ( https : / / github. com / gradio - app / gradio / commit / 5d51fbce7826da840a2fd4940feb5d9ad6f1bc5a ), [ ` 34f9431 ` ] ( https : / / github. com / gradio - app / gradio / commit / 34f943101bf7dd6b8a8974a6131c1ed7c4a0dac0 ) ] : - @ gradio / upload @ 0. 5. 4 - @"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "##f943101bf7dd6b8a8974a6131c1ed7c4a0dac0 ) ] : - @ gradio / upload @ 0. 5. 4 - @ gradio / client @ 0. 9. 1 - @ gradio / image @ 0. 5. 1 # # 0. 4. 11 # # # fixes - [ # 6487 ] ( https : / / github. com / gradio - app / gradio / pull / 6487 ) [ ` 9a5811d ` ] ( https : / / github. com / gradio - app / gradio / commit / 9a5811df9218b622af59ba243a937a9c36ba00f9 ) - fix the download button of the ` gr. gallery ( ) ` component to work. thanks ["
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_4_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_4_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "commit / 9a5811df9218b622af59ba243a937a9c36ba00f9 ) - fix the download button of the ` gr. gallery ( ) ` component to work. thanks [ @ whitphx ] ( https : / / github. com / whitphx )! # # 0. 4. 10 # # # patch changes - updated dependencies [ [ ` b639e04 ` ] ( https : / / github. com / gradio - app / gradio / commit / b639e040741e6c0d9104271c81415d7befbd8cf3 ), [ ` 206af31 ` ] ( https : / / github. com / gradio - app / gradio / commit / 206af31d7c1a31013364a44"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_5_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_5_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "##8cf3 ), [ ` 206af31 ` ] ( https : / / github. com / gradio - app / gradio / commit / 206af31d7c1a31013364a44e9b40cf8df304ba50 ) ] : - @ gradio / image @ 0. 4. 2 - @ gradio / icons @ 0. 3. 1 - @ gradio / atoms @ 0. 3. 1 - @ gradio / statustracker @ 0. 4. 1 - @ gradio / upload @ 0. 5. 2 # # 0. 4. 9 # # # patch changes - updated dependencies [ [ ` 71f1a1f99 ` ] ( https : / / github. com / gradio - app / gradio / commit / 71f1a1f9931489d465c2c"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_6_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_6_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "[ ` 71f1a1f99 ` ] ( https : / / github. com / gradio - app / gradio / commit / 71f1a1f9931489d465c2c1302a5c8d768a3cd23a ) ] : - @ gradio / client @ 0. 8. 2 - @ gradio / image @ 0. 4. 1 - @ gradio / upload @ 0. 5. 1 # # 0. 4. 8 # # # patch changes - updated dependencies [ [ ` 9caddc17b ` ] ( https : / / github. com / gradio - app / gradio / commit / 9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8 ) ] : - @ gradio / atoms @ 0"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_7_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_7_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- app / gradio / commit / 9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8 ) ] : - @ gradio / atoms @ 0. 3. 0 - @ gradio / icons @ 0. 3. 0 - @ gradio / image @ 0. 4. 0 - @ gradio / statustracker @ 0. 4. 0 - @ gradio / upload @ 0. 5. 0 # # 0. 4. 7 # # # patch changes - updated dependencies [ [ ` 2f805a7dd ` ] ( https : / / github. com / gradio - app / gradio / commit / 2f805a7dd3d2b64b098f659dadd5d01258290521 ), [ ` f816136a0"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_8_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_8_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "##o - app / gradio / commit / 2f805a7dd3d2b64b098f659dadd5d01258290521 ), [ ` f816136a0 ` ] ( https : / / github. com / gradio - app / gradio / commit / f816136a039fa6011be9c4fb14f573e4050a681a ) ] : - @ gradio / image @ 0. 3. 6 - @ gradio / upload @ 0. 4. 2 - @ gradio / atoms @ 0. 2. 2 - @ gradio / icons @ 0. 2. 1 - @ gradio / statustracker @ 0. 3. 2 # # 0. 4. 6 # # # patch changes - updated dependencies [ [ ` 324867f63 `"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_9_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_9_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "##o / icons @ 0. 2. 1 - @ gradio / statustracker @ 0. 3. 2 # # 0. 4. 6 # # # patch changes - updated dependencies [ [ ` 324867f63 ` ] ( https : / / github. com / gradio - app / gradio / commit / 324867f63c920113d89a565892aa596cf8b1e486 ) ] : - @ gradio / client @ 0. 8. 1 - @ gradio / image @ 0. 3. 5 - @ gradio / upload @ 0. 4. 1 # # 0. 4. 5 # # # patch changes - updated dependencies [ [ ` 6204ccac5 ` ] ( https : / / github. com / gradio - app / gradio / commit / 6204ccac59"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_10_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_10_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "4. 5 # # # patch changes - updated dependencies [ [ ` 6204ccac5 ` ] ( https : / / github. com / gradio - app / gradio / commit / 6204ccac5967763e0ebde550d04d12584243a120 ), [ ` 4d3aad33a ` ] ( https : / / github. com / gradio - app / gradio / commit / 4d3aad33a0b66639dbbb2928f305a79fb7789b2d ), [ ` 854b482f5 ` ] ( https : / / github. com / gradio - app / gradio / commit / 854b482f598e0dc47673846631643c079576da"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_11_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_11_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "( https : / / github. com / gradio - app / gradio / commit / 854b482f598e0dc47673846631643c079576da9c ), [ ` f1409f95e ` ] ( https : / / github. com / gradio - app / gradio / commit / f1409f95ed39c5565bed6a601e41f94e30196a57 ) ] : - @ gradio / image @ 0. 3. 4 - @ gradio / upload @ 0. 4. 0 - @ gradio / client @ 0. 8. 0 # # 0. 4. 4 # # # patch changes - updated dependencies [ [ ` bca6c2c80 ` ] ( https : / / github. com / gradio"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_12_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_12_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "/ client @ 0. 8. 0 # # 0. 4. 4 # # # patch changes - updated dependencies [ [ ` bca6c2c80 ` ] ( https : / / github. com / gradio - app / gradio / commit / bca6c2c80f7e5062427019de45c282238388af95 ), [ ` 3cdeabc68 ` ] ( https : / / github. com / gradio - app / gradio / commit / 3cdeabc6843000310e1a9e1d17190ecbf3bbc780 ), [ ` fad92c29d ` ] ( https : / / github. com / gradio - app / gradio / commit / fad92c29dc1f5cd84341aa"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_13_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_13_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": ", [ ` fad92c29d ` ] ( https : / / github. com / gradio - app / gradio / commit / fad92c29dc1f5cd84341aae417c495b33e01245f ) ] : - @ gradio / client @ 0. 7. 2 - @ gradio / atoms @ 0. 2. 1 - @ gradio / upload @ 0. 3. 3 - @ gradio / image @ 0. 3. 3 - @ gradio / statustracker @ 0. 3. 1 # # 0. 4. 3 # # # fixes - [ # 6288 ] ( https : / / github. com / gradio - app / gradio / pull / 6288 ) [ ` 92278729e ` ] ( https : / / github. com"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_14_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_14_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "##88 ] ( https : / / github. com / gradio - app / gradio / pull / 6288 ) [ ` 92278729e ` ] ( https : / / github. com / gradio - app / gradio / commit / 92278729ee008126af15ffe6be399236211b2f34 ) - gallery preview fix and optionally skip download of urls in postprcess. thanks [ @ freddyaboulton ] ( https : / / github. com / freddyaboulton )! # # 0. 4. 2 # # # fixes - [ # 6277 ] ( https : / / github. com / gradio - app / gradio / pull / 6277 ) [ ` 5fe091367 ` ] ( https : / / github. com /"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_15_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_15_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "] ( https : / / github. com / gradio - app / gradio / pull / 6277 ) [ ` 5fe091367 ` ] ( https : / / github. com / gradio - app / gradio / commit / 5fe091367fbe0eecdd504aa734ca1c70b0621f52 ) - handle selected _ index prop change for gallery. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! # # 0. 4. 1 # # # patch changes - updated dependencies [ [ ` 2ba14b284 ` ] ( https : / / github. com / gradio - app / gradio / commit / 2ba14b284f908aa13859f4337167a157075a68eb"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_16_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_16_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "] ( https : / / github. com / gradio - app / gradio / commit / 2ba14b284f908aa13859f4337167a157075a68eb ) ] : - @ gradio / client @ 0. 7. 1 - @ gradio / image @ 0. 3. 1 - @ gradio / upload @ 0. 3. 1 # # 0. 4. 0 # # # features - [ # 5498 ] ( https : / / github. com / gradio - app / gradio / pull / 5498 ) [ ` 287fe6782 ` ] ( https : / / github. com / gradio - app / gradio / commit / 287fe6782825479513e79a5cf0ba0fbfe51443d7 ) -"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_17_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_17_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": ": / / github. com / gradio - app / gradio / commit / 287fe6782825479513e79a5cf0ba0fbfe51443d7 ) - fix circular dependency with client + upload. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5498 ] ( https : / / github. com / gradio - app / gradio / pull / 5498 ) [ ` 287fe6782 ` ] ( https : / / github. com / gradio - app / gradio / commit / 287fe6782825479513e79a5cf0ba0fbfe51443d7 ) - clean root url. thanks [ @ pngwn ] ( https : / / github. com"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_18_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_18_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "##fe6782825479513e79a5cf0ba0fbfe51443d7 ) - clean root url. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5498 ] ( https : / / github. com / gradio - app / gradio / pull / 5498 ) [ ` 287fe6782 ` ] ( https : / / github. com / gradio - app / gradio / commit / 287fe6782825479513e79a5cf0ba0fbfe51443d7 ) - image v4. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5498 ] ( https : / / github. com / gradio"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_19_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_19_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": ") - image v4. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5498 ] ( https : / / github. com / gradio - app / gradio / pull / 5498 ) [ ` 287fe6782 ` ] ( https : / / github. com / gradio - app / gradio / commit / 287fe6782825479513e79a5cf0ba0fbfe51443d7 ) - publish all components to npm. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5498 ] ( https : / / github. com / gradio - app / gradio / pull / 5498 ) [ ` 287fe6782 ` ] ("
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_20_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_20_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "pngwn )! - [ # 5498 ] ( https : / / github. com / gradio - app / gradio / pull / 5498 ) [ ` 287fe6782 ` ] ( https : / / github. com / gradio - app / gradio / commit / 287fe6782825479513e79a5cf0ba0fbfe51443d7 ) - custom components. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! # # 0. 4. 0 - beta. 9 # # # features - [ # 6143 ] ( https : / / github. com / gradio - app / gradio / pull / 6143 ) [ ` e4f7b4b40 ` ] ( https : / / github. com / gradio"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_21_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_21_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": ": / / github. com / gradio - app / gradio / pull / 6143 ) [ ` e4f7b4b40 ` ] ( https : / / github. com / gradio - app / gradio / commit / e4f7b4b409323b01aa01b39e15ce6139e29aa073 ) - fix circular dependency with client + upload. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 6136 ] ( https : / / github. com / gradio - app / gradio / pull / 6136 ) [ ` 667802a6c ` ] ( https : / / github. com / gradio - app / gradio / commit / 667802a6cdbfb2ce45"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_22_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_22_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "/ 6136 ) [ ` 667802a6c ` ] ( https : / / github. com / gradio - app / gradio / commit / 667802a6cdbfb2ce454a3be5a78e0990b194548a ) - js component documentation. thanks [ @ freddyaboulton ] ( https : / / github. com / freddyaboulton )! - [ # 6094 ] ( https : / / github. com / gradio - app / gradio / pull / 6094 ) [ ` c476bd5a5 ` ] ( https : / / github. com / gradio - app / gradio / commit / c476bd5a5b70836163b9c69bf4bfe068b17fbe13 )"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_23_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_23_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "github. com / gradio - app / gradio / commit / c476bd5a5b70836163b9c69bf4bfe068b17fbe13 ) - image v4. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! # # 0. 4. 0 - beta. 8 # # # features - [ # 6016 ] ( https : / / github. com / gradio - app / gradio / pull / 6016 ) [ ` 83e947676 ` ] ( https : / / github. com / gradio - app / gradio / commit / 83e947676d327ca2ab6ae2a2d710c78961c771a0 ) - format js in v4"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_24_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_24_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "- app / gradio / commit / 83e947676d327ca2ab6ae2a2d710c78961c771a0 ) - format js in v4 branch. thanks [ @ freddyaboulton ] ( https : / / github. com / freddyaboulton )! # # # fixes - [ # 6046 ] ( https : / / github. com / gradio - app / gradio / pull / 6046 ) [ ` dbb7de5e0 ` ] ( https : / / github. com / gradio - app / gradio / commit / dbb7de5e02c53fee05889d696d764d212cb96c74 ) - fix tests. thanks [ @ pngwn ] ( https : / / github"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_25_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_25_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "##7de5e02c53fee05889d696d764d212cb96c74 ) - fix tests. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! # # 0. 4. 0 - beta. 7 # # # patch changes - updated dependencies [ ] : - @ gradio / image @ 0. 3. 0 - beta. 7 # # 0. 4. 0 - beta. 6 # # # features - [ # 5960 ] ( https : / / github. com / gradio - app / gradio / pull / 5960 ) [ ` 319c30f3f ` ] ( https : / / github. com / gradio - app / gradio / commit / 319c30f3fccf23bfe1da6c9b132a6a"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_26_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_26_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "##f3f ` ] ( https : / / github. com / gradio - app / gradio / commit / 319c30f3fccf23bfe1da6c9b132a6a99d59652f7 ) - rererefactor frontend files. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5938 ] ( https : / / github. com / gradio - app / gradio / pull / 5938 ) [ ` 13ed8a485 ` ] ( https : / / github. com / gradio - app / gradio / commit / 13ed8a485d5e31d7d75af87fe8654b661edcca93 ) - v4 : use beta release versions for ' @ gradio '"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_27_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_27_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "/ gradio / commit / 13ed8a485d5e31d7d75af87fe8654b661edcca93 ) - v4 : use beta release versions for ' @ gradio ' packages. thanks [ @ freddyaboulton ] ( https : / / github. com / freddyaboulton )! # # 0. 5. 1 # # # patch changes - updated dependencies [ [ ` b67115e8e ` ] ( https : / / github. com / gradio - app / gradio / commit / b67115e8e6e489fffd5271ea830211863241ddc5 ), [ ` 8d909624f ` ] ( https : / / github. com / gradio - app / gradio / commit / 8d909624"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_28_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_28_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "##211863241ddc5 ), [ ` 8d909624f ` ] ( https : / / github. com / gradio - app / gradio / commit / 8d909624f61a49536e3c0f71cb2d9efe91216219 ), [ ` e70805d54 ` ] ( https : / / github. com / gradio - app / gradio / commit / e70805d54cc792452545f5d8eccc1aa0212a4695 ) ] : - @ gradio / image @ 0. 4. 0 - @ gradio / atoms @ 0. 2. 0 - @ gradio / statustracker @ 0. 2. 3 - @ gradio / upload @ 0. 3. 3 # # 0. 5"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_29_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_29_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "4. 0 - @ gradio / atoms @ 0. 2. 0 - @ gradio / statustracker @ 0. 2. 3 - @ gradio / upload @ 0. 3. 3 # # 0. 5. 0 # # # features - [ # 5780 ] ( https : / / github. com / gradio - app / gradio / pull / 5780 ) [ ` ed0f9a21b ` ] ( https : / / github. com / gradio - app / gradio / commit / ed0f9a21b04ad6b941b63d2ce45100dbd1abd5c5 ) - adds ` change ( ) ` event to ` gr. gallery `. thanks [ @ abidlabs ] ( https : / / github. com / abidlabs )! - [ # 5783 ] ("
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_30_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_30_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "##5 ) - adds ` change ( ) ` event to ` gr. gallery `. thanks [ @ abidlabs ] ( https : / / github. com / abidlabs )! - [ # 5783 ] ( https : / / github. com / gradio - app / gradio / pull / 5783 ) [ ` 4567788bd ` ] ( https : / / github. com / gradio - app / gradio / commit / 4567788bd1fc25df9322902ba748012e392b520a ) - adds the ability to set the ` selected _ index ` in a ` gr. gallery `. thanks [ @ abidlabs ] ( https : / / github. com / abidlabs )! # # # fixes - [ # 5798 ] ( https"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_31_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_31_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "_ index ` in a ` gr. gallery `. thanks [ @ abidlabs ] ( https : / / github. com / abidlabs )! # # # fixes - [ # 5798 ] ( https : / / github. com / gradio - app / gradio / pull / 5798 ) [ ` a0d3cc45c ` ] ( https : / / github. com / gradio - app / gradio / commit / a0d3cc45c6db48dc0db423c229b8fb285623cdc4 ) - fix ` gr. selectdata ` so that the target attribute is correctly attached, and the filedata is included in the data attribute with ` gr. gallery `. thanks [ @ abidlabs ] ( https : / / github. com / abidlabs )! # # 0"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_32_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_32_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "attribute is correctly attached, and the filedata is included in the data attribute with ` gr. gallery `. thanks [ @ abidlabs ] ( https : / / github. com / abidlabs )! # # 0. 4. 1 # # # fixes - [ # 5735 ] ( https : / / github. com / gradio - app / gradio / pull / 5735 ) [ ` abb5e9df4 ` ] ( https : / / github. com / gradio - app / gradio / commit / abb5e9df47989b2c56c2c312d74944678f9f2d4e ) - ensure images with no caption download in gallery. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )! - [ # 5754 ] ("
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_33_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_33_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "##8f9f2d4e ) - ensure images with no caption download in gallery. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )! - [ # 5754 ] ( https : / / github. com / gradio - app / gradio / pull / 5754 ) [ ` 502054848 ` ] ( https : / / github. com / gradio - app / gradio / commit / 502054848fdbe39fc03ec42445242b4e49b7affc ) - fix gallery ` columns ` and ` rows ` params. thanks [ @ abidlabs ] ( https : / / github. com / abidlabs )! # # 0. 4. 0 # # # features - [ # 5554 ] ( https : / / github. com / gr"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_34_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_34_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "##idlabs ] ( https : / / github. com / abidlabs )! # # 0. 4. 0 # # # features - [ # 5554 ] ( https : / / github. com / gradio - app / gradio / pull / 5554 ) [ ` 75ddeb390 ` ] ( https : / / github. com / gradio - app / gradio / commit / 75ddeb390d665d4484667390a97442081b49a423 ) - accessibility improvements. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )! # # 0. 3. 4 # # # patch changes - updated dependencies [ [ ` e0d61b8ba ` ] ( https : / / github. com / gradio - app / gradi"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_35_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_35_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": ")! # # 0. 3. 4 # # # patch changes - updated dependencies [ [ ` e0d61b8ba ` ] ( https : / / github. com / gradio - app / gradio / commit / e0d61b8baa0f6293f53b9bdb1647d42f9ae2583a ) ] : - @ gradio / image @ 0. 2. 4 # # 0. 3. 3 # # # fixes - [ # 5528 ] ( https : / / github. com / gradio - app / gradio / pull / 5528 ) [ ` dc86e4a7 ` ] ( https : / / github. com / gradio - app / gradio / commit / dc86e4a7e1c40b910c74558e6f88"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_36_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_36_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "##4a7 ` ] ( https : / / github. com / gradio - app / gradio / commit / dc86e4a7e1c40b910c74558e6f88fddf9b3292bc ) - lazy load all images. thanks [ @ aliabid94 ] ( https : / / github. com / aliabid94 )! # # 0. 3. 2 # # # patch changes - updated dependencies [ [ ` afac0006 ` ] ( https : / / github. com / gradio - app / gradio / commit / afac0006337ce2840cf497cd65691f2f60ee5912 ) ] : - @ gradio / statustracker @ 0. 2. 0 - @ gradio / image @ 0. 2. 2 -"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_37_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_37_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "##2840cf497cd65691f2f60ee5912 ) ] : - @ gradio / statustracker @ 0. 2. 0 - @ gradio / image @ 0. 2. 2 - @ gradio / utils @ 0. 1. 1 - @ gradio / atoms @ 0. 1. 2 - @ gradio / upload @ 0. 2. 1 # # 0. 3. 1 # # # patch changes - updated dependencies [ [ ` abf1c57d ` ] ( https : / / github. com / gradio - app / gradio / commit / abf1c57d7d85de0df233ee3b38aeb38b638477db ), [ ` 79d8f9d8 ` ] ( https : / / github. com / gradio - app / gradio"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_38_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_38_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "##0df233ee3b38aeb38b638477db ), [ ` 79d8f9d8 ` ] ( https : / / github. com / gradio - app / gradio / commit / 79d8f9d891901683c5a1b7486efb44eab2478c96 ) ] : - @ gradio / icons @ 0. 1. 0 - @ gradio / utils @ 0. 1. 0 - @ gradio / upload @ 0. 2. 0 - @ gradio / atoms @ 0. 1. 1 - @ gradio / image @ 0. 2. 1 - @ gradio / statustracker @ 0. 1. 1 # # 0. 3. 0 # # # highlights # # # # improve startup performance and markdown support ( [ # 5279 ] ( https"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_39_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_39_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": ". 2. 1 - @ gradio / statustracker @ 0. 1. 1 # # 0. 3. 0 # # # highlights # # # # improve startup performance and markdown support ( [ # 5279 ] ( https : / / github. com / gradio - app / gradio / pull / 5279 ) [ ` fe057300 ` ] ( https : / / github. com / gradio - app / gradio / commit / fe057300f0672c62dab9d9b4501054ac5d45a4ec ) ) # # # # # improved markdown support we now have better support for markdown in ` gr. markdown ` and ` gr. dataframe `. including syntax highlighting and github flavoured markdown. we also have more consistent markdown behaviour and styling. # # # # # various"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_40_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_40_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "have better support for markdown in ` gr. markdown ` and ` gr. dataframe `. including syntax highlighting and github flavoured markdown. we also have more consistent markdown behaviour and styling. # # # # # various performance improvements these improvements will be particularly beneficial to large applications. - rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of gradio. app startup for large applications is now around twice as fast. - optimised the mounting of individual components, leading to a modest performance improvement during startup ( ~ 30 % ). - corrected an issue that was causing markdown to re - render infinitely. - ensured that the ` gr. 3dmodel ` does re - render prematurely. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! # # # features - [ # 5215 ] ( https"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_41_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_41_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "that the ` gr. 3dmodel ` does re - render prematurely. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! # # # features - [ # 5215 ] ( https : / / github. com / gradio - app / gradio / pull / 5215 ) [ ` fbdad78a ` ] ( https : / / github. com / gradio - app / gradio / commit / fbdad78af4c47454cbb570f88cc14bf4479bbceb ) - lazy load interactive or static variants of a component individually, rather than loading both variants regardless. this change will improve performance for many applications. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5216 ] ( https : / / github"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_42_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_42_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "both variants regardless. this change will improve performance for many applications. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5216 ] ( https : / / github. com / gradio - app / gradio / pull / 5216 ) [ ` 4b58ea6d ` ] ( https : / / github. com / gradio - app / gradio / commit / 4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8 ) - update i18n tokens and locale files. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )! # # 0. 2. 0 # # # features - [ # 5025 ] ( https : / / github. com / gradio -"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_43_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_43_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": "##ir ] ( https : / / github. com / hannahblair )! # # 0. 2. 0 # # # features - [ # 5025 ] ( https : / / github. com / gradio - app / gradio / pull / 5025 ) [ ` 6693660a ` ] ( https : / / github. com / gradio - app / gradio / commit / 6693660a790996f8f481feaf22a8c49130d52d89 ) - add download button to selected images in ` gallery `. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )! # # 0. 1. 0 # # # features - [ # 4995 ] ( https : / / github. com / gradio - app / gradio / pull / 49"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_44_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_44_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt"
              },
              "prompt": "",
              "text": ". com / hannahblair )! # # 0. 1. 0 # # # features - [ # 4995 ] ( https : / / github. com / gradio - app / gradio / pull / 4995 ) [ ` 3f8c210b ` ] ( https : / / github. com / gradio - app / gradio / commit / 3f8c210b01ef1ceaaf8ee73be4bf246b5b745bbf ) - implement left and right click in ` gallery ` component and show implicit images in ` gallery ` grid. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )!"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "hat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": ". input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_4_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_4_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": "removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. to get an output linked to our classification problem, we need to use the automodelforsequenceclassification class. it works exactly as the automodel class, except that it will build a model with a classification head. there is one auto class for each common nlp task in the transformers library. here, after giving our model the two sentences, we get a tensor of size two by two : one result for each sentence and for each possible label. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_5_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_5_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt"
              },
              "prompt": "",
              "text": ". those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "prompt": "",
              "text": "! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "prompt": "",
              "text": "to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "prompt": "",
              "text": "- diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors, detailed, 8k \" image = pipeline ( prompt, num _ inference _ steps = 25 ). images [ 0 ] ` ` ` < tip > check out the [ autopipeline ] (.. /.. / tutorials / autopipeline ) tutorial to learn how to use this api! < / tip > ` autopipeline ` supports text - to - image, image - to - image, and inpainting for the following diffusion models : - [ stable diffusion ] (. / stable _ diffusion / overview ) - [ controlnet ] (. / controlnet ) - [ stable diffusion xl ( sdxl ) ] (. / stable _ diffusion / stable _ diffusion _ xl ) - [ deepfloy"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "prompt": "",
              "text": "] (. / stable _ diffusion / overview ) - [ controlnet ] (. / controlnet ) - [ stable diffusion xl ( sdxl ) ] (. / stable _ diffusion / stable _ diffusion _ xl ) - [ deepfloyd if ] (. / deepfloyd _ if ) - [ kandinsky 2. 1 ] (. / kandinsky ) - [ kandinsky 2. 2 ] (. / kandinsky _ v22 ) # # autopipelinefortext2image [ [ autodoc ] ] autopipelinefortext2image - all - from _ pretrained - from _ pipe # # autopipelineforimage2image [ [ autodoc ] ] autopipelineforimage2image - all - from _ pretrained - from _ pipe # # autopipelineforinpainting [ [ autodoc ] ] autopipelineforinpaint"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_4_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_4_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt"
              },
              "prompt": "",
              "text": "autodoc ] ] autopipelineforimage2image - all - from _ pretrained - from _ pipe # # autopipelineforinpainting [ [ autodoc ] ] autopipelineforinpainting - all - from _ pretrained - from _ pipe"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # diffusers â€™ ethical guidelines # # preamble [ diffusers ] ( https : / / huggingface. co / docs / diffusers / index ) provides pre - trained diffusion models and serves as a modular toolbox for inference and training. given its real case applications in the world and potential negative impacts on society, we think it"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "/ huggingface. co / docs / diffusers / index ) provides pre - trained diffusion models and serves as a modular toolbox for inference and training. given its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, users â€™ contributions, and usage of the diffusers library. the risks associated with using this technology are still being examined, but to name a few : copyrights issues for artists ; deep - fake exploitation ; sexual content generation in inappropriate contexts ; non - consensual impersonation ; harmful social biases perpetuating the oppression of marginalized groups. we will keep tracking risks and adapt the following guidelines based on the community ' s responsiveness and valuable feedback. # # scope the diffusers community will apply the following ethical guidelines to the project â€™ s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns. # # ethical"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "' s responsiveness and valuable feedback. # # scope the diffusers community will apply the following ethical guidelines to the project â€™ s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns. # # ethical guidelines the following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question. - * * transparency * * : we are committed to being transparent in managing prs, explaining our choices to users, and making technical decisions. - * * consistency * * : we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "same level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_4_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_4_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and social risks associated with diffusion technology. moreover, the community ' s input is invaluable in ensuring these features ' implementation and raising awareness with us. - [ * * community tab * * ] ( https : / / huggingface. co / docs / hub / repositories - pull - requests - discussions ) : it enables the community to discuss and better collaborate on a project. - * * bias exploration and evaluation * * : the hugging face team provides a [ space ] ( https : / / huggingface. co / spaces / society - ethics / diffusionbiasexplorer ) to demonstrate the biases in stable diffusion interactively. in this sense, we support and encourage bias explorers and evaluations. - *"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_5_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_5_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "https : / / huggingface. co / spaces / society - ethics / diffusionbiasexplorer ) to demonstrate the biases in stable diffusion interactively. in this sense, we support and encourage bias explorers and evaluations. - * * encouraging safety in deployment * * - [ * * safe stable diffusion * * ] ( https : / / huggingface. co / docs / diffusers / main / en / api / pipelines / stable _ diffusion / stable _ diffusion _ safe ) : it mitigates the well - known issue that models, like stable diffusion, that are trained on unfiltered, web - crawled datasets tend to suffer from inappropriate degeneration. related paper : [ safe latent diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / gi"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_6_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_6_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": "diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_7_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_7_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt"
              },
              "prompt": "",
              "text": ", access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https : / / huggingface. co / blog / open _ rail ), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_src_diffusers_experimental_README.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_diffusers_blob_main_src_diffusers_experimental_README.md.txt"
              },
              "prompt": "",
              "text": "diffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "! - - note that this file is in markdown but contain specific syntax for our doc - builder ( similar to mdx ) that may not be rendered properly in your markdown viewer. - - > # environment variables ` huggingface _ hub ` can be configured using environment variables. if you are unfamiliar with environment variable, here are generic articles about them [ on macos and linux ] ( https : / / linuxize. com / post / how - to - set - and - list - environment - variables - in - linux / ) and on [ windows ] ( https : / / phoenixnap. com / kb / windows - set - environment - variable ). this page will guide you through all environment variables specific to ` huggingface _ hub ` and their meaning. # # generic # # # hf _ inference _ endpoint to configure the inference api base url. you might want to set this variable if your organization is pointing at an api gateway rather"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "##face _ hub ` and their meaning. # # generic # # # hf _ inference _ endpoint to configure the inference api base url. you might want to set this variable if your organization is pointing at an api gateway rather than directly at the inference api. defaults to ` \" https : / / api - inference. huggingface. com \" `. # # # hf _ home to configure where ` huggingface _ hub ` will locally store data. in particular, your token and the cache will be stored in this folder. defaults to ` \" ~ /. cache / huggingface \" ` unless [ xdg _ cache _ home ] ( # xdgcachehome ) is set. # # # hf _ hub _ cache to configure where repositories from the hub will be cached locally ( models, datasets and spaces ). defaults to ` \" $ hf _ home /"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "set. # # # hf _ hub _ cache to configure where repositories from the hub will be cached locally ( models, datasets and spaces ). defaults to ` \" $ hf _ home / hub \" ` ( e. g. ` \" ~ /. cache / huggingface / hub \" ` by default ). # # # hf _ assets _ cache to configure where [ assets ] (.. / guides / manage - cache # caching - assets ) created by downstream libraries will be cached locally. those assets can be preprocessed data, files downloaded from github, logs,... defaults to ` \" $ hf _ home / assets \" ` ( e. g. ` \" ~ /. cache / huggingface / assets \" ` by default ). # # # hf _ token to configure the user access token to authenticate to the hub. if"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "/ assets \" ` ( e. g. ` \" ~ /. cache / huggingface / assets \" ` by default ). # # # hf _ token to configure the user access token to authenticate to the hub. if set, this value will overwrite the token stored on the machine ( in ` \" $ hf _ home / token \" ` ). for more details about authentication, check out [ this section ] (.. / quick - start # authentication ). # # # hf _ hub _ verbosity set the verbosity level of the ` huggingface _ hub ` ' s logger. must be one of ` { \" debug \", \" info \", \" warning \", \" error \", \" critical \" } `. defaults to ` \" warning \" `. for more details, see [ logging reference ] (.. / package _ reference / utilities # huggingface _ hub. utils. logging. get _ verb"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_4_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_4_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "error \", \" critical \" } `. defaults to ` \" warning \" `. for more details, see [ logging reference ] (.. / package _ reference / utilities # huggingface _ hub. utils. logging. get _ verbosity ). # # # hf _ hub _ local _ dir _ auto _ symlink _ threshold integer value to define under which size a file is considered as \" small \". when downloading files to a local directory, small files will be duplicated to ease user experience while bigger files are symlinked to save disk usage. for more details, see the [ download guide ] (.. / guides / download # download - files - to - local - folder ). # # # hf _ hub _ etag _ timeout integer value to define the number of seconds to wait for server response when fetching the latest metadata from a repo before downloading a file. if the request times out, ` huggingface"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_5_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_5_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "# # hf _ hub _ etag _ timeout integer value to define the number of seconds to wait for server response when fetching the latest metadata from a repo before downloading a file. if the request times out, ` huggingface _ hub ` will default to the locally cached files. setting a lower value speeds up the workflow for machines with a slow connection that have already cached files. a higher value guarantees the metadata call to succeed in more cases. default to 10s. # # # hf _ hub _ download _ timeout integer value to define the number of seconds to wait for server response when downloading a file. if the request times out, a timeouterror is raised. setting a higher value is beneficial on machine with a slow connection. a smaller value makes the process fail quicker in case of complete network outage. default to 10s. # # boolean values the following environment variables expect a boolean value. the variable will be"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_6_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_6_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "value is beneficial on machine with a slow connection. a smaller value makes the process fail quicker in case of complete network outage. default to 10s. # # boolean values the following environment variables expect a boolean value. the variable will be considered as ` true ` if its value is one of ` { \" 1 \", \" on \", \" yes \", \" true \" } ` ( case - insensitive ). any other value ( or undefined ) will be considered as ` false `. # # # hf _ hub _ offline if set, no http calls will me made to the hugging face hub. if you try to download files, only the cached files will be accessed. if no cache file is detected, an error is raised this is useful in case your network is slow and you don ' t care about having the latest version of a file. if ` hf _ hub _ offline = 1 ` is set as environment variable and you call"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_7_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_7_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "detected, an error is raised this is useful in case your network is slow and you don ' t care about having the latest version of a file. if ` hf _ hub _ offline = 1 ` is set as environment variable and you call any method of [ ` hfapi ` ], an [ ` ~ huggingface _ hub. utils. offlinemodeisenabled ` ] exception will be raised. * * note : * * even if the latest version of a file is cached, calling ` hf _ hub _ download ` still triggers a http request to check that a new version is not available. setting ` hf _ hub _ offline = 1 ` will skip this call which speeds up your loading time. # # # hf _ hub _ disable _ implicit _ token authentication is not mandatory for every requests to the hub. for instance, requesting details about ` \" gpt2 \" ` model does not require to be authenticated. however"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_8_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_8_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "time. # # # hf _ hub _ disable _ implicit _ token authentication is not mandatory for every requests to the hub. for instance, requesting details about ` \" gpt2 \" ` model does not require to be authenticated. however, if a user is [ logged in ] (.. / package _ reference / login ), the default behavior will be to always send the token in order to ease user experience ( never get a http 401 unauthorized ) when accessing private or gated repositories. for privacy, you can disable this behavior by setting ` hf _ hub _ disable _ implicit _ token = 1 `. in this case, the token will be sent only for \" write - access \" calls ( example : create a commit ). * * note : * * disabling implicit sending of token can have weird side effects. for example, if you want to list all models on the hub, your private models will not be listed."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_9_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_9_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "calls ( example : create a commit ). * * note : * * disabling implicit sending of token can have weird side effects. for example, if you want to list all models on the hub, your private models will not be listed. you would need to explicitly pass ` token = true ` argument in your script. # # # hf _ hub _ disable _ progress _ bars for time consuming tasks, ` huggingface _ hub ` displays a progress bar by default ( using tqdm ). you can disable all the progress bars at once by setting ` hf _ hub _ disable _ progress _ bars = 1 `. # # # hf _ hub _ disable _ symlinks _ warning if you are on a windows machine, it is recommended to enable the developer mode or to run ` huggingface _ hub ` in admin mode. if not, ` huggingface _ hub ` will not be able to create symlinks in your cache"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_10_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_10_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "are on a windows machine, it is recommended to enable the developer mode or to run ` huggingface _ hub ` in admin mode. if not, ` huggingface _ hub ` will not be able to create symlinks in your cache system. you will be able to execute any script but your user experience will be degraded as some huge files might end - up duplicated on your hard - drive. a warning message is triggered to warn you about this behavior. set ` hf _ hub _ disable _ symlinks _ warning = 1 `, to disable this warning. for more details, see [ cache limitations ] (.. / guides / manage - cache # limitations ). # # # hf _ hub _ disable _ experimental _ warning some features of ` huggingface _ hub ` are experimental. this means you can use them but we do not guarantee they will be maintained in the future. in particular, we might update the api or behavior of such"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_11_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_11_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "_ disable _ experimental _ warning some features of ` huggingface _ hub ` are experimental. this means you can use them but we do not guarantee they will be maintained in the future. in particular, we might update the api or behavior of such features without any deprecation cycle. a warning message is triggered when using an experimental feature to warn you about it. if you ' re comfortable debugging any potential issues using an experimental feature, you can set ` hf _ hub _ disable _ experimental _ warning = 1 ` to disable the warning. if you are using an experimental feature, please let us know! your feedback can help us design and improve it. # # # hf _ hub _ disable _ telemetry by default, some data is collected by hf libraries ( ` transformers `, ` datasets `, ` gradio `,.. ) to monitor usage, debug issues and help prioritize features. each library defines"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_12_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_12_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "##try by default, some data is collected by hf libraries ( ` transformers `, ` datasets `, ` gradio `,.. ) to monitor usage, debug issues and help prioritize features. each library defines its own policy ( i. e. which usage to monitor ) but the core implementation happens in ` huggingface _ hub ` ( see [ ` send _ telemetry ` ] ). you can set ` hf _ hub _ disable _ telemetry = 1 ` as environment variable to globally disable telemetry. # # # hf _ hub _ enable _ hf _ transfer set to ` true ` for faster uploads and downloads from the hub using ` hf _ transfer `. by default, ` huggingface _ hub ` uses the python - based ` requests. get ` and ` requests. post ` functions. although these are reliable and versatile, they may not be the most efficient choice for machines with high"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_13_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_13_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "_ transfer `. by default, ` huggingface _ hub ` uses the python - based ` requests. get ` and ` requests. post ` functions. although these are reliable and versatile, they may not be the most efficient choice for machines with high bandwidth. [ ` hf _ transfer ` ] ( https : / / github. com / huggingface / hf _ transfer ) is a rust - based package developed to maximize the bandwidth used by dividing large files into smaller parts and transferring them simultaneously using multiple threads. this approach can potentially double the transfer speed. to use ` hf _ transfer `, you need to install it separately [ from pypi ] ( https : / / pypi. org / project / hf - transfer / ) and set ` hf _ hub _ enable _ hf _ transfer = 1 ` as an environment variable. please note that using ` hf _ transfer ` comes with certain limitations. since it is not purely python - based"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_14_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_14_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "##f - transfer / ) and set ` hf _ hub _ enable _ hf _ transfer = 1 ` as an environment variable. please note that using ` hf _ transfer ` comes with certain limitations. since it is not purely python - based, debugging errors may be challenging. additionally, ` hf _ transfer ` lacks several user - friendly features such as resumable downloads and proxies. these omissions are intentional to maintain the simplicity and speed of the rust logic. consequently, ` hf _ transfer ` is not enabled by default in ` huggingface _ hub `. # # deprecated environment variables in order to standardize all environment variables within the hugging face ecosystem, some variables have been marked as deprecated. although they remain functional, they no longer take precedence over their replacements. the following table outlines the deprecated variables and their corresponding alternatives : | deprecated variable | replacement | | - - - | - - - | | `"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_15_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_15_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "##precated. although they remain functional, they no longer take precedence over their replacements. the following table outlines the deprecated variables and their corresponding alternatives : | deprecated variable | replacement | | - - - | - - - | | ` huggingface _ hub _ cache ` | ` hf _ hub _ cache ` | | ` huggingface _ assets _ cache ` | ` hf _ assets _ cache ` | | ` hugging _ face _ hub _ token ` | ` hf _ token ` | | ` huggingface _ hub _ verbosity ` | ` hf _ hub _ verbosity ` | # # from external tools some environment variables are not specific to ` huggingface _ hub ` but are still taken into account when they are set. # # # do _ not _ track boolean value. equivalent to ` hf _ hub _ disable _ telemetry `. when set to true, telemetry is globally disabled in the hugging face python ecosystem ("
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_16_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_16_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "they are set. # # # do _ not _ track boolean value. equivalent to ` hf _ hub _ disable _ telemetry `. when set to true, telemetry is globally disabled in the hugging face python ecosystem ( ` transformers `, ` diffusers `, ` gradio `, etc. ). see https : / / consoledonottrack. com / for more details. # # # no _ color boolean value. when set, ` huggingface - cli ` tool will not print any ansi color. see [ no - color. org ] ( https : / / no - color. org / ). # # # xdg _ cache _ home used only when ` hf _ home ` is not set! this is the default way to configure where [ user - specific non - essential ( cached ) data should be written ] ( https : / / wiki. archlinux. org /"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_17_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_17_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt"
              },
              "prompt": "",
              "text": "hf _ home ` is not set! this is the default way to configure where [ user - specific non - essential ( cached ) data should be written ] ( https : / / wiki. archlinux. org / title / xdg _ base _ directory ) on linux machines. if ` hf _ home ` is not set, the default home will be ` \" $ xdg _ cache _ home / huggingface \" ` instead of ` \" ~ /. cache / huggingface \" `."
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_tokenizers_blob_main_bindings_node_npm_linux-x64-musl_README.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_tokenizers_blob_main_bindings_node_npm_linux-x64-musl_README.md.txt"
              },
              "prompt": "",
              "text": "` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "! - - copyright 2022 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. note that this file is in markdown but contain specific syntax for our doc - builder ( similar to mdx ) that may not be rendered properly in your markdown viewer. - - > # instantiating a big model when you want to use a very big pretrained model, one challenge is to minimize the use of the ram. the usual workflow"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": ") that may not be rendered properly in your markdown viewer. - - > # instantiating a big model when you want to use a very big pretrained model, one challenge is to minimize the use of the ram. the usual workflow from pytorch is : 1. create your model with random weights. 2. load your pretrained weights. 3. put those pretrained weights in your random model. step 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several gigabytes, those two copies can make you get out of ram. even worse, if you are using ` torch. distributed ` to launch a distributed training, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "training, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_4_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_4_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~ pretrainedmodel. save _ pretrained ` ], you will get a new folder with two files : the config of the model and its weights : ` ` ` py > > > import os > > > import tempfile > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir )... print ( sorted ( os. listdir ( tmp _ dir ) ) ) [ ' config. json ', ' pytorch _ model. bin ' ] ` ` ` now let ' s use a maximum shard size of 200mb : `"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_5_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_5_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "listdir ( tmp _ dir ) ) ) [ ' config. json ', ' pytorch _ model. bin ' ] ` ` ` now let ' s use a maximum shard size of 200mb : ` ` ` py > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... print ( sorted ( os. listdir ( tmp _ dir ) ) ) [ ' config. json ', ' pytorch _ model - 00001 - of - 00003. bin ', ' pytorch _ model - 00002 - of - 00003. bin ', ' pytorch _ model - 00003 - of - 00003. bin ', ' pytor"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_6_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_6_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": ". bin ', ' pytorch _ model - 00002 - of - 00003. bin ', ' pytorch _ model - 00003 - of - 00003. bin ', ' pytorch _ model. bin. index. json ' ] ` ` ` on top of the configuration of the model, we see three different weights files, and an ` index. json ` file which is our index. a checkpoint like this can be fully reloaded using the [ ` ~ pretrainedmodel. from _ pretrained ` ] method : ` ` ` py > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_7_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_7_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": ":... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir ) ` ` ` the main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in ram to the model size plus the size of the biggest shard. behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. we can load that index like any json and get a dictionary : ` ` ` py > > > import json > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ sha"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_8_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_8_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "` py > > > import json > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... with open ( os. path. join ( tmp _ dir, \" pytorch _ model. bin. index. json \" ), \" r \" ) as f :... index = json. load ( f ) > > > print ( index. keys ( ) ) dict _ keys ( [ ' metadata ', ' weight _ map ' ] ) ` ` ` the metadata just consists of the total size of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] { ' total _ size ' : 433245184 } ` ` ` the weights map"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_9_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_9_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "size of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] { ' total _ size ' : 433245184 } ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] { ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_10_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_10_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #"
            },
            {
              "fill_history": {
                "chunkId": "Overlapping_Chunking_11_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "metavars": {
                "chunkId": "Overlapping_Chunking_11_HuggingFace_Tokenizers",
                "chunkLibrary": "HuggingFace Tokenizers",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt"
              },
              "prompt": "",
              "text": "... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )"
            }
          ],
          "methods": [
            {
              "baseMethod": "overlapping_langchain",
              "emoji": "ðŸŒ",
              "key": "8e5f203f-6976-4984-85e6-eeda1b60dbe0",
              "library": "LangChain TextSplitter",
              "methodName": "Overlapping Chunking",
              "settings": {
                "chunk_overlap": 50,
                "chunk_size": 300
              }
            },
            {
              "baseMethod": "overlapping_huggingface_tokenizers",
              "emoji": "ðŸ¤—",
              "key": "788c09ad-3b15-4062-a676-f1bb77abed07",
              "library": "HuggingFace Tokenizers",
              "methodName": "Overlapping Chunking",
              "settings": {
                "chunk_size": 300,
                "tokenizer_model": "bert-base-uncased"
              }
            }
          ],
          "output": {
            "HuggingFace Tokenizers": [
              {
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt",
                "id": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "gradio demo : blocks _ multiple _ event _ triggers ` ` `! pip install - q gradio plotly pypistats ` ` ` ` ` ` import gradio as gr import pypistats from datetime import date from dateutil. relativedelta import relativedelta import pandas as pd def get _ plot ( lib, time ) : data = pypistats. overall ( lib, total = true, format = \" pandas \" ) data = data. groupby ( \" category \" ). get _ group ( \" with _ mirrors \" ). sort _ values ( \" date \" ) start _ date = date. today ( ) - relativedelta ( months = int ( time. split ( \" \" ) [ 0 ] ) ) data = data [ ( data [ ' date ' ] > str ( start _ date ) ) ] data. date = pd. to _ datetime ( pd. to _ date"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt",
                "id": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "( time. split ( \" \" ) [ 0 ] ) ) data = data [ ( data [ ' date ' ] > str ( start _ date ) ) ] data. date = pd. to _ datetime ( pd. to _ datetime ( data. date ) ) return gr. lineplot ( value = data, x = \" date \", y = \" downloads \", tooltip = [ ' date ', ' downloads ' ], title = f \" pypi downloads of { lib } over last { time } \", overlay _ point = true, height = 400, width = 900 ) with gr. blocks ( ) as demo : gr. markdown ( \" \" \" # # pypi download stats see live download stats for all of hugging face ' s open - source libraries \" \" \" ) with gr. row ( ) : lib = gr. dropdown ( [ \" transformers \", \" datasets \", \""
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt",
                "id": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "download stats see live download stats for all of hugging face ' s open - source libraries \" \" \" ) with gr. row ( ) : lib = gr. dropdown ( [ \" transformers \", \" datasets \", \" huggingface - hub \", \" gradio \", \" accelerate \" ], value = \" gradio \", label = \" library \" ) time = gr. dropdown ( [ \" 3 months \", \" 6 months \", \" 9 months \", \" 12 months \" ], value = \" 3 months \", label = \" downloads over the last... \" ) plt = gr. lineplot ( ) # you can add multiple event triggers in 2 lines like this for event in [ lib. change, time. change, demo. load ] : event ( get _ plot, [ lib, time ], [ plt ] ) if _ _ name _ _ = = \" _ _ main _"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt",
                "id": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "this for event in [ lib. change, time. change, demo. load ] : event ( get _ plot, [ lib, time ], [ plt ] ) if _ _ name _ _ = = \" _ _ main _ _ \" : demo. launch ( ) ` ` `"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt",
                "id": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "contributing a guide want to help teach gradio? consider contributing a guide! broadly speaking, there are two types of guides : - * * use cases * * : guides that cover step - by - step how to build a particular type of machine learning demo or app using gradio. here ' s an example : [ _ creating a chatbot _ ] ( https : / / github. com / gradio - app / gradio / blob / master / guides / creating _ a _ chatbot. md ) - * * feature explanation * * : guides that describe in detail a particular feature of gradio. here ' s an example : [ _ using flagging _ ] ( https : / / github. com / gradio - app / gradio / blob / master / guides / using _ flagging. md ) we encourage you to submit either type of guide! ( looking for ideas? we may also"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt",
                "id": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "/ github. com / gradio - app / gradio / blob / master / guides / using _ flagging. md ) we encourage you to submit either type of guide! ( looking for ideas? we may also have open [ issues ] ( https : / / github. com / gradio - app / gradio / issues? q = is % 3aopen + is % 3aissue + label % 3aguides ) where users have asked for guides on particular topics ) # # guide structure as you can see with the previous examples, guides are standard markdown documents. they usually : - start with an introduction section describing the topic - include subheadings to make articles easy to navigate - include real code snippets that make it easy to follow along and implement the guide - include embedded gradio demos to make them more interactive and provide immediate demonstrations of the topic being discussed. these gradio demos are hosted on ["
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt",
                "id": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "- include real code snippets that make it easy to follow along and implement the guide - include embedded gradio demos to make them more interactive and provide immediate demonstrations of the topic being discussed. these gradio demos are hosted on [ hugging face spaces ] ( https : / / huggingface. co / spaces ) and are embedded using the standard \\ < iframe \\ > tag. # # how to contribute a guide 1. clone or fork this ` gradio ` repo 2. add a new markdown document with a descriptive title to the ` / guides ` folder 3. write your guide in standard markdown! embed gradio demos wherever helpful 4. add a list of ` related _ spaces ` at the top of the markdown document ( see the previously linked guides for how to do this ) 5. add 3 ` tags ` at the top of the markdown document to help users find your guide ( again, see the previously linked guides for how"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt",
                "id": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "the top of the markdown document ( see the previously linked guides for how to do this ) 5. add 3 ` tags ` at the top of the markdown document to help users find your guide ( again, see the previously linked guides for how to do this ) 6. open a pr to have your guide reviewed that ' s it! we ' re looking forward to reading your guide"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "@ gradio / gallery # # 0. 4. 14 # # # patch changes - updated dependencies [ [ ` 828fb9e ` ] ( https : / / github. com / gradio - app / gradio / commit / 828fb9e6ce15b6ea08318675a2361117596a1b5d ), [ ` 73268ee ` ] ( https : / / github. com / gradio - app / gradio / commit / 73268ee2e39f23ebdd1e927cb49b8d79c4b9a144 ) ] : - @ gradio / client @ 0. 9. 3 - @ gradio / statustracker @ 0. 4. 3 - @ gradio / atoms @ 0. 4. 1 - @ gradio / image @ 0. 5. 3"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "##o / client @ 0. 9. 3 - @ gradio / statustracker @ 0. 4. 3 - @ gradio / atoms @ 0. 4. 1 - @ gradio / image @ 0. 5. 3 - @ gradio / upload @ 0. 5. 6 # # 0. 4. 13 # # # patch changes - updated dependencies [ [ ` 245d58e ` ] ( https : / / github. com / gradio - app / gradio / commit / 245d58eff788e8d44a59d37a2d9b26d0f08a62b4 ) ] : - @ gradio / client @ 0. 9. 2 - @ gradio / image @ 0. 5. 2 - @ gradio / upload @ 0. 5. 5 # # 0. 4. 12 # # # patch changes -"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "##adio / client @ 0. 9. 2 - @ gradio / image @ 0. 5. 2 - @ gradio / upload @ 0. 5. 5 # # 0. 4. 12 # # # patch changes - updated dependencies [ [ ` 5d51fbc ` ] ( https : / / github. com / gradio - app / gradio / commit / 5d51fbce7826da840a2fd4940feb5d9ad6f1bc5a ), [ ` 34f9431 ` ] ( https : / / github. com / gradio - app / gradio / commit / 34f943101bf7dd6b8a8974a6131c1ed7c4a0dac0 ) ] : - @ gradio / upload @ 0. 5. 4 - @"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "##f943101bf7dd6b8a8974a6131c1ed7c4a0dac0 ) ] : - @ gradio / upload @ 0. 5. 4 - @ gradio / client @ 0. 9. 1 - @ gradio / image @ 0. 5. 1 # # 0. 4. 11 # # # fixes - [ # 6487 ] ( https : / / github. com / gradio - app / gradio / pull / 6487 ) [ ` 9a5811d ` ] ( https : / / github. com / gradio - app / gradio / commit / 9a5811df9218b622af59ba243a937a9c36ba00f9 ) - fix the download button of the ` gr. gallery ( ) ` component to work. thanks ["
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_4_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "commit / 9a5811df9218b622af59ba243a937a9c36ba00f9 ) - fix the download button of the ` gr. gallery ( ) ` component to work. thanks [ @ whitphx ] ( https : / / github. com / whitphx )! # # 0. 4. 10 # # # patch changes - updated dependencies [ [ ` b639e04 ` ] ( https : / / github. com / gradio - app / gradio / commit / b639e040741e6c0d9104271c81415d7befbd8cf3 ), [ ` 206af31 ` ] ( https : / / github. com / gradio - app / gradio / commit / 206af31d7c1a31013364a44"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_5_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "##8cf3 ), [ ` 206af31 ` ] ( https : / / github. com / gradio - app / gradio / commit / 206af31d7c1a31013364a44e9b40cf8df304ba50 ) ] : - @ gradio / image @ 0. 4. 2 - @ gradio / icons @ 0. 3. 1 - @ gradio / atoms @ 0. 3. 1 - @ gradio / statustracker @ 0. 4. 1 - @ gradio / upload @ 0. 5. 2 # # 0. 4. 9 # # # patch changes - updated dependencies [ [ ` 71f1a1f99 ` ] ( https : / / github. com / gradio - app / gradio / commit / 71f1a1f9931489d465c2c"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_6_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "[ ` 71f1a1f99 ` ] ( https : / / github. com / gradio - app / gradio / commit / 71f1a1f9931489d465c2c1302a5c8d768a3cd23a ) ] : - @ gradio / client @ 0. 8. 2 - @ gradio / image @ 0. 4. 1 - @ gradio / upload @ 0. 5. 1 # # 0. 4. 8 # # # patch changes - updated dependencies [ [ ` 9caddc17b ` ] ( https : / / github. com / gradio - app / gradio / commit / 9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8 ) ] : - @ gradio / atoms @ 0"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_7_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "- app / gradio / commit / 9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8 ) ] : - @ gradio / atoms @ 0. 3. 0 - @ gradio / icons @ 0. 3. 0 - @ gradio / image @ 0. 4. 0 - @ gradio / statustracker @ 0. 4. 0 - @ gradio / upload @ 0. 5. 0 # # 0. 4. 7 # # # patch changes - updated dependencies [ [ ` 2f805a7dd ` ] ( https : / / github. com / gradio - app / gradio / commit / 2f805a7dd3d2b64b098f659dadd5d01258290521 ), [ ` f816136a0"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_8_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "##o - app / gradio / commit / 2f805a7dd3d2b64b098f659dadd5d01258290521 ), [ ` f816136a0 ` ] ( https : / / github. com / gradio - app / gradio / commit / f816136a039fa6011be9c4fb14f573e4050a681a ) ] : - @ gradio / image @ 0. 3. 6 - @ gradio / upload @ 0. 4. 2 - @ gradio / atoms @ 0. 2. 2 - @ gradio / icons @ 0. 2. 1 - @ gradio / statustracker @ 0. 3. 2 # # 0. 4. 6 # # # patch changes - updated dependencies [ [ ` 324867f63 `"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_9_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "##o / icons @ 0. 2. 1 - @ gradio / statustracker @ 0. 3. 2 # # 0. 4. 6 # # # patch changes - updated dependencies [ [ ` 324867f63 ` ] ( https : / / github. com / gradio - app / gradio / commit / 324867f63c920113d89a565892aa596cf8b1e486 ) ] : - @ gradio / client @ 0. 8. 1 - @ gradio / image @ 0. 3. 5 - @ gradio / upload @ 0. 4. 1 # # 0. 4. 5 # # # patch changes - updated dependencies [ [ ` 6204ccac5 ` ] ( https : / / github. com / gradio - app / gradio / commit / 6204ccac59"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_10_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "4. 5 # # # patch changes - updated dependencies [ [ ` 6204ccac5 ` ] ( https : / / github. com / gradio - app / gradio / commit / 6204ccac5967763e0ebde550d04d12584243a120 ), [ ` 4d3aad33a ` ] ( https : / / github. com / gradio - app / gradio / commit / 4d3aad33a0b66639dbbb2928f305a79fb7789b2d ), [ ` 854b482f5 ` ] ( https : / / github. com / gradio - app / gradio / commit / 854b482f598e0dc47673846631643c079576da"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_11_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "( https : / / github. com / gradio - app / gradio / commit / 854b482f598e0dc47673846631643c079576da9c ), [ ` f1409f95e ` ] ( https : / / github. com / gradio - app / gradio / commit / f1409f95ed39c5565bed6a601e41f94e30196a57 ) ] : - @ gradio / image @ 0. 3. 4 - @ gradio / upload @ 0. 4. 0 - @ gradio / client @ 0. 8. 0 # # 0. 4. 4 # # # patch changes - updated dependencies [ [ ` bca6c2c80 ` ] ( https : / / github. com / gradio"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_12_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "/ client @ 0. 8. 0 # # 0. 4. 4 # # # patch changes - updated dependencies [ [ ` bca6c2c80 ` ] ( https : / / github. com / gradio - app / gradio / commit / bca6c2c80f7e5062427019de45c282238388af95 ), [ ` 3cdeabc68 ` ] ( https : / / github. com / gradio - app / gradio / commit / 3cdeabc6843000310e1a9e1d17190ecbf3bbc780 ), [ ` fad92c29d ` ] ( https : / / github. com / gradio - app / gradio / commit / fad92c29dc1f5cd84341aa"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_13_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": ", [ ` fad92c29d ` ] ( https : / / github. com / gradio - app / gradio / commit / fad92c29dc1f5cd84341aae417c495b33e01245f ) ] : - @ gradio / client @ 0. 7. 2 - @ gradio / atoms @ 0. 2. 1 - @ gradio / upload @ 0. 3. 3 - @ gradio / image @ 0. 3. 3 - @ gradio / statustracker @ 0. 3. 1 # # 0. 4. 3 # # # fixes - [ # 6288 ] ( https : / / github. com / gradio - app / gradio / pull / 6288 ) [ ` 92278729e ` ] ( https : / / github. com"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_14_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "##88 ] ( https : / / github. com / gradio - app / gradio / pull / 6288 ) [ ` 92278729e ` ] ( https : / / github. com / gradio - app / gradio / commit / 92278729ee008126af15ffe6be399236211b2f34 ) - gallery preview fix and optionally skip download of urls in postprcess. thanks [ @ freddyaboulton ] ( https : / / github. com / freddyaboulton )! # # 0. 4. 2 # # # fixes - [ # 6277 ] ( https : / / github. com / gradio - app / gradio / pull / 6277 ) [ ` 5fe091367 ` ] ( https : / / github. com /"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_15_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "] ( https : / / github. com / gradio - app / gradio / pull / 6277 ) [ ` 5fe091367 ` ] ( https : / / github. com / gradio - app / gradio / commit / 5fe091367fbe0eecdd504aa734ca1c70b0621f52 ) - handle selected _ index prop change for gallery. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! # # 0. 4. 1 # # # patch changes - updated dependencies [ [ ` 2ba14b284 ` ] ( https : / / github. com / gradio - app / gradio / commit / 2ba14b284f908aa13859f4337167a157075a68eb"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_16_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "] ( https : / / github. com / gradio - app / gradio / commit / 2ba14b284f908aa13859f4337167a157075a68eb ) ] : - @ gradio / client @ 0. 7. 1 - @ gradio / image @ 0. 3. 1 - @ gradio / upload @ 0. 3. 1 # # 0. 4. 0 # # # features - [ # 5498 ] ( https : / / github. com / gradio - app / gradio / pull / 5498 ) [ ` 287fe6782 ` ] ( https : / / github. com / gradio - app / gradio / commit / 287fe6782825479513e79a5cf0ba0fbfe51443d7 ) -"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_17_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": ": / / github. com / gradio - app / gradio / commit / 287fe6782825479513e79a5cf0ba0fbfe51443d7 ) - fix circular dependency with client + upload. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5498 ] ( https : / / github. com / gradio - app / gradio / pull / 5498 ) [ ` 287fe6782 ` ] ( https : / / github. com / gradio - app / gradio / commit / 287fe6782825479513e79a5cf0ba0fbfe51443d7 ) - clean root url. thanks [ @ pngwn ] ( https : / / github. com"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_18_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "##fe6782825479513e79a5cf0ba0fbfe51443d7 ) - clean root url. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5498 ] ( https : / / github. com / gradio - app / gradio / pull / 5498 ) [ ` 287fe6782 ` ] ( https : / / github. com / gradio - app / gradio / commit / 287fe6782825479513e79a5cf0ba0fbfe51443d7 ) - image v4. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5498 ] ( https : / / github. com / gradio"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_19_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": ") - image v4. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5498 ] ( https : / / github. com / gradio - app / gradio / pull / 5498 ) [ ` 287fe6782 ` ] ( https : / / github. com / gradio - app / gradio / commit / 287fe6782825479513e79a5cf0ba0fbfe51443d7 ) - publish all components to npm. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5498 ] ( https : / / github. com / gradio - app / gradio / pull / 5498 ) [ ` 287fe6782 ` ] ("
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_20_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "pngwn )! - [ # 5498 ] ( https : / / github. com / gradio - app / gradio / pull / 5498 ) [ ` 287fe6782 ` ] ( https : / / github. com / gradio - app / gradio / commit / 287fe6782825479513e79a5cf0ba0fbfe51443d7 ) - custom components. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! # # 0. 4. 0 - beta. 9 # # # features - [ # 6143 ] ( https : / / github. com / gradio - app / gradio / pull / 6143 ) [ ` e4f7b4b40 ` ] ( https : / / github. com / gradio"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_21_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": ": / / github. com / gradio - app / gradio / pull / 6143 ) [ ` e4f7b4b40 ` ] ( https : / / github. com / gradio - app / gradio / commit / e4f7b4b409323b01aa01b39e15ce6139e29aa073 ) - fix circular dependency with client + upload. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 6136 ] ( https : / / github. com / gradio - app / gradio / pull / 6136 ) [ ` 667802a6c ` ] ( https : / / github. com / gradio - app / gradio / commit / 667802a6cdbfb2ce45"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_22_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "/ 6136 ) [ ` 667802a6c ` ] ( https : / / github. com / gradio - app / gradio / commit / 667802a6cdbfb2ce454a3be5a78e0990b194548a ) - js component documentation. thanks [ @ freddyaboulton ] ( https : / / github. com / freddyaboulton )! - [ # 6094 ] ( https : / / github. com / gradio - app / gradio / pull / 6094 ) [ ` c476bd5a5 ` ] ( https : / / github. com / gradio - app / gradio / commit / c476bd5a5b70836163b9c69bf4bfe068b17fbe13 )"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_23_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "github. com / gradio - app / gradio / commit / c476bd5a5b70836163b9c69bf4bfe068b17fbe13 ) - image v4. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! # # 0. 4. 0 - beta. 8 # # # features - [ # 6016 ] ( https : / / github. com / gradio - app / gradio / pull / 6016 ) [ ` 83e947676 ` ] ( https : / / github. com / gradio - app / gradio / commit / 83e947676d327ca2ab6ae2a2d710c78961c771a0 ) - format js in v4"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_24_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "- app / gradio / commit / 83e947676d327ca2ab6ae2a2d710c78961c771a0 ) - format js in v4 branch. thanks [ @ freddyaboulton ] ( https : / / github. com / freddyaboulton )! # # # fixes - [ # 6046 ] ( https : / / github. com / gradio - app / gradio / pull / 6046 ) [ ` dbb7de5e0 ` ] ( https : / / github. com / gradio - app / gradio / commit / dbb7de5e02c53fee05889d696d764d212cb96c74 ) - fix tests. thanks [ @ pngwn ] ( https : / / github"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_25_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "##7de5e02c53fee05889d696d764d212cb96c74 ) - fix tests. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! # # 0. 4. 0 - beta. 7 # # # patch changes - updated dependencies [ ] : - @ gradio / image @ 0. 3. 0 - beta. 7 # # 0. 4. 0 - beta. 6 # # # features - [ # 5960 ] ( https : / / github. com / gradio - app / gradio / pull / 5960 ) [ ` 319c30f3f ` ] ( https : / / github. com / gradio - app / gradio / commit / 319c30f3fccf23bfe1da6c9b132a6a"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_26_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "##f3f ` ] ( https : / / github. com / gradio - app / gradio / commit / 319c30f3fccf23bfe1da6c9b132a6a99d59652f7 ) - rererefactor frontend files. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5938 ] ( https : / / github. com / gradio - app / gradio / pull / 5938 ) [ ` 13ed8a485 ` ] ( https : / / github. com / gradio - app / gradio / commit / 13ed8a485d5e31d7d75af87fe8654b661edcca93 ) - v4 : use beta release versions for ' @ gradio '"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_27_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "/ gradio / commit / 13ed8a485d5e31d7d75af87fe8654b661edcca93 ) - v4 : use beta release versions for ' @ gradio ' packages. thanks [ @ freddyaboulton ] ( https : / / github. com / freddyaboulton )! # # 0. 5. 1 # # # patch changes - updated dependencies [ [ ` b67115e8e ` ] ( https : / / github. com / gradio - app / gradio / commit / b67115e8e6e489fffd5271ea830211863241ddc5 ), [ ` 8d909624f ` ] ( https : / / github. com / gradio - app / gradio / commit / 8d909624"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_28_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "##211863241ddc5 ), [ ` 8d909624f ` ] ( https : / / github. com / gradio - app / gradio / commit / 8d909624f61a49536e3c0f71cb2d9efe91216219 ), [ ` e70805d54 ` ] ( https : / / github. com / gradio - app / gradio / commit / e70805d54cc792452545f5d8eccc1aa0212a4695 ) ] : - @ gradio / image @ 0. 4. 0 - @ gradio / atoms @ 0. 2. 0 - @ gradio / statustracker @ 0. 2. 3 - @ gradio / upload @ 0. 3. 3 # # 0. 5"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_29_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "4. 0 - @ gradio / atoms @ 0. 2. 0 - @ gradio / statustracker @ 0. 2. 3 - @ gradio / upload @ 0. 3. 3 # # 0. 5. 0 # # # features - [ # 5780 ] ( https : / / github. com / gradio - app / gradio / pull / 5780 ) [ ` ed0f9a21b ` ] ( https : / / github. com / gradio - app / gradio / commit / ed0f9a21b04ad6b941b63d2ce45100dbd1abd5c5 ) - adds ` change ( ) ` event to ` gr. gallery `. thanks [ @ abidlabs ] ( https : / / github. com / abidlabs )! - [ # 5783 ] ("
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_30_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "##5 ) - adds ` change ( ) ` event to ` gr. gallery `. thanks [ @ abidlabs ] ( https : / / github. com / abidlabs )! - [ # 5783 ] ( https : / / github. com / gradio - app / gradio / pull / 5783 ) [ ` 4567788bd ` ] ( https : / / github. com / gradio - app / gradio / commit / 4567788bd1fc25df9322902ba748012e392b520a ) - adds the ability to set the ` selected _ index ` in a ` gr. gallery `. thanks [ @ abidlabs ] ( https : / / github. com / abidlabs )! # # # fixes - [ # 5798 ] ( https"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_31_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "_ index ` in a ` gr. gallery `. thanks [ @ abidlabs ] ( https : / / github. com / abidlabs )! # # # fixes - [ # 5798 ] ( https : / / github. com / gradio - app / gradio / pull / 5798 ) [ ` a0d3cc45c ` ] ( https : / / github. com / gradio - app / gradio / commit / a0d3cc45c6db48dc0db423c229b8fb285623cdc4 ) - fix ` gr. selectdata ` so that the target attribute is correctly attached, and the filedata is included in the data attribute with ` gr. gallery `. thanks [ @ abidlabs ] ( https : / / github. com / abidlabs )! # # 0"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_32_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "attribute is correctly attached, and the filedata is included in the data attribute with ` gr. gallery `. thanks [ @ abidlabs ] ( https : / / github. com / abidlabs )! # # 0. 4. 1 # # # fixes - [ # 5735 ] ( https : / / github. com / gradio - app / gradio / pull / 5735 ) [ ` abb5e9df4 ` ] ( https : / / github. com / gradio - app / gradio / commit / abb5e9df47989b2c56c2c312d74944678f9f2d4e ) - ensure images with no caption download in gallery. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )! - [ # 5754 ] ("
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_33_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "##8f9f2d4e ) - ensure images with no caption download in gallery. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )! - [ # 5754 ] ( https : / / github. com / gradio - app / gradio / pull / 5754 ) [ ` 502054848 ` ] ( https : / / github. com / gradio - app / gradio / commit / 502054848fdbe39fc03ec42445242b4e49b7affc ) - fix gallery ` columns ` and ` rows ` params. thanks [ @ abidlabs ] ( https : / / github. com / abidlabs )! # # 0. 4. 0 # # # features - [ # 5554 ] ( https : / / github. com / gr"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_34_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "##idlabs ] ( https : / / github. com / abidlabs )! # # 0. 4. 0 # # # features - [ # 5554 ] ( https : / / github. com / gradio - app / gradio / pull / 5554 ) [ ` 75ddeb390 ` ] ( https : / / github. com / gradio - app / gradio / commit / 75ddeb390d665d4484667390a97442081b49a423 ) - accessibility improvements. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )! # # 0. 3. 4 # # # patch changes - updated dependencies [ [ ` e0d61b8ba ` ] ( https : / / github. com / gradio - app / gradi"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_35_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": ")! # # 0. 3. 4 # # # patch changes - updated dependencies [ [ ` e0d61b8ba ` ] ( https : / / github. com / gradio - app / gradio / commit / e0d61b8baa0f6293f53b9bdb1647d42f9ae2583a ) ] : - @ gradio / image @ 0. 2. 4 # # 0. 3. 3 # # # fixes - [ # 5528 ] ( https : / / github. com / gradio - app / gradio / pull / 5528 ) [ ` dc86e4a7 ` ] ( https : / / github. com / gradio - app / gradio / commit / dc86e4a7e1c40b910c74558e6f88"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_36_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "##4a7 ` ] ( https : / / github. com / gradio - app / gradio / commit / dc86e4a7e1c40b910c74558e6f88fddf9b3292bc ) - lazy load all images. thanks [ @ aliabid94 ] ( https : / / github. com / aliabid94 )! # # 0. 3. 2 # # # patch changes - updated dependencies [ [ ` afac0006 ` ] ( https : / / github. com / gradio - app / gradio / commit / afac0006337ce2840cf497cd65691f2f60ee5912 ) ] : - @ gradio / statustracker @ 0. 2. 0 - @ gradio / image @ 0. 2. 2 -"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_37_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "##2840cf497cd65691f2f60ee5912 ) ] : - @ gradio / statustracker @ 0. 2. 0 - @ gradio / image @ 0. 2. 2 - @ gradio / utils @ 0. 1. 1 - @ gradio / atoms @ 0. 1. 2 - @ gradio / upload @ 0. 2. 1 # # 0. 3. 1 # # # patch changes - updated dependencies [ [ ` abf1c57d ` ] ( https : / / github. com / gradio - app / gradio / commit / abf1c57d7d85de0df233ee3b38aeb38b638477db ), [ ` 79d8f9d8 ` ] ( https : / / github. com / gradio - app / gradio"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_38_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "##0df233ee3b38aeb38b638477db ), [ ` 79d8f9d8 ` ] ( https : / / github. com / gradio - app / gradio / commit / 79d8f9d891901683c5a1b7486efb44eab2478c96 ) ] : - @ gradio / icons @ 0. 1. 0 - @ gradio / utils @ 0. 1. 0 - @ gradio / upload @ 0. 2. 0 - @ gradio / atoms @ 0. 1. 1 - @ gradio / image @ 0. 2. 1 - @ gradio / statustracker @ 0. 1. 1 # # 0. 3. 0 # # # highlights # # # # improve startup performance and markdown support ( [ # 5279 ] ( https"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_39_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": ". 2. 1 - @ gradio / statustracker @ 0. 1. 1 # # 0. 3. 0 # # # highlights # # # # improve startup performance and markdown support ( [ # 5279 ] ( https : / / github. com / gradio - app / gradio / pull / 5279 ) [ ` fe057300 ` ] ( https : / / github. com / gradio - app / gradio / commit / fe057300f0672c62dab9d9b4501054ac5d45a4ec ) ) # # # # # improved markdown support we now have better support for markdown in ` gr. markdown ` and ` gr. dataframe `. including syntax highlighting and github flavoured markdown. we also have more consistent markdown behaviour and styling. # # # # # various"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_40_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "have better support for markdown in ` gr. markdown ` and ` gr. dataframe `. including syntax highlighting and github flavoured markdown. we also have more consistent markdown behaviour and styling. # # # # # various performance improvements these improvements will be particularly beneficial to large applications. - rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of gradio. app startup for large applications is now around twice as fast. - optimised the mounting of individual components, leading to a modest performance improvement during startup ( ~ 30 % ). - corrected an issue that was causing markdown to re - render infinitely. - ensured that the ` gr. 3dmodel ` does re - render prematurely. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! # # # features - [ # 5215 ] ( https"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_41_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "that the ` gr. 3dmodel ` does re - render prematurely. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! # # # features - [ # 5215 ] ( https : / / github. com / gradio - app / gradio / pull / 5215 ) [ ` fbdad78a ` ] ( https : / / github. com / gradio - app / gradio / commit / fbdad78af4c47454cbb570f88cc14bf4479bbceb ) - lazy load interactive or static variants of a component individually, rather than loading both variants regardless. this change will improve performance for many applications. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5216 ] ( https : / / github"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_42_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "both variants regardless. this change will improve performance for many applications. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5216 ] ( https : / / github. com / gradio - app / gradio / pull / 5216 ) [ ` 4b58ea6d ` ] ( https : / / github. com / gradio - app / gradio / commit / 4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8 ) - update i18n tokens and locale files. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )! # # 0. 2. 0 # # # features - [ # 5025 ] ( https : / / github. com / gradio -"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_43_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "##ir ] ( https : / / github. com / hannahblair )! # # 0. 2. 0 # # # features - [ # 5025 ] ( https : / / github. com / gradio - app / gradio / pull / 5025 ) [ ` 6693660a ` ] ( https : / / github. com / gradio - app / gradio / commit / 6693660a790996f8f481feaf22a8c49130d52d89 ) - add download button to selected images in ` gallery `. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )! # # 0. 1. 0 # # # features - [ # 4995 ] ( https : / / github. com / gradio - app / gradio / pull / 49"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_44_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": ". com / hannahblair )! # # 0. 1. 0 # # # features - [ # 4995 ] ( https : / / github. com / gradio - app / gradio / pull / 4995 ) [ ` 3f8c210b ` ] ( https : / / github. com / gradio - app / gradio / commit / 3f8c210b01ef1ceaaf8ee73be4bf246b5b745bbf ) - implement left and right click in ` gallery ` component and show implicit images in ` gallery ` grid. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )!"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "hat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences."
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": ". input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_4_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. to get an output linked to our classification problem, we need to use the automodelforsequenceclassification class. it works exactly as the automodel class, except that it will build a model with a classification head. there is one auto class for each common nlp task in the transformers library. here, after giving our model the two sentences, we get a tensor of size two by two : one result for each sentence and for each possible label. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_5_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": ". those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs."
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "id": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "id": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "id": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "- diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors, detailed, 8k \" image = pipeline ( prompt, num _ inference _ steps = 25 ). images [ 0 ] ` ` ` < tip > check out the [ autopipeline ] (.. /.. / tutorials / autopipeline ) tutorial to learn how to use this api! < / tip > ` autopipeline ` supports text - to - image, image - to - image, and inpainting for the following diffusion models : - [ stable diffusion ] (. / stable _ diffusion / overview ) - [ controlnet ] (. / controlnet ) - [ stable diffusion xl ( sdxl ) ] (. / stable _ diffusion / stable _ diffusion _ xl ) - [ deepfloy"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "id": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "] (. / stable _ diffusion / overview ) - [ controlnet ] (. / controlnet ) - [ stable diffusion xl ( sdxl ) ] (. / stable _ diffusion / stable _ diffusion _ xl ) - [ deepfloyd if ] (. / deepfloyd _ if ) - [ kandinsky 2. 1 ] (. / kandinsky ) - [ kandinsky 2. 2 ] (. / kandinsky _ v22 ) # # autopipelinefortext2image [ [ autodoc ] ] autopipelinefortext2image - all - from _ pretrained - from _ pipe # # autopipelineforimage2image [ [ autodoc ] ] autopipelineforimage2image - all - from _ pretrained - from _ pipe # # autopipelineforinpainting [ [ autodoc ] ] autopipelineforinpaint"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "id": "Overlapping_Chunking_4_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "autodoc ] ] autopipelineforimage2image - all - from _ pretrained - from _ pipe # # autopipelineforinpainting [ [ autodoc ] ] autopipelineforinpainting - all - from _ pretrained - from _ pipe"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # diffusers â€™ ethical guidelines # # preamble [ diffusers ] ( https : / / huggingface. co / docs / diffusers / index ) provides pre - trained diffusion models and serves as a modular toolbox for inference and training. given its real case applications in the world and potential negative impacts on society, we think it"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "/ huggingface. co / docs / diffusers / index ) provides pre - trained diffusion models and serves as a modular toolbox for inference and training. given its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, users â€™ contributions, and usage of the diffusers library. the risks associated with using this technology are still being examined, but to name a few : copyrights issues for artists ; deep - fake exploitation ; sexual content generation in inappropriate contexts ; non - consensual impersonation ; harmful social biases perpetuating the oppression of marginalized groups. we will keep tracking risks and adapt the following guidelines based on the community ' s responsiveness and valuable feedback. # # scope the diffusers community will apply the following ethical guidelines to the project â€™ s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns. # # ethical"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "' s responsiveness and valuable feedback. # # scope the diffusers community will apply the following ethical guidelines to the project â€™ s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns. # # ethical guidelines the following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question. - * * transparency * * : we are committed to being transparent in managing prs, explaining our choices to users, and making technical decisions. - * * consistency * * : we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "same level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_4_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and social risks associated with diffusion technology. moreover, the community ' s input is invaluable in ensuring these features ' implementation and raising awareness with us. - [ * * community tab * * ] ( https : / / huggingface. co / docs / hub / repositories - pull - requests - discussions ) : it enables the community to discuss and better collaborate on a project. - * * bias exploration and evaluation * * : the hugging face team provides a [ space ] ( https : / / huggingface. co / spaces / society - ethics / diffusionbiasexplorer ) to demonstrate the biases in stable diffusion interactively. in this sense, we support and encourage bias explorers and evaluations. - *"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_5_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "https : / / huggingface. co / spaces / society - ethics / diffusionbiasexplorer ) to demonstrate the biases in stable diffusion interactively. in this sense, we support and encourage bias explorers and evaluations. - * * encouraging safety in deployment * * - [ * * safe stable diffusion * * ] ( https : / / huggingface. co / docs / diffusers / main / en / api / pipelines / stable _ diffusion / stable _ diffusion _ safe ) : it mitigates the well - known issue that models, like stable diffusion, that are trained on unfiltered, web - crawled datasets tend to suffer from inappropriate degeneration. related paper : [ safe latent diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / gi"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_6_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_7_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": ", access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https : / / huggingface. co / blog / open _ rail ), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use."
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_src_diffusers_experimental_README.md.txt",
                "id": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "diffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "! - - note that this file is in markdown but contain specific syntax for our doc - builder ( similar to mdx ) that may not be rendered properly in your markdown viewer. - - > # environment variables ` huggingface _ hub ` can be configured using environment variables. if you are unfamiliar with environment variable, here are generic articles about them [ on macos and linux ] ( https : / / linuxize. com / post / how - to - set - and - list - environment - variables - in - linux / ) and on [ windows ] ( https : / / phoenixnap. com / kb / windows - set - environment - variable ). this page will guide you through all environment variables specific to ` huggingface _ hub ` and their meaning. # # generic # # # hf _ inference _ endpoint to configure the inference api base url. you might want to set this variable if your organization is pointing at an api gateway rather"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "##face _ hub ` and their meaning. # # generic # # # hf _ inference _ endpoint to configure the inference api base url. you might want to set this variable if your organization is pointing at an api gateway rather than directly at the inference api. defaults to ` \" https : / / api - inference. huggingface. com \" `. # # # hf _ home to configure where ` huggingface _ hub ` will locally store data. in particular, your token and the cache will be stored in this folder. defaults to ` \" ~ /. cache / huggingface \" ` unless [ xdg _ cache _ home ] ( # xdgcachehome ) is set. # # # hf _ hub _ cache to configure where repositories from the hub will be cached locally ( models, datasets and spaces ). defaults to ` \" $ hf _ home /"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "set. # # # hf _ hub _ cache to configure where repositories from the hub will be cached locally ( models, datasets and spaces ). defaults to ` \" $ hf _ home / hub \" ` ( e. g. ` \" ~ /. cache / huggingface / hub \" ` by default ). # # # hf _ assets _ cache to configure where [ assets ] (.. / guides / manage - cache # caching - assets ) created by downstream libraries will be cached locally. those assets can be preprocessed data, files downloaded from github, logs,... defaults to ` \" $ hf _ home / assets \" ` ( e. g. ` \" ~ /. cache / huggingface / assets \" ` by default ). # # # hf _ token to configure the user access token to authenticate to the hub. if"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "/ assets \" ` ( e. g. ` \" ~ /. cache / huggingface / assets \" ` by default ). # # # hf _ token to configure the user access token to authenticate to the hub. if set, this value will overwrite the token stored on the machine ( in ` \" $ hf _ home / token \" ` ). for more details about authentication, check out [ this section ] (.. / quick - start # authentication ). # # # hf _ hub _ verbosity set the verbosity level of the ` huggingface _ hub ` ' s logger. must be one of ` { \" debug \", \" info \", \" warning \", \" error \", \" critical \" } `. defaults to ` \" warning \" `. for more details, see [ logging reference ] (.. / package _ reference / utilities # huggingface _ hub. utils. logging. get _ verb"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_4_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "error \", \" critical \" } `. defaults to ` \" warning \" `. for more details, see [ logging reference ] (.. / package _ reference / utilities # huggingface _ hub. utils. logging. get _ verbosity ). # # # hf _ hub _ local _ dir _ auto _ symlink _ threshold integer value to define under which size a file is considered as \" small \". when downloading files to a local directory, small files will be duplicated to ease user experience while bigger files are symlinked to save disk usage. for more details, see the [ download guide ] (.. / guides / download # download - files - to - local - folder ). # # # hf _ hub _ etag _ timeout integer value to define the number of seconds to wait for server response when fetching the latest metadata from a repo before downloading a file. if the request times out, ` huggingface"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_5_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "# # hf _ hub _ etag _ timeout integer value to define the number of seconds to wait for server response when fetching the latest metadata from a repo before downloading a file. if the request times out, ` huggingface _ hub ` will default to the locally cached files. setting a lower value speeds up the workflow for machines with a slow connection that have already cached files. a higher value guarantees the metadata call to succeed in more cases. default to 10s. # # # hf _ hub _ download _ timeout integer value to define the number of seconds to wait for server response when downloading a file. if the request times out, a timeouterror is raised. setting a higher value is beneficial on machine with a slow connection. a smaller value makes the process fail quicker in case of complete network outage. default to 10s. # # boolean values the following environment variables expect a boolean value. the variable will be"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_6_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "value is beneficial on machine with a slow connection. a smaller value makes the process fail quicker in case of complete network outage. default to 10s. # # boolean values the following environment variables expect a boolean value. the variable will be considered as ` true ` if its value is one of ` { \" 1 \", \" on \", \" yes \", \" true \" } ` ( case - insensitive ). any other value ( or undefined ) will be considered as ` false `. # # # hf _ hub _ offline if set, no http calls will me made to the hugging face hub. if you try to download files, only the cached files will be accessed. if no cache file is detected, an error is raised this is useful in case your network is slow and you don ' t care about having the latest version of a file. if ` hf _ hub _ offline = 1 ` is set as environment variable and you call"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_7_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "detected, an error is raised this is useful in case your network is slow and you don ' t care about having the latest version of a file. if ` hf _ hub _ offline = 1 ` is set as environment variable and you call any method of [ ` hfapi ` ], an [ ` ~ huggingface _ hub. utils. offlinemodeisenabled ` ] exception will be raised. * * note : * * even if the latest version of a file is cached, calling ` hf _ hub _ download ` still triggers a http request to check that a new version is not available. setting ` hf _ hub _ offline = 1 ` will skip this call which speeds up your loading time. # # # hf _ hub _ disable _ implicit _ token authentication is not mandatory for every requests to the hub. for instance, requesting details about ` \" gpt2 \" ` model does not require to be authenticated. however"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_8_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "time. # # # hf _ hub _ disable _ implicit _ token authentication is not mandatory for every requests to the hub. for instance, requesting details about ` \" gpt2 \" ` model does not require to be authenticated. however, if a user is [ logged in ] (.. / package _ reference / login ), the default behavior will be to always send the token in order to ease user experience ( never get a http 401 unauthorized ) when accessing private or gated repositories. for privacy, you can disable this behavior by setting ` hf _ hub _ disable _ implicit _ token = 1 `. in this case, the token will be sent only for \" write - access \" calls ( example : create a commit ). * * note : * * disabling implicit sending of token can have weird side effects. for example, if you want to list all models on the hub, your private models will not be listed."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_9_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "calls ( example : create a commit ). * * note : * * disabling implicit sending of token can have weird side effects. for example, if you want to list all models on the hub, your private models will not be listed. you would need to explicitly pass ` token = true ` argument in your script. # # # hf _ hub _ disable _ progress _ bars for time consuming tasks, ` huggingface _ hub ` displays a progress bar by default ( using tqdm ). you can disable all the progress bars at once by setting ` hf _ hub _ disable _ progress _ bars = 1 `. # # # hf _ hub _ disable _ symlinks _ warning if you are on a windows machine, it is recommended to enable the developer mode or to run ` huggingface _ hub ` in admin mode. if not, ` huggingface _ hub ` will not be able to create symlinks in your cache"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_10_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "are on a windows machine, it is recommended to enable the developer mode or to run ` huggingface _ hub ` in admin mode. if not, ` huggingface _ hub ` will not be able to create symlinks in your cache system. you will be able to execute any script but your user experience will be degraded as some huge files might end - up duplicated on your hard - drive. a warning message is triggered to warn you about this behavior. set ` hf _ hub _ disable _ symlinks _ warning = 1 `, to disable this warning. for more details, see [ cache limitations ] (.. / guides / manage - cache # limitations ). # # # hf _ hub _ disable _ experimental _ warning some features of ` huggingface _ hub ` are experimental. this means you can use them but we do not guarantee they will be maintained in the future. in particular, we might update the api or behavior of such"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_11_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "_ disable _ experimental _ warning some features of ` huggingface _ hub ` are experimental. this means you can use them but we do not guarantee they will be maintained in the future. in particular, we might update the api or behavior of such features without any deprecation cycle. a warning message is triggered when using an experimental feature to warn you about it. if you ' re comfortable debugging any potential issues using an experimental feature, you can set ` hf _ hub _ disable _ experimental _ warning = 1 ` to disable the warning. if you are using an experimental feature, please let us know! your feedback can help us design and improve it. # # # hf _ hub _ disable _ telemetry by default, some data is collected by hf libraries ( ` transformers `, ` datasets `, ` gradio `,.. ) to monitor usage, debug issues and help prioritize features. each library defines"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_12_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "##try by default, some data is collected by hf libraries ( ` transformers `, ` datasets `, ` gradio `,.. ) to monitor usage, debug issues and help prioritize features. each library defines its own policy ( i. e. which usage to monitor ) but the core implementation happens in ` huggingface _ hub ` ( see [ ` send _ telemetry ` ] ). you can set ` hf _ hub _ disable _ telemetry = 1 ` as environment variable to globally disable telemetry. # # # hf _ hub _ enable _ hf _ transfer set to ` true ` for faster uploads and downloads from the hub using ` hf _ transfer `. by default, ` huggingface _ hub ` uses the python - based ` requests. get ` and ` requests. post ` functions. although these are reliable and versatile, they may not be the most efficient choice for machines with high"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_13_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "_ transfer `. by default, ` huggingface _ hub ` uses the python - based ` requests. get ` and ` requests. post ` functions. although these are reliable and versatile, they may not be the most efficient choice for machines with high bandwidth. [ ` hf _ transfer ` ] ( https : / / github. com / huggingface / hf _ transfer ) is a rust - based package developed to maximize the bandwidth used by dividing large files into smaller parts and transferring them simultaneously using multiple threads. this approach can potentially double the transfer speed. to use ` hf _ transfer `, you need to install it separately [ from pypi ] ( https : / / pypi. org / project / hf - transfer / ) and set ` hf _ hub _ enable _ hf _ transfer = 1 ` as an environment variable. please note that using ` hf _ transfer ` comes with certain limitations. since it is not purely python - based"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_14_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "##f - transfer / ) and set ` hf _ hub _ enable _ hf _ transfer = 1 ` as an environment variable. please note that using ` hf _ transfer ` comes with certain limitations. since it is not purely python - based, debugging errors may be challenging. additionally, ` hf _ transfer ` lacks several user - friendly features such as resumable downloads and proxies. these omissions are intentional to maintain the simplicity and speed of the rust logic. consequently, ` hf _ transfer ` is not enabled by default in ` huggingface _ hub `. # # deprecated environment variables in order to standardize all environment variables within the hugging face ecosystem, some variables have been marked as deprecated. although they remain functional, they no longer take precedence over their replacements. the following table outlines the deprecated variables and their corresponding alternatives : | deprecated variable | replacement | | - - - | - - - | | `"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_15_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "##precated. although they remain functional, they no longer take precedence over their replacements. the following table outlines the deprecated variables and their corresponding alternatives : | deprecated variable | replacement | | - - - | - - - | | ` huggingface _ hub _ cache ` | ` hf _ hub _ cache ` | | ` huggingface _ assets _ cache ` | ` hf _ assets _ cache ` | | ` hugging _ face _ hub _ token ` | ` hf _ token ` | | ` huggingface _ hub _ verbosity ` | ` hf _ hub _ verbosity ` | # # from external tools some environment variables are not specific to ` huggingface _ hub ` but are still taken into account when they are set. # # # do _ not _ track boolean value. equivalent to ` hf _ hub _ disable _ telemetry `. when set to true, telemetry is globally disabled in the hugging face python ecosystem ("
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_16_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "they are set. # # # do _ not _ track boolean value. equivalent to ` hf _ hub _ disable _ telemetry `. when set to true, telemetry is globally disabled in the hugging face python ecosystem ( ` transformers `, ` diffusers `, ` gradio `, etc. ). see https : / / consoledonottrack. com / for more details. # # # no _ color boolean value. when set, ` huggingface - cli ` tool will not print any ansi color. see [ no - color. org ] ( https : / / no - color. org / ). # # # xdg _ cache _ home used only when ` hf _ home ` is not set! this is the default way to configure where [ user - specific non - essential ( cached ) data should be written ] ( https : / / wiki. archlinux. org /"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_17_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "hf _ home ` is not set! this is the default way to configure where [ user - specific non - essential ( cached ) data should be written ] ( https : / / wiki. archlinux. org / title / xdg _ base _ directory ) on linux machines. if ` hf _ home ` is not set, the default home will be ` \" $ xdg _ cache _ home / huggingface \" ` instead of ` \" ~ /. cache / huggingface \" `."
              },
              {
                "docTitle": "huggingface_tokenizers_blob_main_bindings_node_npm_linux-x64-musl_README.md.txt",
                "id": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "! - - copyright 2022 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. note that this file is in markdown but contain specific syntax for our doc - builder ( similar to mdx ) that may not be rendered properly in your markdown viewer. - - > # instantiating a big model when you want to use a very big pretrained model, one challenge is to minimize the use of the ram. the usual workflow"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": ") that may not be rendered properly in your markdown viewer. - - > # instantiating a big model when you want to use a very big pretrained model, one challenge is to minimize the use of the ram. the usual workflow from pytorch is : 1. create your model with random weights. 2. load your pretrained weights. 3. put those pretrained weights in your random model. step 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several gigabytes, those two copies can make you get out of ram. even worse, if you are using ` torch. distributed ` to launch a distributed training, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "training, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_4_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~ pretrainedmodel. save _ pretrained ` ], you will get a new folder with two files : the config of the model and its weights : ` ` ` py > > > import os > > > import tempfile > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir )... print ( sorted ( os. listdir ( tmp _ dir ) ) ) [ ' config. json ', ' pytorch _ model. bin ' ] ` ` ` now let ' s use a maximum shard size of 200mb : `"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_5_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "listdir ( tmp _ dir ) ) ) [ ' config. json ', ' pytorch _ model. bin ' ] ` ` ` now let ' s use a maximum shard size of 200mb : ` ` ` py > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... print ( sorted ( os. listdir ( tmp _ dir ) ) ) [ ' config. json ', ' pytorch _ model - 00001 - of - 00003. bin ', ' pytorch _ model - 00002 - of - 00003. bin ', ' pytorch _ model - 00003 - of - 00003. bin ', ' pytor"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_6_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": ". bin ', ' pytorch _ model - 00002 - of - 00003. bin ', ' pytorch _ model - 00003 - of - 00003. bin ', ' pytorch _ model. bin. index. json ' ] ` ` ` on top of the configuration of the model, we see three different weights files, and an ` index. json ` file which is our index. a checkpoint like this can be fully reloaded using the [ ` ~ pretrainedmodel. from _ pretrained ` ] method : ` ` ` py > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_7_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": ":... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir ) ` ` ` the main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in ram to the model size plus the size of the biggest shard. behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. we can load that index like any json and get a dictionary : ` ` ` py > > > import json > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ sha"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_8_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "` py > > > import json > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... with open ( os. path. join ( tmp _ dir, \" pytorch _ model. bin. index. json \" ), \" r \" ) as f :... index = json. load ( f ) > > > print ( index. keys ( ) ) dict _ keys ( [ ' metadata ', ' weight _ map ' ] ) ` ` ` the metadata just consists of the total size of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] { ' total _ size ' : 433245184 } ` ` ` the weights map"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_9_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "size of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] { ' total _ size ' : 433245184 } ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] { ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_10_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_11_HuggingFace_Tokenizers",
                "method": "Overlapping Chunking (HuggingFace Tokenizers)",
                "text": "... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )"
              }
            ],
            "LangChain TextSplitter": [
              {
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt",
                "id": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "Gradio Demo: blocks_multiple_event_triggers\r\n\r\n\r\n```\r\n!pip install -q gradio plotly pypistats\r\n```\r\n\r\n\r\n```\r\nimport gradio as gr\r\nimport pypistats\r\nfrom datetime import date"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt",
                "id": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "import pypistats\r\nfrom datetime import date\r\nfrom dateutil.relativedelta import relativedelta\r\nimport pandas as pd\r\n\r\ndef get_plot(lib, time):"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt",
                "id": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "import pandas as pd\r\n\r\ndef get_plot(lib, time):\r\n    data = pypistats.overall(lib, total=True, format=\"pandas\")\r\n    data = data.groupby(\"category\").get_group(\"with_mirrors\").sort_values(\"date\")"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt",
                "id": "Overlapping_Chunking_3_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "start_date = date.today() - relativedelta(months=int(time.split(\" \")[0]))\r\n    data = data[(data['date'] > str(start_date))]\r\n    data.date = pd.to_datetime(pd.to_datetime(data.date))"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt",
                "id": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "return gr.LinePlot(value=data, x=\"date\", y=\"downloads\",\r\n                              tooltip=['date', 'downloads'],"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt",
                "id": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "title=f\"Pypi downloads of {lib} over last {time}\",\r\n                              overlay_point=True,\r\n                              height=400,"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt",
                "id": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "height=400,\r\n                              width=900)\r\n\r\n\r\nwith gr.Blocks() as demo:\r\n    gr.Markdown(\r\n        \"\"\"\r\n        ## Pypi Download Stats ðŸ“ˆ"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt",
                "id": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "\"\"\"\r\n        ## Pypi Download Stats ðŸ“ˆ\r\n        See live download stats for all of Hugging Face's open-source libraries ðŸ¤—\r\n        \"\"\")\r\n    with gr.Row():"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt",
                "id": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "\"\"\")\r\n    with gr.Row():\r\n        lib = gr.Dropdown([\"transformers\", \"datasets\", \"huggingface-hub\", \"gradio\", \"accelerate\"],\r\n                          value=\"gradio\", label=\"Library\")"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt",
                "id": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "time = gr.Dropdown([\"3 months\", \"6 months\", \"9 months\", \"12 months\"],\r\n                           value=\"3 months\", label=\"Downloads over the last...\")\r\n\r\n    plt = gr.LinePlot()"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt",
                "id": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "plt = gr.LinePlot()\r\n    # You can add multiple event triggers in 2 lines like this\r\n    for event in [lib.change, time.change, demo.load]:\r\n        event(get_plot, [lib, time], [plt])"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_demo_blocks_multiple_event_triggers_run.ipynb.txt",
                "id": "Overlapping_Chunking_11_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "event(get_plot, [lib, time], [plt])\r\n\r\nif __name__ == \"__main__\":\r\n    demo.launch()\r\n\r\n```"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt",
                "id": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "Contributing a Guide\r\n\r\nWant to help teach Gradio? Consider contributing a Guide! ðŸ¤—\r\n\r\nBroadly speaking, there are two types of guides:"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt",
                "id": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- **Use cases**: guides that cover step-by-step how to build a particular type of machine learning demo or app using Gradio. Here's an example: [_Creating a"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt",
                "id": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "app using Gradio. Here's an example: [_Creating a Chatbot_](https://github.com/gradio-app/gradio/blob/master/guides/creating_a_chatbot.md)"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt",
                "id": "Overlapping_Chunking_3_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- **Feature explanation**: guides that describe in detail a particular feature of Gradio. Here's an example: [_Using"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt",
                "id": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "feature of Gradio. Here's an example: [_Using Flagging_](https://github.com/gradio-app/gradio/blob/master/guides/using_flagging.md)"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt",
                "id": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "We encourage you to submit either type of Guide! (Looking for ideas? We may also have open [issues](https://github.com/gradio-app/gradio/issues?q=is%3Aopen+is%3Aissue+label%3Aguides) where users have"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt",
                "id": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "where users have asked for guides on particular topics)"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt",
                "id": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## Guide Structure\r\n\r\nAs you can see with the previous examples, Guides are standard markdown documents. They usually:\r\n\r\n- start with an Introduction section describing the topic"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt",
                "id": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- include subheadings to make articles easy to navigate\r\n- include real code snippets that make it easy to follow along and implement the Guide"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt",
                "id": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- include embedded Gradio demos to make them more interactive and provide immediate demonstrations of the topic being discussed. These Gradio demos are hosted on [Hugging Face"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt",
                "id": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "These Gradio demos are hosted on [Hugging Face Spaces](https://huggingface.co/spaces) and are embedded using the standard \\<iframe\\> tag."
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt",
                "id": "Overlapping_Chunking_11_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## How to Contribute a Guide\r\n\r\n1. Clone or fork this `gradio` repo\r\n2. Add a new markdown document with a descriptive title to the `/guides` folder"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt",
                "id": "Overlapping_Chunking_12_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "3. Write your Guide in standard markdown! Embed Gradio demos wherever helpful"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt",
                "id": "Overlapping_Chunking_13_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "4. Add a list of `related_spaces` at the top of the markdown document (see the previously linked Guides for how to do this)"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt",
                "id": "Overlapping_Chunking_14_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "5. Add 3 `tags` at the top of the markdown document to help users find your guide (again, see the previously linked Guides for how to do this)\r\n6. Open a PR to have your guide reviewed"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_guides_CONTRIBUTING.md.txt",
                "id": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "6. Open a PR to have your guide reviewed\r\n\r\nThat's it! We're looking forward to reading your Guide ðŸ¥³"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "@gradio/gallery\r\n\r\n## 0.4.14\r\n\r\n### Patch Changes"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Updated dependencies [[`828fb9e`](https://github.com/gradio-app/gradio/commit/828fb9e6ce15b6ea08318675a2361117596a1b5d),"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "[`73268ee`](https://github.com/gradio-app/gradio/commit/73268ee2e39f23ebdd1e927cb49b8d79c4b9a144)]:"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_3_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- @gradio/client@0.9.3\r\n  - @gradio/statustracker@0.4.3\r\n  - @gradio/atoms@0.4.1\r\n  - @gradio/image@0.5.3\r\n  - @gradio/upload@0.5.6\r\n\r\n## 0.4.13\r\n\r\n### Patch Changes"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## 0.4.13\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`245d58e`](https://github.com/gradio-app/gradio/commit/245d58eff788e8d44a59d37a2d9b26d0f08a62b4)]:\r\n  - @gradio/client@0.9.2"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- @gradio/client@0.9.2\r\n  - @gradio/image@0.5.2\r\n  - @gradio/upload@0.5.5\r\n\r\n## 0.4.12\r\n\r\n### Patch Changes"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Updated dependencies [[`5d51fbc`](https://github.com/gradio-app/gradio/commit/5d51fbce7826da840a2fd4940feb5d9ad6f1bc5a),"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "[`34f9431`](https://github.com/gradio-app/gradio/commit/34f943101bf7dd6b8a8974a6131c1ed7c4a0dac0)]:"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- @gradio/upload@0.5.4\r\n  - @gradio/client@0.9.1\r\n  - @gradio/image@0.5.1\r\n\r\n## 0.4.11\r\n\r\n### Fixes"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#6487](https://github.com/gradio-app/gradio/pull/6487) [`9a5811d`](https://github.com/gradio-app/gradio/commit/9a5811df9218b622af59ba243a937a9c36ba00f9) - Fix the download button of the"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Fix the download button of the `gr.Gallery()` component to work. Thanks [@whitphx](https://github.com/whitphx)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_11_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## 0.4.10\r\n\r\n### Patch Changes"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_12_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Updated dependencies [[`b639e04`](https://github.com/gradio-app/gradio/commit/b639e040741e6c0d9104271c81415d7befbd8cf3),"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_13_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "[`206af31`](https://github.com/gradio-app/gradio/commit/206af31d7c1a31013364a44e9b40cf8df304ba50)]:"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_14_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- @gradio/image@0.4.2\r\n  - @gradio/icons@0.3.1\r\n  - @gradio/atoms@0.3.1\r\n  - @gradio/statustracker@0.4.1\r\n  - @gradio/upload@0.5.2\r\n\r\n## 0.4.9\r\n\r\n### Patch Changes"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## 0.4.9\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`71f1a1f99`](https://github.com/gradio-app/gradio/commit/71f1a1f9931489d465c2c1302a5c8d768a3cd23a)]:\r\n  - @gradio/client@0.8.2"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_16_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- @gradio/client@0.8.2\r\n  - @gradio/image@0.4.1\r\n  - @gradio/upload@0.5.1\r\n\r\n## 0.4.8\r\n\r\n### Patch Changes"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_17_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## 0.4.8\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`9caddc17b`](https://github.com/gradio-app/gradio/commit/9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8)]:\r\n  - @gradio/atoms@0.3.0"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_18_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- @gradio/atoms@0.3.0\r\n  - @gradio/icons@0.3.0\r\n  - @gradio/image@0.4.0\r\n  - @gradio/statustracker@0.4.0\r\n  - @gradio/upload@0.5.0\r\n\r\n## 0.4.7\r\n\r\n### Patch Changes"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_19_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Updated dependencies [[`2f805a7dd`](https://github.com/gradio-app/gradio/commit/2f805a7dd3d2b64b098f659dadd5d01258290521),"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_20_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "[`f816136a0`](https://github.com/gradio-app/gradio/commit/f816136a039fa6011be9c4fb14f573e4050a681a)]:"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_21_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- @gradio/image@0.3.6\r\n  - @gradio/upload@0.4.2\r\n  - @gradio/atoms@0.2.2\r\n  - @gradio/icons@0.2.1\r\n  - @gradio/statustracker@0.3.2\r\n\r\n## 0.4.6\r\n\r\n### Patch Changes"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_22_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## 0.4.6\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`324867f63`](https://github.com/gradio-app/gradio/commit/324867f63c920113d89a565892aa596cf8b1e486)]:\r\n  - @gradio/client@0.8.1"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_23_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- @gradio/client@0.8.1\r\n  - @gradio/image@0.3.5\r\n  - @gradio/upload@0.4.1\r\n\r\n## 0.4.5\r\n\r\n### Patch Changes"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_24_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Updated dependencies [[`6204ccac5`](https://github.com/gradio-app/gradio/commit/6204ccac5967763e0ebde550d04d12584243a120),"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_25_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "[`4d3aad33a`](https://github.com/gradio-app/gradio/commit/4d3aad33a0b66639dbbb2928f305a79fb7789b2d),"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_26_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "[`854b482f5`](https://github.com/gradio-app/gradio/commit/854b482f598e0dc47673846631643c079576da9c),"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_27_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "[`f1409f95e`](https://github.com/gradio-app/gradio/commit/f1409f95ed39c5565bed6a601e41f94e30196a57)]:"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_28_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- @gradio/image@0.3.4\r\n  - @gradio/upload@0.4.0\r\n  - @gradio/client@0.8.0\r\n\r\n## 0.4.4\r\n\r\n### Patch Changes"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_29_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Updated dependencies [[`bca6c2c80`](https://github.com/gradio-app/gradio/commit/bca6c2c80f7e5062427019de45c282238388af95),"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_30_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "[`3cdeabc68`](https://github.com/gradio-app/gradio/commit/3cdeabc6843000310e1a9e1d17190ecbf3bbc780),"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_31_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "[`fad92c29d`](https://github.com/gradio-app/gradio/commit/fad92c29dc1f5cd84341aae417c495b33e01245f)]:"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_32_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- @gradio/client@0.7.2\r\n  - @gradio/atoms@0.2.1\r\n  - @gradio/upload@0.3.3\r\n  - @gradio/image@0.3.3\r\n  - @gradio/statustracker@0.3.1\r\n\r\n## 0.4.3\r\n\r\n### Fixes"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_33_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#6288](https://github.com/gradio-app/gradio/pull/6288) [`92278729e`](https://github.com/gradio-app/gradio/commit/92278729ee008126af15ffe6be399236211b2f34) - Gallery preview fix and optionally skip"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_34_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Gallery preview fix and optionally skip download of urls in postprcess. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_35_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## 0.4.2\r\n\r\n### Fixes"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_36_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#6277](https://github.com/gradio-app/gradio/pull/6277) [`5fe091367`](https://github.com/gradio-app/gradio/commit/5fe091367fbe0eecdd504aa734ca1c70b0621f52) - handle selected_index prop change for"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_37_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- handle selected_index prop change for gallery. Thanks [@pngwn](https://github.com/pngwn)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_38_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## 0.4.1\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`2ba14b284`](https://github.com/gradio-app/gradio/commit/2ba14b284f908aa13859f4337167a157075a68eb)]:\r\n  - @gradio/client@0.7.1"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_39_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- @gradio/client@0.7.1\r\n  - @gradio/image@0.3.1\r\n  - @gradio/upload@0.3.1\r\n\r\n## 0.4.0\r\n\r\n### Features"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_40_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - fix circular dependency with client +"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_41_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- fix circular dependency with client + upload. Thanks [@pngwn](https://github.com/pngwn)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_42_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Clean root url. Thanks"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_43_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Clean root url. Thanks [@pngwn](https://github.com/pngwn)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_44_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Image v4. Thanks"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_45_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Image v4. Thanks [@pngwn](https://github.com/pngwn)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_46_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Publish all components to npm. Thanks"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_47_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Publish all components to npm. Thanks [@pngwn](https://github.com/pngwn)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_48_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Custom components. Thanks"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_49_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Custom components. Thanks [@pngwn](https://github.com/pngwn)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_50_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## 0.4.0-beta.9\r\n\r\n### Features"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_51_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#6143](https://github.com/gradio-app/gradio/pull/6143) [`e4f7b4b40`](https://github.com/gradio-app/gradio/commit/e4f7b4b409323b01aa01b39e15ce6139e29aa073) - fix circular dependency with client +"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_52_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- fix circular dependency with client + upload. Thanks [@pngwn](https://github.com/pngwn)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_53_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#6136](https://github.com/gradio-app/gradio/pull/6136) [`667802a6c`](https://github.com/gradio-app/gradio/commit/667802a6cdbfb2ce454a3be5a78e0990b194548a) - JS Component Documentation. Thanks"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_54_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- JS Component Documentation. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_55_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#6094](https://github.com/gradio-app/gradio/pull/6094) [`c476bd5a5`](https://github.com/gradio-app/gradio/commit/c476bd5a5b70836163b9c69bf4bfe068b17fbe13) - Image v4. Thanks"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_56_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Image v4. Thanks [@pngwn](https://github.com/pngwn)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_57_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## 0.4.0-beta.8\r\n\r\n### Features"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_58_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676`](https://github.com/gradio-app/gradio/commit/83e947676d327ca2ab6ae2a2d710c78961c771a0) - Format js in v4 branch. Thanks"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_59_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Format js in v4 branch. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_60_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "### Fixes"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_61_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_62_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- fix tests. Thanks [@pngwn](https://github.com/pngwn)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_63_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## 0.4.0-beta.7\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies []:\r\n  - @gradio/image@0.3.0-beta.7\r\n\r\n## 0.4.0-beta.6\r\n\r\n### Features"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_64_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f`](https://github.com/gradio-app/gradio/commit/319c30f3fccf23bfe1da6c9b132a6a99d59652f7) - rererefactor frontend files. Thanks"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_65_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- rererefactor frontend files. Thanks [@pngwn](https://github.com/pngwn)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_66_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#5938](https://github.com/gradio-app/gradio/pull/5938) [`13ed8a485`](https://github.com/gradio-app/gradio/commit/13ed8a485d5e31d7d75af87fe8654b661edcca93) - V4: Use beta release versions for"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_67_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- V4: Use beta release versions for '@gradio' packages. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_68_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## 0.5.1\r\n\r\n### Patch Changes"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_69_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Updated dependencies [[`b67115e8e`](https://github.com/gradio-app/gradio/commit/b67115e8e6e489fffd5271ea830211863241ddc5),"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_70_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "[`8d909624f`](https://github.com/gradio-app/gradio/commit/8d909624f61a49536e3c0f71cb2d9efe91216219),"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_71_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "[`e70805d54`](https://github.com/gradio-app/gradio/commit/e70805d54cc792452545f5d8eccc1aa0212a4695)]:"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_72_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- @gradio/image@0.4.0\r\n  - @gradio/atoms@0.2.0\r\n  - @gradio/statustracker@0.2.3\r\n  - @gradio/upload@0.3.3\r\n\r\n## 0.5.0\r\n\r\n### Features"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_73_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#5780](https://github.com/gradio-app/gradio/pull/5780) [`ed0f9a21b`](https://github.com/gradio-app/gradio/commit/ed0f9a21b04ad6b941b63d2ce45100dbd1abd5c5) - Adds `change()` event to `gr.Gallery`."
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_74_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Adds `change()` event to `gr.Gallery`. Thanks [@abidlabs](https://github.com/abidlabs)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_75_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#5783](https://github.com/gradio-app/gradio/pull/5783) [`4567788bd`](https://github.com/gradio-app/gradio/commit/4567788bd1fc25df9322902ba748012e392b520a) - Adds the ability to set the"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_76_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Adds the ability to set the `selected_index` in a `gr.Gallery`. Thanks [@abidlabs](https://github.com/abidlabs)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_77_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "### Fixes"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_78_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#5798](https://github.com/gradio-app/gradio/pull/5798) [`a0d3cc45c`](https://github.com/gradio-app/gradio/commit/a0d3cc45c6db48dc0db423c229b8fb285623cdc4) - Fix `gr.SelectData` so that the target"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_79_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Fix `gr.SelectData` so that the target attribute is correctly attached, and the filedata is included in the data attribute with `gr.Gallery`. Thanks [@abidlabs](https://github.com/abidlabs)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_80_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## 0.4.1\r\n\r\n### Fixes"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_81_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#5735](https://github.com/gradio-app/gradio/pull/5735) [`abb5e9df4`](https://github.com/gradio-app/gradio/commit/abb5e9df47989b2c56c2c312d74944678f9f2d4e) - Ensure images with no caption download"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_82_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Ensure images with no caption download in gallery. Thanks [@hannahblair](https://github.com/hannahblair)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_83_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#5754](https://github.com/gradio-app/gradio/pull/5754) [`502054848`](https://github.com/gradio-app/gradio/commit/502054848fdbe39fc03ec42445242b4e49b7affc) - Fix Gallery `columns` and `rows`"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_84_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Fix Gallery `columns` and `rows` params. Thanks [@abidlabs](https://github.com/abidlabs)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_85_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## 0.4.0\r\n\r\n### Features"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_86_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#5554](https://github.com/gradio-app/gradio/pull/5554) [`75ddeb390`](https://github.com/gradio-app/gradio/commit/75ddeb390d665d4484667390a97442081b49a423) - Accessibility Improvements. Thanks"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_87_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Accessibility Improvements. Thanks [@hannahblair](https://github.com/hannahblair)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_88_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## 0.3.4\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`e0d61b8ba`](https://github.com/gradio-app/gradio/commit/e0d61b8baa0f6293f53b9bdb1647d42f9ae2583a)]:\r\n  - @gradio/image@0.2.4\r\n\r\n## 0.3.3"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_89_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- @gradio/image@0.2.4\r\n\r\n## 0.3.3\r\n\r\n### Fixes"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_90_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#5528](https://github.com/gradio-app/gradio/pull/5528) [`dc86e4a7`](https://github.com/gradio-app/gradio/commit/dc86e4a7e1c40b910c74558e6f88fddf9b3292bc) - Lazy load all images. Thanks"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_91_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Lazy load all images. Thanks [@aliabid94](https://github.com/aliabid94)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_92_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## 0.3.2\r\n\r\n### Patch Changes\r\n\r\n- Updated dependencies [[`afac0006`](https://github.com/gradio-app/gradio/commit/afac0006337ce2840cf497cd65691f2f60ee5912)]:\r\n  - @gradio/statustracker@0.2.0"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_93_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- @gradio/statustracker@0.2.0\r\n  - @gradio/image@0.2.2\r\n  - @gradio/utils@0.1.1\r\n  - @gradio/atoms@0.1.2\r\n  - @gradio/upload@0.2.1\r\n\r\n## 0.3.1\r\n\r\n### Patch Changes"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_94_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Updated dependencies [[`abf1c57d`](https://github.com/gradio-app/gradio/commit/abf1c57d7d85de0df233ee3b38aeb38b638477db),"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_95_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "[`79d8f9d8`](https://github.com/gradio-app/gradio/commit/79d8f9d891901683c5a1b7486efb44eab2478c96)]:"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_96_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- @gradio/icons@0.1.0\r\n  - @gradio/utils@0.1.0\r\n  - @gradio/upload@0.2.0\r\n  - @gradio/atoms@0.1.1\r\n  - @gradio/image@0.2.1\r\n  - @gradio/statustracker@0.1.1\r\n\r\n## 0.3.0\r\n\r\n### Highlights"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_97_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "#### Improve startup performance and markdown support ([#5279](https://github.com/gradio-app/gradio/pull/5279)"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_98_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "[`fe057300`](https://github.com/gradio-app/gradio/commit/fe057300f0672c62dab9d9b4501054ac5d45a4ec))"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_99_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "##### Improved markdown support"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_100_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "We now have better support for markdown in `gr.Markdown` and `gr.Dataframe`. Including syntax highlighting and Github Flavoured Markdown. We also have more consistent markdown behaviour and styling."
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_101_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "##### Various performance improvements\r\n\r\nThese improvements will be particularly beneficial to large applications."
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_102_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_103_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "introduced in a recent version of Gradio. App startup for large applications is now around twice as fast."
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_104_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).\r\n- Corrected an issue that was causing markdown to re-render infinitely."
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_105_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Ensured that the `gr.3DModel` does re-render prematurely.\r\n\r\nThanks [@pngwn](https://github.com/pngwn)!\r\n\r\n### Features"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_106_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#5215](https://github.com/gradio-app/gradio/pull/5215) [`fbdad78a`](https://github.com/gradio-app/gradio/commit/fbdad78af4c47454cbb570f88cc14bf4479bbceb) - Lazy load interactive or static variants"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_107_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Lazy load interactive or static variants of a component individually, rather than loading both variants regardless. This change will improve performance for many applications. Thanks"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_108_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "improve performance for many applications. Thanks [@pngwn](https://github.com/pngwn)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_109_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#5216](https://github.com/gradio-app/gradio/pull/5216) [`4b58ea6d`](https://github.com/gradio-app/gradio/commit/4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8) - Update i18n tokens and locale files."
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_110_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Update i18n tokens and locale files. Thanks [@hannahblair](https://github.com/hannahblair)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_111_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## 0.2.0\r\n\r\n### Features"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_112_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#5025](https://github.com/gradio-app/gradio/pull/5025) [`6693660a`](https://github.com/gradio-app/gradio/commit/6693660a790996f8f481feaf22a8c49130d52d89) - Add download button to selected images"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_113_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Add download button to selected images in `Gallery`. Thanks [@hannahblair](https://github.com/hannahblair)!"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_114_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## 0.1.0\r\n\r\n### Features"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_115_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [#4995](https://github.com/gradio-app/gradio/pull/4995) [`3f8c210b`](https://github.com/gradio-app/gradio/commit/3f8c210b01ef1ceaaf8ee73be4bf246b5b745bbf) - Implement left and right click in"
              },
              {
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "id": "Overlapping_Chunking_116_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- Implement left and right click in `Gallery` component and show implicit images in `Gallery` grid. Thanks [@hannahblair](https://github.com/hannahblair)!"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_3_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage,"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_11_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_12_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated."
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_13_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys."
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_14_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_16_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_17_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_18_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_19_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_20_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_21_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_22_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_23_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_24_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_25_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_26_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_27_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "(index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed"
              },
              {
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "id": "Overlapping_Chunking_28_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs."
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "id": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "id": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "the License. You may obtain a copy of the License at\r\n\r\nhttp://www.apache.org/licenses/LICENSE-2.0"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "id": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "http://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "id": "Overlapping_Chunking_3_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\r\nspecific language governing permissions and limitations under the License.\r\n-->"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "id": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "id": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "2. use multiple pipelines in your workflow"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "id": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "Based on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method."
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "id": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "To seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one."
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "id": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "```py\r\nfrom diffusers import AutoPipelineForText2Image\r\nimport torch\r\n\r\npipeline = AutoPipelineForText2Image.from_pretrained("
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "id": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\r\n).to(\"cuda\")\r\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\""
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "id": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "image = pipeline(prompt, num_inference_steps=25).images[0]\r\n```\r\n\r\n<Tip>\r\n\r\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\r\n\r\n</Tip>"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "id": "Overlapping_Chunking_11_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "</Tip>\r\n\r\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\r\n\r\n- [Stable Diffusion](./stable_diffusion/overview)"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "id": "Overlapping_Chunking_12_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [ControlNet](./controlnet)\r\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\r\n- [DeepFloyd IF](./deepfloyd_if)\r\n- [Kandinsky 2.1](./kandinsky)"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "id": "Overlapping_Chunking_13_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [Kandinsky 2.1](./kandinsky)\r\n- [Kandinsky 2.2](./kandinsky_v22)\r\n\r\n\r\n## AutoPipelineForText2Image\r\n\r\n[[autodoc]] AutoPipelineForText2Image\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "id": "Overlapping_Chunking_14_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- all\r\n\t- from_pretrained\r\n\t- from_pipe\r\n\r\n## AutoPipelineForImage2Image\r\n\r\n[[autodoc]] AutoPipelineForImage2Image\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe\r\n\r\n## AutoPipelineForInpainting"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "id": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- from_pipe\r\n\r\n## AutoPipelineForInpainting\r\n\r\n[[autodoc]] AutoPipelineForInpainting\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "the License. You may obtain a copy of the License at\r\n\r\nhttp://www.apache.org/licenses/LICENSE-2.0"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "http://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_3_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\r\nspecific language governing permissions and limitations under the License.\r\n-->"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "-->\r\n\r\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\r\n\r\n## Preamble"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## Preamble\r\n\r\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training."
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "Given its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library."
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "The risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups."
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "We will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\r\n\r\n\r\n## Scope"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_11_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "The Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_12_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "the contributions, especially concerning sensitive topics related to ethical concerns."
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_13_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## Ethical guidelines"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_14_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "The following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question."
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_16_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions."
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_17_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent."
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_18_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent."
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_19_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_20_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "makes research artifacts more accessible to the community."
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_21_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library."
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_22_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers."
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_23_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## Examples of implementations: Safety features and Mechanisms"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_24_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "The team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_25_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us."
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_26_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project."
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_27_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_28_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations."
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_29_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- **Encouraging safety in deployment**"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_30_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_31_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion:"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_32_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105)."
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_33_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_34_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_35_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "The harmful concepts are intentionally hidden to prevent reverse engineering of the checker."
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_36_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_37_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "an intermediary step that allows the repositoryâ€™s authors to have more control over its use."
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "id": "Overlapping_Chunking_38_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use."
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_src_diffusers_experimental_README.md.txt",
                "id": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "ðŸ§¨ Diffusers Experimental\r\n\r\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\r\nCurrently, the following experiments are supported:"
              },
              {
                "docTitle": "huggingface_diffusers_blob_main_src_diffusers_experimental_README.md.txt",
                "id": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "* Reinforcement learning via an implementation of the [Diffuser](https://arxiv.org/abs/2205.09991) model."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\r\nrendered properly in your Markdown viewer.\r\n-->\r\n\r\n# Environment variables"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "-->\r\n\r\n# Environment variables\r\n\r\n`huggingface_hub` can be configured using environment variables.\r\n\r\nIf you are unfamiliar with environment variable, here are generic articles about them"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "[on macOS and Linux](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/)\r\nand on [Windows](https://phoenixnap.com/kb/windows-set-environment-variable)."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_3_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "This page will guide you through all environment variables specific to `huggingface_hub`\r\nand their meaning.\r\n\r\n## Generic\r\n\r\n### HF_INFERENCE_ENDPOINT"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## Generic\r\n\r\n### HF_INFERENCE_ENDPOINT\r\n\r\nTo configure the inference api base url. You might want to set this variable if your organization"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "is pointing at an API Gateway rather than directly at the inference api.\r\n\r\nDefaults to `\"https://api-inference.huggingface.com\"`.\r\n\r\n### HF_HOME"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "### HF_HOME\r\n\r\nTo configure where `huggingface_hub` will locally store data. In particular, your token\r\nand the cache will be stored in this folder."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "and the cache will be stored in this folder.\r\n\r\nDefaults to `\"~/.cache/huggingface\"` unless [XDG_CACHE_HOME](#xdgcachehome) is set.\r\n\r\n### HF_HUB_CACHE"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "### HF_HUB_CACHE\r\n\r\nTo configure where repositories from the Hub will be cached locally (models, datasets and\r\nspaces)."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "spaces).\r\n\r\nDefaults to `\"$HF_HOME/hub\"` (e.g. `\"~/.cache/huggingface/hub\"` by default).\r\n\r\n### HF_ASSETS_CACHE"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "### HF_ASSETS_CACHE\r\n\r\nTo configure where [assets](../guides/manage-cache#caching-assets) created by downstream libraries"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_11_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "will be cached locally. Those assets can be preprocessed data, files downloaded from GitHub,\r\nlogs,...\r\n\r\nDefaults to `\"$HF_HOME/assets\"` (e.g. `\"~/.cache/huggingface/assets\"` by default)."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_12_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "### HF_TOKEN\r\n\r\nTo configure the User Access Token to authenticate to the Hub. If set, this value will\r\noverwrite the token stored on the machine (in `\"$HF_HOME/token\"`)."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_13_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "For more details about authentication, check out [this section](../quick-start#authentication).\r\n\r\n### HF_HUB_VERBOSITY\r\n\r\nSet the verbosity level of the `huggingface_hub`'s logger. Must be one of"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_14_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "`{\"debug\", \"info\", \"warning\", \"error\", \"critical\"}`.\r\n\r\nDefaults to `\"warning\"`."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "Defaults to `\"warning\"`.\r\n\r\nFor more details, see [logging reference](../package_reference/utilities#huggingface_hub.utils.logging.get_verbosity).\r\n\r\n### HF_HUB_LOCAL_DIR_AUTO_SYMLINK_THRESHOLD"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_16_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "### HF_HUB_LOCAL_DIR_AUTO_SYMLINK_THRESHOLD\r\n\r\nInteger value to define under which size a file is considered as \"small\". When downloading files to a local directory,"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_17_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "small files will be duplicated to ease user experience while bigger files are symlinked to save disk usage."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_18_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "For more details, see the [download guide](../guides/download#download-files-to-local-folder).\r\n\r\n### HF_HUB_ETAG_TIMEOUT"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_19_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "Integer value to define the number of seconds to wait for server response when fetching the latest metadata from a repo before downloading a file. If the request times out, `huggingface_hub` will"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_20_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "If the request times out, `huggingface_hub` will default to the locally cached files. Setting a lower value speeds up the workflow for machines with a slow connection that have already cached files."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_21_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "a slow connection that have already cached files. A higher value guarantees the metadata call to succeed in more cases. Default to 10s."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_22_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "### HF_HUB_DOWNLOAD_TIMEOUT"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_23_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "Integer value to define the number of seconds to wait for server response when downloading a file. If the request times out, a TimeoutError is raised. Setting a higher value is beneficial on machine"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_24_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "Setting a higher value is beneficial on machine with a slow connection. A smaller value makes the process fail quicker in case of complete network outage. Default to 10s."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_25_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## Boolean values\r\n\r\nThe following environment variables expect a boolean value. The variable will be considered"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_26_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "as `True` if its value is one of `{\"1\", \"ON\", \"YES\", \"TRUE\"}` (case-insensitive). Any other value\r\n(or undefined) will be considered as `False`.\r\n\r\n### HF_HUB_OFFLINE"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_27_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "If set, no HTTP calls will me made to the Hugging Face Hub. If you try to download files, only the cached files will be accessed. If no cache file is detected, an error is raised This is useful in"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_28_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "is detected, an error is raised This is useful in case your network is slow and you don't care about having the latest version of a file."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_29_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "If `HF_HUB_OFFLINE=1` is set as environment variable and you call any method of [`HfApi`], an [`~huggingface_hub.utils.OfflineModeIsEnabled`] exception will be raised."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_30_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "**Note:** even if the latest version of a file is cached, calling `hf_hub_download` still triggers a HTTP request to check that a new version is not available. Setting `HF_HUB_OFFLINE=1` will skip"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_31_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "available. Setting `HF_HUB_OFFLINE=1` will skip this call which speeds up your loading time."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_32_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "### HF_HUB_DISABLE_IMPLICIT_TOKEN\r\n\r\nAuthentication is not mandatory for every requests to the Hub. For instance, requesting"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_33_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "details about `\"gpt2\"` model does not require to be authenticated. However, if a user is\r\n[logged in](../package_reference/login), the default behavior will be to always send the token"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_34_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "in order to ease user experience (never get a HTTP 401 Unauthorized) when accessing private or gated repositories. For privacy, you can"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_35_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "disable this behavior by setting `HF_HUB_DISABLE_IMPLICIT_TOKEN=1`. In this case,\r\nthe token will be sent only for \"write-access\" calls (example: create a commit)."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_36_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "**Note:** disabling implicit sending of token can have weird side effects. For example,\r\nif you want to list all models on the Hub, your private models will not be listed. You"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_37_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "would need to explicitly pass `token=True` argument in your script.\r\n\r\n### HF_HUB_DISABLE_PROGRESS_BARS"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_38_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "### HF_HUB_DISABLE_PROGRESS_BARS\r\n\r\nFor time consuming tasks, `huggingface_hub` displays a progress bar by default (using tqdm)."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_39_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "You can disable all the progress bars at once by setting `HF_HUB_DISABLE_PROGRESS_BARS=1`.\r\n\r\n### HF_HUB_DISABLE_SYMLINKS_WARNING"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_40_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "### HF_HUB_DISABLE_SYMLINKS_WARNING\r\n\r\nIf you are on a Windows machine, it is recommended to enable the developer mode or to run"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_41_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "`huggingface_hub` in admin mode. If not, `huggingface_hub` will not be able to create\r\nsymlinks in your cache system. You will be able to execute any script but your user experience"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_42_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "will be degraded as some huge files might end-up duplicated on your hard-drive. A warning\r\nmessage is triggered to warn you about this behavior. Set `HF_HUB_DISABLE_SYMLINKS_WARNING=1`,"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_43_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "to disable this warning.\r\n\r\nFor more details, see [cache limitations](../guides/manage-cache#limitations).\r\n\r\n### HF_HUB_DISABLE_EXPERIMENTAL_WARNING"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_44_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "### HF_HUB_DISABLE_EXPERIMENTAL_WARNING\r\n\r\nSome features of `huggingface_hub` are experimental. This means you can use them but we do not guarantee they will be"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_45_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "maintained in the future. In particular, we might update the API or behavior of such features without any deprecation"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_46_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "cycle. A warning message is triggered when using an experimental feature to warn you about it. If you're comfortable debugging any potential issues using an experimental feature, you can set"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_47_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "issues using an experimental feature, you can set `HF_HUB_DISABLE_EXPERIMENTAL_WARNING=1` to disable the warning."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_48_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "If you are using an experimental feature, please let us know! Your feedback can help us design and improve it.\r\n\r\n### HF_HUB_DISABLE_TELEMETRY"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_49_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "### HF_HUB_DISABLE_TELEMETRY\r\n\r\nBy default, some data is collected by HF libraries (`transformers`, `datasets`, `gradio`,..) to monitor usage, debug issues and help prioritize features."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_50_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "Each library defines its own policy (i.e. which usage to monitor) but the core implementation happens in `huggingface_hub` (see [`send_telemetry`])."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_51_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "You can set `HF_HUB_DISABLE_TELEMETRY=1` as environment variable to globally disable telemetry.\r\n\r\n### HF_HUB_ENABLE_HF_TRANSFER"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_52_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "### HF_HUB_ENABLE_HF_TRANSFER\r\n\r\nSet to `True` for faster uploads and downloads from the Hub using `hf_transfer`."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_53_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "By default, `huggingface_hub` uses the Python-based `requests.get` and `requests.post` functions. Although these are reliable and versatile, they may not be the most efficient choice for machines"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_54_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "may not be the most efficient choice for machines with high bandwidth. [`hf_transfer`](https://github.com/huggingface/hf_transfer) is a Rust-based package developed to maximize the bandwidth used by"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_55_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "developed to maximize the bandwidth used by dividing large files into smaller parts and transferring them simultaneously using multiple threads. This approach can potentially double the transfer"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_56_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "This approach can potentially double the transfer speed. To use `hf_transfer`, you need to install it separately [from PyPI](https://pypi.org/project/hf-transfer/) and set"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_57_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "and set `HF_HUB_ENABLE_HF_TRANSFER=1` as an environment variable."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_58_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "Please note that using `hf_transfer` comes with certain limitations. Since it is not purely Python-based, debugging errors may be challenging. Additionally, `hf_transfer` lacks several user-friendly"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_59_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "`hf_transfer` lacks several user-friendly features such as resumable downloads and proxies. These omissions are intentional to maintain the simplicity and speed of the Rust logic. Consequently,"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_60_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "and speed of the Rust logic. Consequently, `hf_transfer` is not enabled by default in `huggingface_hub`."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_61_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## Deprecated environment variables"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_62_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "In order to standardize all environment variables within the Hugging Face ecosystem, some variables have been marked as deprecated. Although they remain functional, they no longer take precedence"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_63_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "remain functional, they no longer take precedence over their replacements. The following table outlines the deprecated variables and their corresponding alternatives:"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_64_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "| Deprecated Variable | Replacement |\r\n| --- | --- |\r\n| `HUGGINGFACE_HUB_CACHE` | `HF_HUB_CACHE` |\r\n| `HUGGINGFACE_ASSETS_CACHE` | `HF_ASSETS_CACHE` |\r\n| `HUGGING_FACE_HUB_TOKEN` | `HF_TOKEN` |"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_65_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "| `HUGGING_FACE_HUB_TOKEN` | `HF_TOKEN` |\r\n| `HUGGINGFACE_HUB_VERBOSITY` | `HF_HUB_VERBOSITY` |\r\n\r\n## From external tools"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_66_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## From external tools\r\n\r\nSome environment variables are not specific to `huggingface_hub` but are still taken into account when they are set.\r\n\r\n### DO_NOT_TRACK"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_67_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "Boolean value. Equivalent to `HF_HUB_DISABLE_TELEMETRY`. When set to true, telemetry is globally disabled in the Hugging Face Python ecosystem (`transformers`, `diffusers`, `gradio`, etc.). See"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_68_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "`diffusers`, `gradio`, etc.). See https://consoledonottrack.com/ for more details."
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_69_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "### NO_COLOR\r\n\r\nBoolean value. When set, `huggingface-cli` tool will not print any ANSI color.\r\nSee [no-color.org](https://no-color.org/).\r\n\r\n### XDG_CACHE_HOME"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_70_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "### XDG_CACHE_HOME\r\n\r\nUsed only when `HF_HOME` is not set!"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_71_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "Used only when `HF_HOME` is not set!\r\n\r\nThis is the default way to configure where [user-specific non-essential (cached) data should be written](https://wiki.archlinux.org/title/XDG_Base_Directory)"
              },
              {
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "id": "Overlapping_Chunking_72_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "on linux machines. \r\n\r\nIf `HF_HOME` is not set, the default home will be `\"$XDG_CACHE_HOME/huggingface\"` instead\r\nof `\"~/.cache/huggingface\"`."
              },
              {
                "docTitle": "huggingface_tokenizers_blob_main_bindings_node_npm_linux-x64-musl_README.md.txt",
                "id": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "`tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "the License. You may obtain a copy of the License at\r\n\r\nhttp://www.apache.org/licenses/LICENSE-2.0"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "http://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_3_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\r\nspecific language governing permissions and limitations under the License."
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\r\nrendered properly in your Markdown viewer.\r\n\r\n-->"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "-->\r\n\r\n# Instantiating a big model\r\n\r\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\r\nfrom PyTorch is:"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "from PyTorch is:\r\n\r\n1. Create your model with random weights.\r\n2. Load your pretrained weights.\r\n3. Put those pretrained weights in your random model."
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "Step 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "the pretrained model and store these two copies in RAM."
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "<Tip>"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_11_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "Note that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_12_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_13_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible!"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_14_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "</Tip>"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "In this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future."
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_16_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "## Sharded checkpoints"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_17_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_18_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_19_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "being of size < 10GB) and an index that maps parameter names to the files they are stored in."
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_20_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "You can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_21_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "with a small shard size: let's take a traditional BERT model."
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_22_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "```py\r\nfrom transformers import AutoModel\r\n\r\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\r\n```"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_23_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "```\r\n\r\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\r\n\r\n```py\r\n>>> import os\r\n>>> import tempfile"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_24_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "```py\r\n>>> import os\r\n>>> import tempfile\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir)\r\n...     print(sorted(os.listdir(tmp_dir)))"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_25_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "...     print(sorted(os.listdir(tmp_dir)))\r\n['config.json', 'pytorch_model.bin']\r\n```\r\n\r\nNow let's use a maximum shard size of 200MB:\r\n\r\n```py\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_26_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\r\n...     print(sorted(os.listdir(tmp_dir)))"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_27_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\r\n```"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_28_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "On top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_29_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_30_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "```py\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\r\n...     new_model = AutoModel.from_pretrained(tmp_dir)\r\n```"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_31_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "The main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_32_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard."
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_33_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "Behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_34_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "```py\r\n>>> import json\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_35_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\r\n...         index = json.load(f)\r\n\r\n>>> print(index.keys())\r\ndict_keys(['metadata', 'weight_map'])\r\n```"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_36_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "dict_keys(['metadata', 'weight_map'])\r\n```\r\n\r\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\r\n\r\n```py\r\n>>> index[\"metadata\"]"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_37_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "```py\r\n>>> index[\"metadata\"]\r\n{'total_size': 433245184}\r\n```"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_38_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "{'total_size': 433245184}\r\n```\r\n\r\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_39_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "```py\r\n>>> index[\"weight_map\"]\r\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\r\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\r\n ...\r\n```"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_40_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "If you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_41_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_42_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "```py\r\n>>> from transformers.modeling_utils import load_sharded_checkpoint\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_43_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "...     load_sharded_checkpoint(model, tmp_dir)\r\n```\r\n\r\n## Low memory loading"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_44_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "Sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the"
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_45_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "we recommend leveraging our tools based on the Accelerate library."
              },
              {
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "id": "Overlapping_Chunking_46_LangChain_TextSplitter",
                "method": "Overlapping Chunking (LangChain TextSplitter)",
                "text": "Please read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)"
              }
            ]
          }
        },
        "dragging": false,
        "id": "chunking-9a8eac2d-45cc-460f-ad25-a59404dfd84b",
        "position": {
          "x": -1552,
          "y": 592
        },
        "positionAbsolute": {
          "x": -1552,
          "y": 592
        },
        "selected": false,
        "type": "chunking"
      },
      {
        "width": 422,
        "height": 348,
        "data": {
          "methods": [
            {
              "baseMethod": "bm25",
              "emoji": "ðŸ“Š",
              "key": "37590bb9-8313-4bdd-97f0-bb147727726c",
              "library": "BM25",
              "methodName": "BM25",
              "needsEmbeddingModel": false,
              "settings": {
                "bm25_b": 0.75,
                "bm25_k1": 1.5,
                "shortName": "BM25 Retrieval",
                "top_k": 5
              }
            },
            {
              "baseMethod": "cosine",
              "embeddingProvider": "huggingface",
              "emoji": "ðŸ“",
              "key": "d5820512-daae-46fe-b47c-7d82218a4402",
              "library": "Cosine",
              "methodName": "Cosine Similarity",
              "needsEmbeddingModel": true,
              "settings": {
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "shortName": "Cosine Similarity",
                "similarity_threshold": 50,
                "top_k": 5
              }
            }
          ],
          "output": [
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
                "retrievalMethod": "BM25",
                "similarity": 1
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "method": "BM25",
                "rank": 1,
                "reference": "distilbert base uncased finetuned sst2 english",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
              },
              "prompt": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
              "text": "the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two",
              "uid": "chunk-1745101037027-0.24319266122137795"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.7350158139998298
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "method": "BM25",
                "rank": 2,
                "reference": "distilbert base uncased finetuned sst2 english",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
              },
              "prompt": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
              "text": "method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment",
              "uid": "chunk-1745101037027-0.8747156345450886"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.5951491656058835
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "method": "BM25",
                "rank": 3,
                "reference": "distilbert base uncased finetuned sst2 english",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
              },
              "prompt": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
              "text": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the",
              "uid": "chunk-1745101037027-0.7147926498734352"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.5726053115464012
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "method": "BM25",
                "rank": 4,
                "reference": "distilbert base uncased finetuned sst2 english",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
              },
              "prompt": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
              "text": "library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in",
              "uid": "chunk-1745101037027-0.6774960478681935"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_49_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.45290112958932455
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_49_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "method": "BM25",
                "rank": 5,
                "reference": "distilbert base uncased finetuned sst2 english",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
              },
              "prompt": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
              "text": "### HF_HUB_DISABLE_TELEMETRY\r\n\r\nBy default, some data is collected by HF libraries (`transformers`, `datasets`, `gradio`,..) to monitor usage, debug issues and help prioritize features.",
              "uid": "chunk-1745101037027-0.049698721249604394"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_17_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
                "retrievalMethod": "BM25",
                "similarity": 1
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_17_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "method": "BM25",
                "rank": 1,
                "reference": "10GB",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
              },
              "prompt": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
              "text": "Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do",
              "uid": "chunk-1745101037027-0.8841116743207903"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_20_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.7472743058375536
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_20_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "method": "BM25",
                "rank": 2,
                "reference": "10GB",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
              },
              "prompt": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
              "text": "You can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional",
              "uid": "chunk-1745101037027-0.2261256980982742"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.5976757730541392
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "method": "BM25",
                "rank": 3,
                "reference": "10GB",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
              },
              "prompt": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
              "text": "attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the",
              "uid": "chunk-1745101037027-0.07089016287464334"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_40_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.5962104977847036
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_40_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "method": "BM25",
                "rank": 4,
                "reference": "10GB",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
              },
              "prompt": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
              "text": "If you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should",
              "uid": "chunk-1745101037027-0.702305983775225"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_21_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.5842192396287725
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_21_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "method": "BM25",
                "rank": 5,
                "reference": "10GB",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
              },
              "prompt": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
              "text": "class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after",
              "uid": "chunk-1745101037027-0.40675375322254514"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_tokenizers_blob_main_bindings_node_npm_linux-x64-musl_README.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
                "retrievalMethod": "BM25",
                "similarity": 1
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_tokenizers_blob_main_bindings_node_npm_linux-x64-musl_README.md.txt",
                "method": "BM25",
                "rank": 1,
                "reference": "x86_64-unknown-linux-musl",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
              },
              "prompt": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
              "text": "`tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`",
              "uid": "chunk-1745101037027-0.2741598625514886"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.24528991366064404
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "method": "BM25",
                "rank": 2,
                "reference": "x86_64-unknown-linux-musl",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
              },
              "prompt": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
              "text": "-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use",
              "uid": "chunk-1745101037027-0.6058344492225048"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_72_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.17693120093722234
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_72_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "method": "BM25",
                "rank": 3,
                "reference": "x86_64-unknown-linux-musl",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
              },
              "prompt": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
              "text": "on linux machines. \r\n\r\nIf `HF_HOME` is not set, the default home will be `\"$XDG_CACHE_HOME/huggingface\"` instead\r\nof `\"~/.cache/huggingface\"`.",
              "uid": "chunk-1745101037027-0.9247257940043776"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.15944280291758267
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_2_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "method": "BM25",
                "rank": 4,
                "reference": "x86_64-unknown-linux-musl",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
              },
              "prompt": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
              "text": "[on macOS and Linux](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/)\r\nand on [Windows](https://phoenixnap.com/kb/windows-set-environment-variable).",
              "uid": "chunk-1745101037027-0.9967702070740497"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.15803251597565512
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "method": "BM25",
                "rank": 5,
                "reference": "x86_64-unknown-linux-musl",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
              },
              "prompt": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
              "text": "attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the",
              "uid": "chunk-1745101037027-0.430548964852253"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_33_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
                "retrievalMethod": "BM25",
                "similarity": 1
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_33_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "method": "BM25",
                "rank": 1,
                "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
              },
              "prompt": "What is the purpose of the Safety Checker in the Diffusers library?\n",
              "text": "- [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of",
              "uid": "chunk-1745101037027-0.8585591664866706"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.6661125155695281
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "method": "BM25",
                "rank": 2,
                "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
              },
              "prompt": "What is the purpose of the Safety Checker in the Diffusers library?\n",
              "text": "guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.",
              "uid": "chunk-1745101037027-0.5545426867977654"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_src_diffusers_experimental_README.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.6184192754172843
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_src_diffusers_experimental_README.md.txt",
                "method": "BM25",
                "rank": 3,
                "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
              },
              "prompt": "What is the purpose of the Safety Checker in the Diffusers library?\n",
              "text": "ðŸ§¨ Diffusers Experimental\r\n\r\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\r\nCurrently, the following experiments are supported:",
              "uid": "chunk-1745101037027-0.10059753804832827"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.6108131022360351
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "method": "BM25",
                "rank": 4,
                "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
              },
              "prompt": "What is the purpose of the Safety Checker in the Diffusers library?\n",
              "text": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the",
              "uid": "chunk-1745101037027-0.3716951362173645"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_29_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.6058459609776176
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_29_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "method": "BM25",
                "rank": 5,
                "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
              },
              "prompt": "What is the purpose of the Safety Checker in the Diffusers library?\n",
              "text": "- **Encouraging safety in deployment**",
              "uid": "chunk-1745101037027-0.2595366260929267"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
                "retrievalMethod": "BM25",
                "similarity": 1
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "method": "BM25",
                "rank": 1,
                "reference": "from_pretrained()",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
              },
              "prompt": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
              "text": "-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use",
              "uid": "chunk-1745101037027-0.3919197884447657"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.6513366338355057
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "method": "BM25",
                "rank": 2,
                "reference": "from_pretrained()",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
              },
              "prompt": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
              "text": "method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment",
              "uid": "chunk-1745101037027-0.6741126229582256"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.5659950144359884
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "method": "BM25",
                "rank": 3,
                "reference": "from_pretrained()",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
              },
              "prompt": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
              "text": "Based on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.",
              "uid": "chunk-1745101037027-0.24955689494696465"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_40_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.5585238356251861
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_40_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "method": "BM25",
                "rank": 4,
                "reference": "from_pretrained()",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
              },
              "prompt": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
              "text": "If you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should",
              "uid": "chunk-1745101037027-0.6396622884511854"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.47777459551591334
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "method": "BM25",
                "rank": 5,
                "reference": "from_pretrained()",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
              },
              "prompt": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
              "text": "the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two",
              "uid": "chunk-1745101037027-0.6608045348115174"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.6831600403739134
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 1,
                "reference": "distilbert base uncased finetuned sst2 english",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
              },
              "prompt": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
              "text": "the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two",
              "uid": "chunk-1745101037027-0.5651105168535961"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.5769317825783699
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 2,
                "reference": "distilbert base uncased finetuned sst2 english",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
              },
              "prompt": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
              "text": "method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment",
              "uid": "chunk-1745101037027-0.16274369515468545"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.5709325505415817
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_1_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 3,
                "reference": "distilbert base uncased finetuned sst2 english",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
              },
              "prompt": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
              "text": "library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in",
              "uid": "chunk-1745101037027-0.3244923251099172"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.4545738188092565
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 4,
                "reference": "distilbert base uncased finetuned sst2 english",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
              },
              "prompt": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
              "text": "steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage,",
              "uid": "chunk-1745101037027-0.5125008791419766"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.44298949648543284
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 5,
                "reference": "distilbert base uncased finetuned sst2 english",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
              },
              "prompt": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
              "text": "its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,",
              "uid": "chunk-1745101037027-0.5847337569936756"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_17_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.7434103034223855
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_17_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 1,
                "reference": "10GB",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
              },
              "prompt": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
              "text": "Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do",
              "uid": "chunk-1745101037027-0.7179322993765663"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_42_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.6371099430308824
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_42_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 2,
                "reference": "10GB",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
              },
              "prompt": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
              "text": "```py\r\n>>> from transformers.modeling_utils import load_sharded_checkpoint\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")",
              "uid": "chunk-1745101037027-0.022967833840742302"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_41_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.5838608662363409
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_41_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 3,
                "reference": "10GB",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
              },
              "prompt": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
              "text": "for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:",
              "uid": "chunk-1745101037027-0.9588324163869585"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_31_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.5488224773534289
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_31_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 4,
                "reference": "10GB",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
              },
              "prompt": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
              "text": "The main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to",
              "uid": "chunk-1745101037027-0.9281611577838657"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_44_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.5467476625338024
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_44_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 5,
                "reference": "10GB",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
              },
              "prompt": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
              "text": "Sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the",
              "uid": "chunk-1745101037027-0.8038873472950265"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_tokenizers_blob_main_bindings_node_npm_linux-x64-musl_README.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.8547497095705013
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_tokenizers_blob_main_bindings_node_npm_linux-x64-musl_README.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 1,
                "reference": "x86_64-unknown-linux-musl",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
              },
              "prompt": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
              "text": "`tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`",
              "uid": "chunk-1745101037027-0.573829138051344"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.41609025733590926
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_5_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 2,
                "reference": "x86_64-unknown-linux-musl",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
              },
              "prompt": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
              "text": "library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or",
              "uid": "chunk-1745101037027-0.9027275704810339"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.3794411601543982
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 3,
                "reference": "x86_64-unknown-linux-musl",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
              },
              "prompt": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
              "text": "tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning",
              "uid": "chunk-1745101037027-0.25870126972905083"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.3659420707152563
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_8_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 4,
                "reference": "x86_64-unknown-linux-musl",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
              },
              "prompt": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
              "text": "its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,",
              "uid": "chunk-1745101037027-0.4235758599697377"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_16_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.3581700472676653
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_16_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 5,
                "reference": "x86_64-unknown-linux-musl",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
              },
              "prompt": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
              "text": "Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well",
              "uid": "chunk-1745101037027-0.8958080760329636"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_33_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.6822047125924968
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_33_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 1,
                "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
              },
              "prompt": "What is the purpose of the Safety Checker in the Diffusers library?\n",
              "text": "- [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of",
              "uid": "chunk-1745101037027-0.5492935206896059"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.6241342958952832
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 2,
                "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
              },
              "prompt": "What is the purpose of the Safety Checker in the Diffusers library?\n",
              "text": "guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.",
              "uid": "chunk-1745101037027-0.6164963190088506"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.537806532717705
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 3,
                "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
              },
              "prompt": "What is the purpose of the Safety Checker in the Diffusers library?\n",
              "text": "-->\r\n\r\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\r\n\r\n## Preamble",
              "uid": "chunk-1745101037027-0.7547493951778588"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_30_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.5349502519099344
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_30_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 4,
                "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
              },
              "prompt": "What is the purpose of the Safety Checker in the Diffusers library?\n",
              "text": "- [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable",
              "uid": "chunk-1745101037027-0.21700518364465082"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_18_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.5292048853104937
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_18_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 5,
                "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
              },
              "prompt": "What is the purpose of the Safety Checker in the Diffusers library?\n",
              "text": "- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.",
              "uid": "chunk-1745101037027-0.33914764244580475"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.8924103182846942
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 1,
                "reference": "from_pretrained()",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
              },
              "prompt": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
              "text": "-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use",
              "uid": "chunk-1745101037027-0.7178644941195707"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.6289645010444016
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 2,
                "reference": "from_pretrained()",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
              },
              "prompt": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
              "text": "To seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.",
              "uid": "chunk-1745101037027-0.19267985982254676"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.5911360903575882
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 3,
                "reference": "from_pretrained()",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
              },
              "prompt": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
              "text": "Based on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.",
              "uid": "chunk-1745101037027-0.7416295089309218"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_41_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.57771081118823
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_41_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 4,
                "reference": "from_pretrained()",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
              },
              "prompt": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
              "text": "for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:",
              "uid": "chunk-1745101037027-0.4671193338535764"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.5401940039579186
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_15_LangChain_TextSplitter",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 5,
                "reference": "from_pretrained()",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
              },
              "prompt": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
              "text": "- from_pipe\r\n\r\n## AutoPipelineForInpainting\r\n\r\n[[autodoc]] AutoPipelineForInpainting\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe",
              "uid": "chunk-1745101037027-0.47464677123271215"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
                "retrievalMethod": "BM25",
                "similarity": 1
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "method": "BM25",
                "rank": 1,
                "reference": "distilbert base uncased finetuned sst2 english",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
              },
              "prompt": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
              "text": "the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.",
              "uid": "chunk-1745101037027-0.7344454164045322"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.9188763387823734
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "method": "BM25",
                "rank": 2,
                "reference": "distilbert base uncased finetuned sst2 english",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
              },
              "prompt": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
              "text": "hat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special",
              "uid": "chunk-1745101037027-0.023284286470651394"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.8992622219466135
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "method": "BM25",
                "rank": 3,
                "reference": "distilbert base uncased finetuned sst2 english",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
              },
              "prompt": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
              "text": "to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all",
              "uid": "chunk-1745101037027-0.5469815982356213"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_12_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.5544303323930434
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_12_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "method": "BM25",
                "rank": 4,
                "reference": "distilbert base uncased finetuned sst2 english",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
              },
              "prompt": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
              "text": "##try by default, some data is collected by hf libraries ( ` transformers `, ` datasets `, ` gradio `,.. ) to monitor usage, debug issues and help prioritize features. each library defines its own policy ( i. e. which usage to monitor ) but the core implementation happens in ` huggingface _ hub ` ( see [ ` send _ telemetry ` ] ). you can set ` hf _ hub _ disable _ telemetry = 1 ` as environment variable to globally disable telemetry. # # # hf _ hub _ enable _ hf _ transfer set to ` true ` for faster uploads and downloads from the hub using ` hf _ transfer `. by default, ` huggingface _ hub ` uses the python - based ` requests. get ` and ` requests. post ` functions. although these are reliable and versatile, they may not be the most efficient choice for machines with high",
              "uid": "chunk-1745101037027-0.4325208548887288"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_5_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.5530994255816127
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_5_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "method": "BM25",
                "rank": 5,
                "reference": "distilbert base uncased finetuned sst2 english",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
              },
              "prompt": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
              "text": ". those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs.",
              "uid": "chunk-1745101037027-0.010315284182348172"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
                "retrievalMethod": "BM25",
                "similarity": 1
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "method": "BM25",
                "rank": 1,
                "reference": "10GB",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
              },
              "prompt": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
              "text": "slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~",
              "uid": "chunk-1745101037027-0.7804199065213121"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_10_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.66828798185768
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_10_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "method": "BM25",
                "rank": 2,
                "reference": "10GB",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
              },
              "prompt": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
              "text": "weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #",
              "uid": "chunk-1745101037027-0.44220610910558"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.6610077781734277
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "method": "BM25",
                "rank": 3,
                "reference": "10GB",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
              },
              "prompt": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
              "text": "training, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when",
              "uid": "chunk-1745101037027-0.03618070471300516"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_9_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.5841885996679934
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_9_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "method": "BM25",
                "rank": 4,
                "reference": "10GB",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
              },
              "prompt": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
              "text": "size of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] { ' total _ size ' : 433245184 } ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] { ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre",
              "uid": "chunk-1745101037027-0.19288662499173437"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_11_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.5344367052818971
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_11_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "method": "BM25",
                "rank": 5,
                "reference": "10GB",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
              },
              "prompt": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
              "text": "... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )",
              "uid": "chunk-1745101037027-0.7694185957564488"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_tokenizers_blob_main_bindings_node_npm_linux-x64-musl_README.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
                "retrievalMethod": "BM25",
                "similarity": 1
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_tokenizers_blob_main_bindings_node_npm_linux-x64-musl_README.md.txt",
                "method": "BM25",
                "rank": 1,
                "reference": "x86_64-unknown-linux-musl",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
              },
              "prompt": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
              "text": "` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `",
              "uid": "chunk-1745101037027-0.4422151468113541"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_17_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.19156269890361466
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_17_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "method": "BM25",
                "rank": 2,
                "reference": "x86_64-unknown-linux-musl",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
              },
              "prompt": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
              "text": "hf _ home ` is not set! this is the default way to configure where [ user - specific non - essential ( cached ) data should be written ] ( https : / / wiki. archlinux. org / title / xdg _ base _ directory ) on linux machines. if ` hf _ home ` is not set, the default home will be ` \" $ xdg _ cache _ home / huggingface \" ` instead of ` \" ~ /. cache / huggingface \" `.",
              "uid": "chunk-1745101037027-0.5130596415149596"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.18932874094696112
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "method": "BM25",
                "rank": 3,
                "reference": "x86_64-unknown-linux-musl",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
              },
              "prompt": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
              "text": "! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the",
              "uid": "chunk-1745101037027-0.05026626528096423"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.180354595952368
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "method": "BM25",
                "rank": 4,
                "reference": "x86_64-unknown-linux-musl",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
              },
              "prompt": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
              "text": ". input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size",
              "uid": "chunk-1745101037027-0.3541915217171264"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.17041164908194634
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                "method": "BM25",
                "rank": 5,
                "reference": "x86_64-unknown-linux-musl",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
              },
              "prompt": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
              "text": "! - - note that this file is in markdown but contain specific syntax for our doc - builder ( similar to mdx ) that may not be rendered properly in your markdown viewer. - - > # environment variables ` huggingface _ hub ` can be configured using environment variables. if you are unfamiliar with environment variable, here are generic articles about them [ on macos and linux ] ( https : / / linuxize. com / post / how - to - set - and - list - environment - variables - in - linux / ) and on [ windows ] ( https : / / phoenixnap. com / kb / windows - set - environment - variable ). this page will guide you through all environment variables specific to ` huggingface _ hub ` and their meaning. # # generic # # # hf _ inference _ endpoint to configure the inference api base url. you might want to set this variable if your organization is pointing at an api gateway rather",
              "uid": "chunk-1745101037027-0.4882391889393686"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_6_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
                "retrievalMethod": "BM25",
                "similarity": 1
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_6_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "method": "BM25",
                "rank": 1,
                "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
              },
              "prompt": "What is the purpose of the Safety Checker in the Diffusers library?\n",
              "text": "diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https",
              "uid": "chunk-1745101037027-0.8298188383894795"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_5_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.7373422920371591
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_5_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "method": "BM25",
                "rank": 2,
                "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
              },
              "prompt": "What is the purpose of the Safety Checker in the Diffusers library?\n",
              "text": "https : / / huggingface. co / spaces / society - ethics / diffusionbiasexplorer ) to demonstrate the biases in stable diffusion interactively. in this sense, we support and encourage bias explorers and evaluations. - * * encouraging safety in deployment * * - [ * * safe stable diffusion * * ] ( https : / / huggingface. co / docs / diffusers / main / en / api / pipelines / stable _ diffusion / stable _ diffusion _ safe ) : it mitigates the well - known issue that models, like stable diffusion, that are trained on unfiltered, web - crawled datasets tend to suffer from inappropriate degeneration. related paper : [ safe latent diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / gi",
              "uid": "chunk-1745101037027-0.9743727826900498"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.7239617766707714
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "method": "BM25",
                "rank": 3,
                "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
              },
              "prompt": "What is the purpose of the Safety Checker in the Diffusers library?\n",
              "text": "same level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and",
              "uid": "chunk-1745101037027-0.8403001995802697"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_src_diffusers_experimental_README.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.6490789204980913
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_src_diffusers_experimental_README.md.txt",
                "method": "BM25",
                "rank": 4,
                "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
              },
              "prompt": "What is the purpose of the Safety Checker in the Diffusers library?\n",
              "text": "diffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model.",
              "uid": "chunk-1745101037027-0.7106817492799378"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.5946338281681647
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "method": "BM25",
                "rank": 5,
                "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
              },
              "prompt": "What is the purpose of the Safety Checker in the Diffusers library?\n",
              "text": "hat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special",
              "uid": "chunk-1745101037027-0.1714302641635116"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
                "retrievalMethod": "BM25",
                "similarity": 1
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "method": "BM25",
                "rank": 1,
                "reference": "from_pretrained()",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
              },
              "prompt": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
              "text": "to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,",
              "uid": "chunk-1745101037027-0.44486402717157214"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.8297674943395532
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "method": "BM25",
                "rank": 2,
                "reference": "from_pretrained()",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
              },
              "prompt": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
              "text": "! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the",
              "uid": "chunk-1745101037027-0.7530579095619842"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_10_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.6199296081369338
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_10_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "method": "BM25",
                "rank": 3,
                "reference": "from_pretrained()",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
              },
              "prompt": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
              "text": "weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #",
              "uid": "chunk-1745101037027-0.0364801550684265"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.6176867870890784
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "method": "BM25",
                "rank": 4,
                "reference": "from_pretrained()",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
              },
              "prompt": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
              "text": "the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.",
              "uid": "chunk-1745101037027-0.612813393351801"
            },
            {
              "fill_history": {
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_7_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
                "retrievalMethod": "BM25",
                "similarity": 0.5102220585089914
              },
              "llm": "BM25",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
                "baseMethod": "bm25",
                "chunkId": "Overlapping_Chunking_7_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "method": "BM25",
                "rank": 5,
                "reference": "from_pretrained()",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
              },
              "prompt": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
              "text": ":... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir ) ` ` ` the main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in ram to the model size plus the size of the biggest shard. behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. we can load that index like any json and get a dictionary : ` ` ` py > > > import json > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ sha",
              "uid": "chunk-1745101037027-0.22190555067404094"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.6290633377094689
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 1,
                "reference": "distilbert base uncased finetuned sst2 english",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
              },
              "prompt": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
              "text": "the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.",
              "uid": "chunk-1745101037027-0.567934895538027"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.6224960005660833
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 2,
                "reference": "distilbert base uncased finetuned sst2 english",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
              },
              "prompt": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
              "text": "hat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special",
              "uid": "chunk-1745101037027-0.4879107276447059"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.5249786960920674
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 3,
                "reference": "distilbert base uncased finetuned sst2 english",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
              },
              "prompt": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
              "text": "to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all",
              "uid": "chunk-1745101037027-0.20468865050786655"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_4_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.443655420837465
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_4_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 4,
                "reference": "distilbert base uncased finetuned sst2 english",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
              },
              "prompt": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
              "text": "removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. to get an output linked to our classification problem, we need to use the automodelforsequenceclassification class. it works exactly as the automodel class, except that it will build a model with a classification head. there is one auto class for each common nlp task in the transformers library. here, after giving our model the two sentences, we get a tensor of size two by two : one result for each sentence and for each possible label. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third",
              "uid": "chunk-1745101037027-0.05142823125706897"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_5_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.43377065396823317
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_5_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 5,
                "reference": "distilbert base uncased finetuned sst2 english",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
              },
              "prompt": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
              "text": ". those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs.",
              "uid": "chunk-1745101037027-0.11156763404814163"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.6644535229332378
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 1,
                "reference": "10GB",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
              },
              "prompt": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
              "text": "slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~",
              "uid": "chunk-1745101037027-0.0976191802625267"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.5762710337333394
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 2,
                "reference": "10GB",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
              },
              "prompt": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
              "text": "training, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when",
              "uid": "chunk-1745101037027-0.24069919899253078"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_10_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.5649069845406874
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_10_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 3,
                "reference": "10GB",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
              },
              "prompt": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
              "text": "weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #",
              "uid": "chunk-1745101037027-0.8517278430101891"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_11_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.5229975076585714
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_11_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 4,
                "reference": "10GB",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
              },
              "prompt": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
              "text": "... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )",
              "uid": "chunk-1745101037027-0.15219697420742717"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_9_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.5089080952694254
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_9_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 5,
                "reference": "10GB",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
              },
              "prompt": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
              "text": "size of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] { ' total _ size ' : 433245184 } ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] { ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre",
              "uid": "chunk-1745101037027-0.944360143150988"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_tokenizers_blob_main_bindings_node_npm_linux-x64-musl_README.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.8547497095705013
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_tokenizers_blob_main_bindings_node_npm_linux-x64-musl_README.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 1,
                "reference": "x86_64-unknown-linux-musl",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
              },
              "prompt": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
              "text": "` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `",
              "uid": "chunk-1745101037027-0.7204101604207898"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_42_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.3032923746403597
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_42_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 2,
                "reference": "x86_64-unknown-linux-musl",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
              },
              "prompt": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
              "text": "both variants regardless. this change will improve performance for many applications. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5216 ] ( https : / / github. com / gradio - app / gradio / pull / 5216 ) [ ` 4b58ea6d ` ] ( https : / / github. com / gradio - app / gradio / commit / 4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8 ) - update i18n tokens and locale files. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )! # # 0. 2. 0 # # # features - [ # 5025 ] ( https : / / github. com / gradio -",
              "uid": "chunk-1745101037027-0.4353888215716384"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.30240200919460275
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 3,
                "reference": "x86_64-unknown-linux-musl",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
              },
              "prompt": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
              "text": "the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.",
              "uid": "chunk-1745101037027-0.8228681114242068"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.2936881277350962
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 4,
                "reference": "x86_64-unknown-linux-musl",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
              },
              "prompt": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
              "text": ". input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size",
              "uid": "chunk-1745101037027-0.9197222397786803"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.2609388468689024
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 5,
                "reference": "x86_64-unknown-linux-musl",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
              },
              "prompt": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
              "text": "hat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special",
              "uid": "chunk-1745101037027-0.9376159654094912"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_6_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.581820766755503
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_6_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 1,
                "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
              },
              "prompt": "What is the purpose of the Safety Checker in the Diffusers library?\n",
              "text": "diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https",
              "uid": "chunk-1745101037027-0.23769928872322732"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.5104310219657868
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 2,
                "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
              },
              "prompt": "What is the purpose of the Safety Checker in the Diffusers library?\n",
              "text": "same level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and",
              "uid": "chunk-1745101037027-0.7670711318512934"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.43447342774771164
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 3,
                "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
              },
              "prompt": "What is the purpose of the Safety Checker in the Diffusers library?\n",
              "text": "' s responsiveness and valuable feedback. # # scope the diffusers community will apply the following ethical guidelines to the project â€™ s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns. # # ethical guidelines the following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question. - * * transparency * * : we are committed to being transparent in managing prs, explaining our choices to users, and making technical decisions. - * * consistency * * : we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and",
              "uid": "chunk-1745101037027-0.7382880773058434"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.42773127555562257
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 4,
                "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
              },
              "prompt": "What is the purpose of the Safety Checker in the Diffusers library?\n",
              "text": "! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # diffusers â€™ ethical guidelines # # preamble [ diffusers ] ( https : / / huggingface. co / docs / diffusers / index ) provides pre - trained diffusion models and serves as a modular toolbox for inference and training. given its real case applications in the world and potential negative impacts on society, we think it",
              "uid": "chunk-1745101037027-0.5957251646904769"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_src_diffusers_experimental_README.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.4124241026051033
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_src_diffusers_experimental_README.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 5,
                "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
              },
              "prompt": "What is the purpose of the Safety Checker in the Diffusers library?\n",
              "text": "diffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model.",
              "uid": "chunk-1745101037027-0.5732341665978478"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.6898500953693341
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 1,
                "reference": "from_pretrained()",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
              },
              "prompt": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
              "text": "! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the",
              "uid": "chunk-1745101037027-0.08695734873773853"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.6449463947529198
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 2,
                "reference": "from_pretrained()",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
              },
              "prompt": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
              "text": "to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,",
              "uid": "chunk-1745101037027-0.47876453326468615"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_4_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.48137210232026134
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_4_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 3,
                "reference": "from_pretrained()",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
              },
              "prompt": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
              "text": "autodoc ] ] autopipelineforimage2image - all - from _ pretrained - from _ pipe # # autopipelineforinpainting [ [ autodoc ] ] autopipelineforinpainting - all - from _ pretrained - from _ pipe",
              "uid": "chunk-1745101037027-0.5586769449725782"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_10_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.46018818477181794
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_10_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 4,
                "reference": "from_pretrained()",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
              },
              "prompt": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
              "text": "weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #",
              "uid": "chunk-1745101037027-0.6899205767765888"
            },
            {
              "fill_history": {
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_6_HuggingFace_Tokenizers",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
                "retrievalMethod": "Cosine Similarity",
                "similarity": 0.45353325729601984
              },
              "llm": "Cosine Similarity",
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
                "baseMethod": "cosine",
                "chunkId": "Overlapping_Chunking_6_HuggingFace_Tokenizers",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "rank": 5,
                "reference": "from_pretrained()",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
              },
              "prompt": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
              "text": ". bin ', ' pytorch _ model - 00002 - of - 00003. bin ', ' pytorch _ model - 00003 - of - 00003. bin ', ' pytorch _ model. bin. index. json ' ] ` ` ` on top of the configuration of the model, we see three different weights files, and an ` index. json ` file which is our index. a checkpoint like this can be fully reloaded using the [ ` ~ pretrainedmodel. from _ pretrained ` ] method : ` ` ` py > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir",
              "uid": "chunk-1745101037027-0.844372380384864"
            }
          ],
          "results": {
            "37590bb9-8313-4bdd-97f0-bb147727726c": {
              "metavars": {
                "baseMethod": "bm25",
                "method": "BM25"
              },
              "retrieved": {
                "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n": [
                  {
                    "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                    "docTitle": "huggingface_tokenizers_blob_main_bindings_node_npm_linux-x64-musl_README.md.txt",
                    "text": "`tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                    "text": "-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_72_LangChain_TextSplitter",
                    "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                    "text": "on linux machines. \r\n\r\nIf `HF_HOME` is not set, the default home will be `\"$XDG_CACHE_HOME/huggingface\"` instead\r\nof `\"~/.cache/huggingface\"`."
                  },
                  {
                    "chunkId": "Overlapping_Chunking_2_LangChain_TextSplitter",
                    "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                    "text": "[on macOS and Linux](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/)\r\nand on [Windows](https://phoenixnap.com/kb/windows-set-environment-variable)."
                  },
                  {
                    "chunkId": "Overlapping_Chunking_15_LangChain_TextSplitter",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_tokenizers_blob_main_bindings_node_npm_linux-x64-musl_README.md.txt",
                    "text": "` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_17_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                    "text": "hf _ home ` is not set! this is the default way to configure where [ user - specific non - essential ( cached ) data should be written ] ( https : / / wiki. archlinux. org / title / xdg _ base _ directory ) on linux machines. if ` hf _ home ` is not set, the default home will be ` \" $ xdg _ cache _ home / huggingface \" ` instead of ` \" ~ /. cache / huggingface \" `."
                  },
                  {
                    "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                    "text": "! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": ". input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                    "text": "! - - note that this file is in markdown but contain specific syntax for our doc - builder ( similar to mdx ) that may not be rendered properly in your markdown viewer. - - > # environment variables ` huggingface _ hub ` can be configured using environment variables. if you are unfamiliar with environment variable, here are generic articles about them [ on macos and linux ] ( https : / / linuxize. com / post / how - to - set - and - list - environment - variables - in - linux / ) and on [ windows ] ( https : / / phoenixnap. com / kb / windows - set - environment - variable ). this page will guide you through all environment variables specific to ` huggingface _ hub ` and their meaning. # # generic # # # hf _ inference _ endpoint to configure the inference api base url. you might want to set this variable if your organization is pointing at an api gateway rather"
                  }
                ],
                "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n": [
                  {
                    "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_1_LangChain_TextSplitter",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_49_LangChain_TextSplitter",
                    "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                    "text": "### HF_HUB_DISABLE_TELEMETRY\r\n\r\nBy default, some data is collected by HF libraries (`transformers`, `datasets`, `gradio`,..) to monitor usage, debug issues and help prioritize features."
                  },
                  {
                    "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences."
                  },
                  {
                    "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "hat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_12_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_huggingface_hub_blob_main_docs_source_en_package_reference_environment_variables.md.txt",
                    "text": "##try by default, some data is collected by hf libraries ( ` transformers `, ` datasets `, ` gradio `,.. ) to monitor usage, debug issues and help prioritize features. each library defines its own policy ( i. e. which usage to monitor ) but the core implementation happens in ` huggingface _ hub ` ( see [ ` send _ telemetry ` ] ). you can set ` hf _ hub _ disable _ telemetry = 1 ` as environment variable to globally disable telemetry. # # # hf _ hub _ enable _ hf _ transfer set to ` true ` for faster uploads and downloads from the hub using ` hf _ transfer `. by default, ` huggingface _ hub ` uses the python - based ` requests. get ` and ` requests. post ` functions. although these are reliable and versatile, they may not be the most efficient choice for machines with high"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_5_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": ". those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs."
                  }
                ],
                "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n": [
                  {
                    "chunkId": "Overlapping_Chunking_17_LangChain_TextSplitter",
                    "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                    "text": "Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_20_LangChain_TextSplitter",
                    "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                    "text": "You can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_15_LangChain_TextSplitter",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_40_LangChain_TextSplitter",
                    "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                    "text": "If you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_21_LangChain_TextSplitter",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                    "text": "slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_10_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                    "text": "weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                    "text": "training, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_9_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                    "text": "size of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] { ' total _ size ' : 433245184 } ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] { ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_11_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                    "text": "... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )"
                  }
                ],
                "What is the purpose of the Safety Checker in the Diffusers library?\n": [
                  {
                    "chunkId": "Overlapping_Chunking_33_LangChain_TextSplitter",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                    "text": "- [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                    "text": "guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library."
                  },
                  {
                    "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                    "docTitle": "huggingface_diffusers_blob_main_src_diffusers_experimental_README.md.txt",
                    "text": "ðŸ§¨ Diffusers Experimental\r\n\r\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\r\nCurrently, the following experiments are supported:"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_29_LangChain_TextSplitter",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                    "text": "- **Encouraging safety in deployment**"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_6_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                    "text": "diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_5_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                    "text": "https : / / huggingface. co / spaces / society - ethics / diffusionbiasexplorer ) to demonstrate the biases in stable diffusion interactively. in this sense, we support and encourage bias explorers and evaluations. - * * encouraging safety in deployment * * - [ * * safe stable diffusion * * ] ( https : / / huggingface. co / docs / diffusers / main / en / api / pipelines / stable _ diffusion / stable _ diffusion _ safe ) : it mitigates the well - known issue that models, like stable diffusion, that are trained on unfiltered, web - crawled datasets tend to suffer from inappropriate degeneration. related paper : [ safe latent diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / gi"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                    "text": "same level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_diffusers_blob_main_src_diffusers_experimental_README.md.txt",
                    "text": "diffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model."
                  },
                  {
                    "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "hat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special"
                  }
                ],
                "What method is used to load a checkpoint for a task using `AutoPipeline`?\n": [
                  {
                    "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                    "text": "-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                    "text": "Based on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method."
                  },
                  {
                    "chunkId": "Overlapping_Chunking_40_LangChain_TextSplitter",
                    "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                    "text": "If you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                    "text": "to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                    "text": "! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_10_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                    "text": "weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences."
                  },
                  {
                    "chunkId": "Overlapping_Chunking_7_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                    "text": ":... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir ) ` ` ` the main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in ram to the model size plus the size of the biggest shard. behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. we can load that index like any json and get a dictionary : ` ` ` py > > > import json > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ sha"
                  }
                ]
              }
            },
            "d5820512-daae-46fe-b47c-7d82218a4402": {
              "metavars": {
                "baseMethod": "cosine",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity"
              },
              "retrieved": {
                "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n": [
                  {
                    "chunkId": "Overlapping_Chunking_0_LangChain_TextSplitter",
                    "docTitle": "huggingface_tokenizers_blob_main_bindings_node_npm_linux-x64-musl_README.md.txt",
                    "text": "`tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_5_LangChain_TextSplitter",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_8_LangChain_TextSplitter",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_16_LangChain_TextSplitter",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_tokenizers_blob_main_bindings_node_npm_linux-x64-musl_README.md.txt",
                    "text": "` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_42_HuggingFace_Tokenizers",
                    "docTitle": "gradio-app_gradio_blob_main_js_gallery_CHANGELOG.md.txt",
                    "text": "both variants regardless. this change will improve performance for many applications. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5216 ] ( https : / / github. com / gradio - app / gradio / pull / 5216 ) [ ` 4b58ea6d ` ] ( https : / / github. com / gradio - app / gradio / commit / 4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8 ) - update i18n tokens and locale files. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )! # # 0. 2. 0 # # # features - [ # 5025 ] ( https : / / github. com / gradio -"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences."
                  },
                  {
                    "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": ". input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "hat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special"
                  }
                ],
                "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n": [
                  {
                    "chunkId": "Overlapping_Chunking_10_LangChain_TextSplitter",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_9_LangChain_TextSplitter",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_1_LangChain_TextSplitter",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage,"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_8_LangChain_TextSplitter",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences."
                  },
                  {
                    "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "hat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_4_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": "removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. to get an output linked to our classification problem, we need to use the automodelforsequenceclassification class. it works exactly as the automodel class, except that it will build a model with a classification head. there is one auto class for each common nlp task in the transformers library. here, after giving our model the two sentences, we get a tensor of size two by two : one result for each sentence and for each possible label. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_5_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                    "text": ". those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs."
                  }
                ],
                "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n": [
                  {
                    "chunkId": "Overlapping_Chunking_17_LangChain_TextSplitter",
                    "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                    "text": "Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_42_LangChain_TextSplitter",
                    "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                    "text": "```py\r\n>>> from transformers.modeling_utils import load_sharded_checkpoint\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_41_LangChain_TextSplitter",
                    "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                    "text": "for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_31_LangChain_TextSplitter",
                    "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                    "text": "The main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_44_LangChain_TextSplitter",
                    "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                    "text": "Sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                    "text": "slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                    "text": "training, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_10_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                    "text": "weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_11_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                    "text": "... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_9_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                    "text": "size of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] { ' total _ size ' : 433245184 } ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] { ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre"
                  }
                ],
                "What is the purpose of the Safety Checker in the Diffusers library?\n": [
                  {
                    "chunkId": "Overlapping_Chunking_33_LangChain_TextSplitter",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                    "text": "- [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                    "text": "guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library."
                  },
                  {
                    "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                    "text": "-->\r\n\r\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\r\n\r\n## Preamble"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_30_LangChain_TextSplitter",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                    "text": "- [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_18_LangChain_TextSplitter",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                    "text": "- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent."
                  },
                  {
                    "chunkId": "Overlapping_Chunking_6_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                    "text": "diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_3_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                    "text": "same level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_2_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                    "text": "' s responsiveness and valuable feedback. # # scope the diffusers community will apply the following ethical guidelines to the project â€™ s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns. # # ethical guidelines the following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question. - * * transparency * * : we are committed to being transparent in managing prs, explaining our choices to users, and making technical decisions. - * * consistency * * : we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                    "text": "! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # diffusers â€™ ethical guidelines # # preamble [ diffusers ] ( https : / / huggingface. co / docs / diffusers / index ) provides pre - trained diffusion models and serves as a modular toolbox for inference and training. given its real case applications in the world and potential negative impacts on society, we think it"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_diffusers_blob_main_src_diffusers_experimental_README.md.txt",
                    "text": "diffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model."
                  }
                ],
                "What method is used to load a checkpoint for a task using `AutoPipeline`?\n": [
                  {
                    "chunkId": "Overlapping_Chunking_4_LangChain_TextSplitter",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                    "text": "-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_7_LangChain_TextSplitter",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                    "text": "To seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one."
                  },
                  {
                    "chunkId": "Overlapping_Chunking_6_LangChain_TextSplitter",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                    "text": "Based on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method."
                  },
                  {
                    "chunkId": "Overlapping_Chunking_41_LangChain_TextSplitter",
                    "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                    "text": "for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_15_LangChain_TextSplitter",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                    "text": "- from_pipe\r\n\r\n## AutoPipelineForInpainting\r\n\r\n[[autodoc]] AutoPipelineForInpainting\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_0_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                    "text": "! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_1_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                    "text": "to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_4_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_diffusers_blob_main_docs_source_en_api_pipelines_auto_pipeline.md.txt",
                    "text": "autodoc ] ] autopipelineforimage2image - all - from _ pretrained - from _ pipe # # autopipelineforinpainting [ [ autodoc ] ] autopipelineforinpainting - all - from _ pretrained - from _ pipe"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_10_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                    "text": "weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #"
                  },
                  {
                    "chunkId": "Overlapping_Chunking_6_HuggingFace_Tokenizers",
                    "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                    "text": ". bin ', ' pytorch _ model - 00002 - of - 00003. bin ', ' pytorch _ model - 00003 - of - 00003. bin ', ' pytorch _ model. bin. index. json ' ] ` ` ` on top of the configuration of the model, we see three different weights files, and an ` index. json ` file which is our index. a checkpoint like this can be fully reloaded using the [ ` ~ pretrainedmodel. from _ pretrained ` ] method : ` ` ` py > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir"
                  }
                ]
              }
            }
          }
        },
        "dragging": false,
        "id": "retrieval-e8c56605-eb3b-4a26-a4c0-6b437d2d517d",
        "position": {
          "x": -880,
          "y": 496
        },
        "positionAbsolute": {
          "x": -880,
          "y": 496
        },
        "selected": false,
        "type": "retrieval"
      },
      {
        "width": 222,
        "height": 283,
        "data": {
          "fields": [
            {
              "text": "the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nhat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the\n\nlibrary. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in\n\n### HF_HUB_DISABLE_TELEMETRY\r\n\r\nBy default, some data is collected by HF libraries (`transformers`, `datasets`, `gradio`,..) to monitor usage, debug issues and help prioritize features.",
              "fill_history": {
                "baseMethod": "bm25",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
                "retrievalMethod": "BM25"
              },
              "metavars": {
                "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
                "baseMethod": "bm25",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "method": "BM25",
                "reference": "distilbert base uncased finetuned sst2 english",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
              },
              "uid": "1258cfb6-9c83-4fd2-a1d6-cce00964039e"
            },
            {
              "text": "Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional\n\nattention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should\n\nclass. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after",
              "fill_history": {
                "baseMethod": "bm25",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
                "retrievalMethod": "BM25"
              },
              "metavars": {
                "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
                "baseMethod": "bm25",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "method": "BM25",
                "reference": "10GB",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
              },
              "uid": "81892b45-ef15-49fc-a829-96ec411b949e"
            },
            {
              "text": "`tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n\n-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\non linux machines. \r\n\r\nIf `HF_HOME` is not set, the default home will be `\"$XDG_CACHE_HOME/huggingface\"` instead\r\nof `\"~/.cache/huggingface\"`.\n\n[on macOS and Linux](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/)\r\nand on [Windows](https://phoenixnap.com/kb/windows-set-environment-variable).\n\nattention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the",
              "fill_history": {
                "baseMethod": "bm25",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
                "retrievalMethod": "BM25"
              },
              "metavars": {
                "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
                "baseMethod": "bm25",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "method": "BM25",
                "reference": "x86_64-unknown-linux-musl",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
              },
              "uid": "77068abc-c5dd-4754-bb98-018c2b7b518e"
            },
            {
              "text": "- [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of\n\nguidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nðŸ§¨ Diffusers Experimental\r\n\r\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\r\nCurrently, the following experiments are supported:\n\nhat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the\n\n- **Encouraging safety in deployment**",
              "fill_history": {
                "baseMethod": "bm25",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
                "retrievalMethod": "BM25"
              },
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
                "baseMethod": "bm25",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "method": "BM25",
                "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
              },
              "uid": "50d23599-1adf-4309-a2b1-e0433705f7d1"
            },
            {
              "text": "-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should\n\nthe checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two",
              "fill_history": {
                "baseMethod": "bm25",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
                "retrievalMethod": "BM25"
              },
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
                "baseMethod": "bm25",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "method": "BM25",
                "reference": "from_pretrained()",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
              },
              "uid": "7ef6a74a-ce30-4a8f-a4e0-80cc345a7d83"
            },
            {
              "text": "the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nlibrary. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in\n\nsteps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage,\n\nits unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,",
              "fill_history": {
                "baseMethod": "cosine",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
                "retrievalMethod": "Cosine Similarity"
              },
              "metavars": {
                "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
                "baseMethod": "cosine",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "reference": "distilbert base uncased finetuned sst2 english",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
              },
              "uid": "b5b82eaf-5c4d-46e4-aba9-253dd22dce54"
            },
            {
              "text": "Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do\n\n```py\r\n>>> from transformers.modeling_utils import load_sharded_checkpoint\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n\nfor a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the",
              "fill_history": {
                "baseMethod": "cosine",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
                "retrievalMethod": "Cosine Similarity"
              },
              "metavars": {
                "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
                "baseMethod": "cosine",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "reference": "10GB",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
              },
              "uid": "3135ebd4-e07f-4470-9e43-1adfbb43ba6a"
            },
            {
              "text": "`tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n\nlibrary, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or\n\ntokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning\n\nits unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,\n\nNow let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well",
              "fill_history": {
                "baseMethod": "cosine",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
                "retrievalMethod": "Cosine Similarity"
              },
              "metavars": {
                "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
                "baseMethod": "cosine",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "reference": "x86_64-unknown-linux-musl",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
              },
              "uid": "a3e7219f-9811-4c8a-85cf-5f8911d2f970"
            },
            {
              "text": "- [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of\n\nguidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\n-->\r\n\r\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\r\n\r\n## Preamble\n\n- [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.",
              "fill_history": {
                "baseMethod": "cosine",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
                "retrievalMethod": "Cosine Similarity"
              },
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
                "baseMethod": "cosine",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
              },
              "uid": "b8c1bb74-05e3-4bcd-8919-13aebbd68085"
            },
            {
              "text": "-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nfor a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n- from_pipe\r\n\r\n## AutoPipelineForInpainting\r\n\r\n[[autodoc]] AutoPipelineForInpainting\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe",
              "fill_history": {
                "baseMethod": "cosine",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
                "retrievalMethod": "Cosine Similarity"
              },
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
                "baseMethod": "cosine",
                "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "reference": "from_pretrained()",
                "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
              },
              "uid": "153d6a99-a273-4dd5-b732-98ff7f5f08f5"
            },
            {
              "text": "the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nto a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all\n\n##try by default, some data is collected by hf libraries ( ` transformers `, ` datasets `, ` gradio `,.. ) to monitor usage, debug issues and help prioritize features. each library defines its own policy ( i. e. which usage to monitor ) but the core implementation happens in ` huggingface _ hub ` ( see [ ` send _ telemetry ` ] ). you can set ` hf _ hub _ disable _ telemetry = 1 ` as environment variable to globally disable telemetry. # # # hf _ hub _ enable _ hf _ transfer set to ` true ` for faster uploads and downloads from the hub using ` hf _ transfer `. by default, ` huggingface _ hub ` uses the python - based ` requests. get ` and ` requests. post ` functions. although these are reliable and versatile, they may not be the most efficient choice for machines with high\n\n. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs.",
              "fill_history": {
                "baseMethod": "bm25",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
                "retrievalMethod": "BM25"
              },
              "metavars": {
                "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
                "baseMethod": "bm25",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "method": "BM25",
                "reference": "distilbert base uncased finetuned sst2 english",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
              },
              "uid": "82779f2a-1f2b-4a95-af41-2d2c7012e813"
            },
            {
              "text": "slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\ntraining, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when\n\nsize of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] \\{ ' total _ size ' : 433245184 \\} ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] \\{ ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre\n\n... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )",
              "fill_history": {
                "baseMethod": "bm25",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
                "retrievalMethod": "BM25"
              },
              "metavars": {
                "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
                "baseMethod": "bm25",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "method": "BM25",
                "reference": "10GB",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
              },
              "uid": "65b0fefe-da51-431c-8a9d-38349b2357e2"
            },
            {
              "text": "` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `\n\nhf _ home ` is not set! this is the default way to configure where [ user - specific non - essential ( cached ) data should be written ] ( https : / / wiki. archlinux. org / title / xdg _ base _ directory ) on linux machines. if ` hf _ home ` is not set, the default home will be ` \" $ xdg _ cache _ home / huggingface \" ` instead of ` \" ~ /. cache / huggingface \" `.\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\n. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size\n\n! - - note that this file is in markdown but contain specific syntax for our doc - builder ( similar to mdx ) that may not be rendered properly in your markdown viewer. - - > # environment variables ` huggingface _ hub ` can be configured using environment variables. if you are unfamiliar with environment variable, here are generic articles about them [ on macos and linux ] ( https : / / linuxize. com / post / how - to - set - and - list - environment - variables - in - linux / ) and on [ windows ] ( https : / / phoenixnap. com / kb / windows - set - environment - variable ). this page will guide you through all environment variables specific to ` huggingface _ hub ` and their meaning. # # generic # # # hf _ inference _ endpoint to configure the inference api base url. you might want to set this variable if your organization is pointing at an api gateway rather",
              "fill_history": {
                "baseMethod": "bm25",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
                "retrievalMethod": "BM25"
              },
              "metavars": {
                "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
                "baseMethod": "bm25",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "method": "BM25",
                "reference": "x86_64-unknown-linux-musl",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
              },
              "uid": "e5e4347c-c12c-4ccc-a2c9-431809dfc69e"
            },
            {
              "text": "diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https\n\nhttps : / / huggingface. co / spaces / society - ethics / diffusionbiasexplorer ) to demonstrate the biases in stable diffusion interactively. in this sense, we support and encourage bias explorers and evaluations. - * * encouraging safety in deployment * * - [ * * safe stable diffusion * * ] ( https : / / huggingface. co / docs / diffusers / main / en / api / pipelines / stable _ diffusion / stable _ diffusion _ safe ) : it mitigates the well - known issue that models, like stable diffusion, that are trained on unfiltered, web - crawled datasets tend to suffer from inappropriate degeneration. related paper : [ safe latent diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / gi\n\nsame level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and\n\ndiffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special",
              "fill_history": {
                "baseMethod": "bm25",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
                "retrievalMethod": "BM25"
              },
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
                "baseMethod": "bm25",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "method": "BM25",
                "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
              },
              "uid": "922b2750-c385-438b-97aa-cb9319433db9"
            },
            {
              "text": "to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\nthe first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\n:... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir ) ` ` ` the main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in ram to the model size plus the size of the biggest shard. behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. we can load that index like any json and get a dictionary : ` ` ` py > > > import json > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ sha",
              "fill_history": {
                "baseMethod": "bm25",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
                "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
                "retrievalMethod": "BM25"
              },
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
                "baseMethod": "bm25",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "method": "BM25",
                "reference": "from_pretrained()",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
              },
              "uid": "378f8a7a-4613-49ca-8ca1-e8c685f9cc5b"
            },
            {
              "text": "the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nto a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all\n\nremoved. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. to get an output linked to our classification problem, we need to use the automodelforsequenceclassification class. it works exactly as the automodel class, except that it will build a model with a classification head. there is one auto class for each common nlp task in the transformers library. here, after giving our model the two sentences, we get a tensor of size two by two : one result for each sentence and for each possible label. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third\n\n. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs.",
              "fill_history": {
                "baseMethod": "cosine",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
                "retrievalMethod": "Cosine Similarity"
              },
              "metavars": {
                "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
                "baseMethod": "cosine",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "reference": "distilbert base uncased finetuned sst2 english",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
              },
              "uid": "a8a3e1a5-0669-4f59-81a0-b42c7641982f"
            },
            {
              "text": "slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~\n\ntraining, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\n... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )\n\nsize of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] \\{ ' total _ size ' : 433245184 \\} ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] \\{ ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre",
              "fill_history": {
                "baseMethod": "cosine",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
                "retrievalMethod": "Cosine Similarity"
              },
              "metavars": {
                "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
                "baseMethod": "cosine",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "reference": "10GB",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
              },
              "uid": "c1ab4cd7-3c4b-4f60-ad2b-fbe67338c078"
            },
            {
              "text": "` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `\n\nboth variants regardless. this change will improve performance for many applications. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5216 ] ( https : / / github. com / gradio - app / gradio / pull / 5216 ) [ ` 4b58ea6d ` ] ( https : / / github. com / gradio - app / gradio / commit / 4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8 ) - update i18n tokens and locale files. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )! # # 0. 2. 0 # # # features - [ # 5025 ] ( https : / / github. com / gradio -\n\nthe first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\n. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special",
              "fill_history": {
                "baseMethod": "cosine",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
                "retrievalMethod": "Cosine Similarity"
              },
              "metavars": {
                "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
                "baseMethod": "cosine",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "reference": "x86_64-unknown-linux-musl",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
              },
              "uid": "a99d7353-31ef-44aa-ab99-79a44cbd6e5f"
            },
            {
              "text": "diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https\n\nsame level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and\n\n' s responsiveness and valuable feedback. # # scope the diffusers community will apply the following ethical guidelines to the project â€™ s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns. # # ethical guidelines the following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question. - * * transparency * * : we are committed to being transparent in managing prs, explaining our choices to users, and making technical decisions. - * * consistency * * : we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # diffusers â€™ ethical guidelines # # preamble [ diffusers ] ( https : / / huggingface. co / docs / diffusers / index ) provides pre - trained diffusion models and serves as a modular toolbox for inference and training. given its real case applications in the world and potential negative impacts on society, we think it\n\ndiffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model.",
              "fill_history": {
                "baseMethod": "cosine",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
                "retrievalMethod": "Cosine Similarity"
              },
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
                "baseMethod": "cosine",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
              },
              "uid": "b9b8bc0e-aecf-41d1-af14-a7950ecf216a"
            },
            {
              "text": "! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\nto load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,\n\nautodoc ] ] autopipelineforimage2image - all - from _ pretrained - from _ pipe # # autopipelineforinpainting [ [ autodoc ] ] autopipelineforinpainting - all - from _ pretrained - from _ pipe\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\n. bin ', ' pytorch _ model - 00002 - of - 00003. bin ', ' pytorch _ model - 00003 - of - 00003. bin ', ' pytorch _ model. bin. index. json ' ] ` ` ` on top of the configuration of the model, we see three different weights files, and an ` index. json ` file which is our index. a checkpoint like this can be fully reloaded using the [ ` ~ pretrainedmodel. from _ pretrained ` ] method : ` ` ` py > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir",
              "fill_history": {
                "baseMethod": "cosine",
                "chunkLibrary": "",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
                "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
                "retrievalMethod": "Cosine Similarity"
              },
              "metavars": {
                "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
                "baseMethod": "cosine",
                "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
                "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
                "method": "Cosine Similarity",
                "reference": "from_pretrained()",
                "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
                "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
              },
              "uid": "9711c621-7e76-4669-ad25-06f4a8645b8f"
            }
          ],
          "groupByLLM": "across",
          "groupByVars": [
            {
              "label": "all text",
              "value": "A"
            },
            {
              "label": "by baseMethod",
              "value": "VbaseMethod"
            },
            {
              "label": "by chunkId",
              "value": "VchunkId"
            },
            {
              "label": "by chunkLibrary",
              "value": "VchunkLibrary"
            },
            {
              "label": "by chunkMethod",
              "value": "VchunkMethod"
            },
            {
              "label": "by docTitle",
              "value": "VdocTitle"
            },
            {
              "label": "by methodId",
              "value": "VmethodId"
            },
            {
              "label": "by query",
              "value": "Vquery"
            },
            {
              "label": "by retrievalMethod",
              "value": "VretrievalMethod"
            },
            {
              "label": "by similarity",
              "value": "Vsimilarity"
            },
            {
              "label": "by embeddingModel",
              "value": "VembeddingModel"
            },
            {
              "label": "by answer_context (meta)",
              "value": "Manswer_context"
            },
            {
              "label": "by baseMethod (meta)",
              "value": "MbaseMethod"
            },
            {
              "label": "by chunkId (meta)",
              "value": "MchunkId"
            },
            {
              "label": "by chunkMethod (meta)",
              "value": "MchunkMethod"
            },
            {
              "label": "by docTitle (meta)",
              "value": "MdocTitle"
            },
            {
              "label": "by method (meta)",
              "value": "Mmethod"
            },
            {
              "label": "by rank (meta)",
              "value": "Mrank"
            },
            {
              "label": "by reference (meta)",
              "value": "Mreference"
            },
            {
              "label": "by signature (meta)",
              "value": "Msignature"
            },
            {
              "label": "by source_doc (meta)",
              "value": "Msource_doc"
            },
            {
              "label": "by embeddingModel (meta)",
              "value": "MembeddingModel"
            }
          ],
          "refresh": false,
          "selectedGroupVars": [
            "VchunkMethod",
            "VretrievalMethod",
            "Vquery"
          ]
        },
        "dragging": false,
        "id": "join-72bbabbc-ede0-4ba5-9797-ac161da5e1e2",
        "position": {
          "x": -352,
          "y": 592
        },
        "positionAbsolute": {
          "x": -352,
          "y": 592
        },
        "selected": false,
        "type": "join"
      },
      {
        "width": 282,
        "height": 145,
        "id": "comment-9f37d19f-7be6-4501-8c90-ad0f8f7c55b6",
        "type": "comment",
        "data": {
          "text": "Knowledge base: 10 docs from the Hugging Face documentation\n"
        },
        "position": {
          "x": -2064,
          "y": 672
        },
        "selected": false,
        "positionAbsolute": {
          "x": -2064,
          "y": 672
        },
        "dragging": false
      },
      {
        "width": 282,
        "height": 145,
        "id": "comment-7732d1d4-0bfe-4ca4-a26a-dc0fbc153c8a",
        "type": "comment",
        "data": {
          "text": "QA dataset: LLM-synthesized question and answer pairs using the Hugging Face docs"
        },
        "position": {
          "x": -1296,
          "y": -80
        },
        "selected": false,
        "positionAbsolute": {
          "x": -1296,
          "y": -80
        },
        "dragging": false
      }
    ],
    "edges": [
      {
        "id": "reactflow__edge-upload-e9d11bb2-f4f5-43a2-8d28-4c54f345659etext-chunking-ace7ec19-152e-4a59-93d1-8eb7fc3d19detext",
        "interactionWidth": 40,
        "markerEnd": {
          "height": 22,
          "type": "arrow",
          "width": 22
        },
        "source": "upload-e9d11bb2-f4f5-43a2-8d28-4c54f345659e",
        "sourceHandle": "text",
        "target": "chunking-ace7ec19-152e-4a59-93d1-8eb7fc3d19de",
        "targetHandle": "text",
        "type": "default"
      },
      {
        "id": "reactflow__edge-chunking-ace7ec19-152e-4a59-93d1-8eb7fc3d19deoutput-retrieval-3359ce4e-b556-4d0a-8322-8aa3ab862788chunks",
        "interactionWidth": 40,
        "markerEnd": {
          "height": 22,
          "type": "arrow",
          "width": 22
        },
        "source": "chunking-ace7ec19-152e-4a59-93d1-8eb7fc3d19de",
        "sourceHandle": "output",
        "target": "retrieval-3359ce4e-b556-4d0a-8322-8aa3ab862788",
        "targetHandle": "chunks",
        "type": "default"
      },
      {
        "id": "reactflow__edge-table-ab424041-6fa2-46b0-9121-517805638f94question-retrieval-3359ce4e-b556-4d0a-8322-8aa3ab862788queries",
        "interactionWidth": 40,
        "markerEnd": {
          "height": 22,
          "type": "arrow",
          "width": 22
        },
        "source": "table-ab424041-6fa2-46b0-9121-517805638f94",
        "sourceHandle": "question",
        "target": "retrieval-3359ce4e-b556-4d0a-8322-8aa3ab862788",
        "targetHandle": "queries",
        "type": "default"
      },
      {
        "id": "reactflow__edge-retrieval-3359ce4e-b556-4d0a-8322-8aa3ab862788output-join-6e386790-b3c4-4639-a84b-232737e7d8ed__input",
        "interactionWidth": 40,
        "markerEnd": {
          "height": 22,
          "type": "arrow",
          "width": 22
        },
        "source": "retrieval-3359ce4e-b556-4d0a-8322-8aa3ab862788",
        "sourceHandle": "output",
        "target": "join-6e386790-b3c4-4639-a84b-232737e7d8ed",
        "targetHandle": "__input",
        "type": "default"
      },
      {
        "id": "reactflow__edge-join-6e386790-b3c4-4639-a84b-232737e7d8edoutput-promptNode-d95a71d4-5df6-4689-9bb1-f18b2f9049d3context",
        "interactionWidth": 40,
        "markerEnd": {
          "height": 22,
          "type": "arrow",
          "width": 22
        },
        "source": "join-6e386790-b3c4-4639-a84b-232737e7d8ed",
        "sourceHandle": "output",
        "target": "promptNode-d95a71d4-5df6-4689-9bb1-f18b2f9049d3",
        "targetHandle": "context",
        "type": "default"
      },
      {
        "id": "reactflow__edge-promptNode-d95a71d4-5df6-4689-9bb1-f18b2f9049d3prompt-multieval-7beb361f-660e-4a12-a370-1c2bb8cb0818responseBatch",
        "interactionWidth": 40,
        "markerEnd": {
          "height": 22,
          "type": "arrow",
          "width": 22
        },
        "source": "promptNode-d95a71d4-5df6-4689-9bb1-f18b2f9049d3",
        "sourceHandle": "prompt",
        "target": "multieval-7beb361f-660e-4a12-a370-1c2bb8cb0818",
        "targetHandle": "responseBatch",
        "type": "default"
      },
      {
        "id": "reactflow__edge-multieval-7beb361f-660e-4a12-a370-1c2bb8cb0818output-visNode-33c183d7-6baf-4954-a2a4-e06ce50add74input",
        "interactionWidth": 40,
        "markerEnd": {
          "height": 22,
          "type": "arrow",
          "width": 22
        },
        "source": "multieval-7beb361f-660e-4a12-a370-1c2bb8cb0818",
        "sourceHandle": "output",
        "target": "visNode-33c183d7-6baf-4954-a2a4-e06ce50add74",
        "targetHandle": "input",
        "type": "default"
      },
      {
        "id": "reactflow__edge-upload-e9d11bb2-f4f5-43a2-8d28-4c54f345659etext-chunking-9a8eac2d-45cc-460f-ad25-a59404dfd84btext",
        "interactionWidth": 40,
        "markerEnd": {
          "height": 22,
          "type": "arrow",
          "width": 22
        },
        "source": "upload-e9d11bb2-f4f5-43a2-8d28-4c54f345659e",
        "sourceHandle": "text",
        "target": "chunking-9a8eac2d-45cc-460f-ad25-a59404dfd84b",
        "targetHandle": "text",
        "type": "default"
      },
      {
        "id": "reactflow__edge-table-ab424041-6fa2-46b0-9121-517805638f94question-retrieval-e8c56605-eb3b-4a26-a4c0-6b437d2d517dqueries",
        "interactionWidth": 40,
        "markerEnd": {
          "height": 22,
          "type": "arrow",
          "width": 22
        },
        "source": "table-ab424041-6fa2-46b0-9121-517805638f94",
        "sourceHandle": "question",
        "target": "retrieval-e8c56605-eb3b-4a26-a4c0-6b437d2d517d",
        "targetHandle": "queries",
        "type": "default"
      },
      {
        "id": "reactflow__edge-chunking-9a8eac2d-45cc-460f-ad25-a59404dfd84boutput-retrieval-e8c56605-eb3b-4a26-a4c0-6b437d2d517dchunks",
        "interactionWidth": 40,
        "markerEnd": {
          "height": 22,
          "type": "arrow",
          "width": 22
        },
        "source": "chunking-9a8eac2d-45cc-460f-ad25-a59404dfd84b",
        "sourceHandle": "output",
        "target": "retrieval-e8c56605-eb3b-4a26-a4c0-6b437d2d517d",
        "targetHandle": "chunks",
        "type": "default"
      },
      {
        "id": "reactflow__edge-retrieval-e8c56605-eb3b-4a26-a4c0-6b437d2d517doutput-join-6e386790-b3c4-4639-a84b-232737e7d8ed__input",
        "interactionWidth": 40,
        "markerEnd": {
          "height": 22,
          "type": "arrow",
          "width": 22
        },
        "source": "retrieval-e8c56605-eb3b-4a26-a4c0-6b437d2d517d",
        "sourceHandle": "output",
        "target": "join-6e386790-b3c4-4639-a84b-232737e7d8ed",
        "targetHandle": "__input",
        "type": "default"
      },
      {
        "id": "reactflow__edge-retrieval-e8c56605-eb3b-4a26-a4c0-6b437d2d517doutput-join-72bbabbc-ede0-4ba5-9797-ac161da5e1e2__input",
        "interactionWidth": 40,
        "markerEnd": {
          "height": 22,
          "type": "arrow",
          "width": 22
        },
        "source": "retrieval-e8c56605-eb3b-4a26-a4c0-6b437d2d517d",
        "sourceHandle": "output",
        "target": "join-72bbabbc-ede0-4ba5-9797-ac161da5e1e2",
        "targetHandle": "__input",
        "type": "default"
      },
      {
        "id": "reactflow__edge-join-72bbabbc-ede0-4ba5-9797-ac161da5e1e2output-promptNode-d95a71d4-5df6-4689-9bb1-f18b2f9049d3context",
        "interactionWidth": 40,
        "markerEnd": {
          "height": 22,
          "type": "arrow",
          "width": 22
        },
        "source": "join-72bbabbc-ede0-4ba5-9797-ac161da5e1e2",
        "sourceHandle": "output",
        "target": "promptNode-d95a71d4-5df6-4689-9bb1-f18b2f9049d3",
        "targetHandle": "context",
        "type": "default"
      }
    ],
    "viewport": {
      "x": 1187.1759259259254,
      "y": 65.81481481481504,
      "zoom": 0.5787037037037038
    }
  },
  "cache": {
    "multieval-7beb361f-660e-4a12-a370-1c2bb8cb0818.json": [
      {
        "llm": "gpt-4o-mini",
        "metavars": {
          "LLM_0": "gpt-4o-mini",
          "answer_context": 22,
          "baseMethod": 34,
          "chunkMethod": 5,
          "method": 36,
          "reference": 21,
          "signature": 38,
          "source_doc": 23
        },
        "prompt": 182,
        "responses": [
          17
        ],
        "uid": "fac8100e-19a6-4045-b612-2587c5fde08b",
        "vars": {
          "baseMethod": 34,
          "chunkLibrary": 4,
          "chunkMethod": 5,
          "context": 37,
          "methodId": 92,
          "query": 19,
          "retrievalMethod": 36
        },
        "eval_res": {
          "items": [
            {
              "Faithfulness": "1.0",
              "Answer Relevancy": 1,
              "Context Precision": "0.5",
              "Context Relevancy": 1,
              "Answer Correctness": "0.5",
              "Chunk Overlap": true
            }
          ],
          "dtype": "KeyValue_Mixed"
        }
      },
      {
        "llm": "gpt-4o-mini",
        "metavars": {
          "LLM_0": "gpt-4o-mini",
          "answer_context": 13,
          "baseMethod": 34,
          "chunkMethod": 5,
          "method": 36,
          "reference": 12,
          "signature": 38,
          "source_doc": 15
        },
        "prompt": 183,
        "responses": [
          2
        ],
        "uid": "07647594-a268-4772-8358-783edb8c47e2",
        "vars": {
          "baseMethod": 34,
          "chunkLibrary": 4,
          "chunkMethod": 5,
          "context": 43,
          "methodId": 92,
          "query": 9,
          "retrievalMethod": 36
        },
        "eval_res": {
          "items": [
            {
              "Faithfulness": "1.0",
              "Answer Relevancy": 1,
              "Context Precision": "To evaluate the context precision, let's first break down the context and the answer provided.\n\n### Identifying Relevant Parts of the Context\n\nThe relevant portion of the context is:\n- \"the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english.\"\n\nThe answer states:\n- \"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n\n### Analysis\n\n1. **Information Density**:\n   - The context does contain information explicitly stating the default checkpoint for the sentiment analysis pipeline, which is essential to answer the question.\n   - However, there is surrounding information in the context that does not pertain directly to the question (e.g., details about the tokenizer, methods from the class, and the note on telemetry). \n\n2. **Information Utilization**:\n   - The specific detail regarding the checkpoint is directly quoted in the answer. The answer used only the relevant part without needing any extraneous information provided in the context.\n   - Thus, the relevant information that was used to construct the answer is present and utilized effectively.\n\n### Calculation of Precision\n- The relevant context used for the answer was a very clear statement of the default checkpoint.\n- The entire context provided is somewhat distracting, with minor details about methods and telemetry that do not directly contribute to the answer.\n\nBased on this:\n- Out of the total retrieved context, only a small fraction (one distinct sentence) provides the required information to answer the question. The rest is irrelevant.\n- Therefore, the information density is low as the rest of the context is largely irrelevant. However, what was relevant was fully utilized.\n\n### Final Rating\nGiven the analysis:\n- Due to the low density of relevant context (only 1 sentence out of multiple sentences being relevant), I would rate the precision as moderate, but with the information used to answer effectively. Thus, I would land on the lower side of moderate precision.\n\nFinal score justification:\n- Information density: Low (due to irrelevant details) \n- Information utilization: High (the relevant information was used effectively).\n\nFinal score: **0.5**. \n\nThus, the output is:\n```\nSCORE: 0.5\n```",
              "Context Relevancy": 1,
              "Answer Correctness": "1.0",
              "Chunk Overlap": true
            }
          ],
          "dtype": "KeyValue_Mixed"
        }
      },
      {
        "llm": "gpt-4o-mini",
        "metavars": {
          "LLM_0": "gpt-4o-mini",
          "answer_context": 22,
          "baseMethod": 3,
          "chunkMethod": 5,
          "docTitle": 18,
          "embeddingModel": 7,
          "method": 10,
          "reference": 21,
          "signature": 14,
          "source_doc": 23
        },
        "prompt": 184,
        "responses": [
          17
        ],
        "uid": "2621bb44-b73e-40af-a553-3768d82cfc9a",
        "vars": {
          "baseMethod": 3,
          "chunkLibrary": 4,
          "chunkMethod": 5,
          "context": 20,
          "docTitle": 18,
          "embeddingModel": 7,
          "methodId": 93,
          "query": 19,
          "retrievalMethod": 10
        },
        "eval_res": {
          "items": [
            {
              "Faithfulness": "1.0",
              "Answer Relevancy": 1,
              "Context Precision": "0.5",
              "Context Relevancy": 1,
              "Answer Correctness": "1.0",
              "Chunk Overlap": true
            }
          ],
          "dtype": "KeyValue_Mixed"
        }
      },
      {
        "llm": "gpt-4o-mini",
        "metavars": {
          "LLM_0": "gpt-4o-mini",
          "answer_context": 49,
          "baseMethod": 34,
          "chunkMethod": 5,
          "method": 36,
          "reference": 48,
          "signature": 38,
          "source_doc": 50
        },
        "prompt": 185,
        "responses": [
          45
        ],
        "uid": "1846f4c6-4ed0-4505-9e24-002214fe6ed5",
        "vars": {
          "baseMethod": 34,
          "chunkLibrary": 4,
          "chunkMethod": 5,
          "context": 47,
          "methodId": 92,
          "query": 46,
          "retrievalMethod": 36
        },
        "eval_res": {
          "items": [
            {
              "Faithfulness": "0.8333",
              "Answer Relevancy": 1,
              "Context Precision": "0.5",
              "Context Relevancy": 1,
              "Answer Correctness": "0.25",
              "Chunk Overlap": true
            }
          ],
          "dtype": "KeyValue_Mixed"
        }
      },
      {
        "llm": "gpt-4o-mini",
        "metavars": {
          "LLM_0": "gpt-4o-mini",
          "answer_context": 30,
          "baseMethod": 34,
          "chunkMethod": 5,
          "method": 36,
          "reference": 29,
          "signature": 38,
          "source_doc": 32
        },
        "prompt": 186,
        "responses": [
          40
        ],
        "uid": "41ea9c17-ae54-417a-90b3-eef3320edcf8",
        "vars": {
          "baseMethod": 34,
          "chunkLibrary": 4,
          "chunkMethod": 5,
          "context": 41,
          "methodId": 92,
          "query": 27,
          "retrievalMethod": 36
        },
        "eval_res": {
          "items": [
            {
              "Faithfulness": "1.0",
              "Answer Relevancy": 1,
              "Context Precision": "1.0",
              "Context Relevancy": 1,
              "Answer Correctness": "1.0",
              "Chunk Overlap": false
            }
          ],
          "dtype": "KeyValue_Mixed"
        }
      },
      {
        "llm": "gpt-4o-mini",
        "metavars": {
          "LLM_0": "gpt-4o-mini",
          "answer_context": 63,
          "baseMethod": 34,
          "chunkMethod": 5,
          "method": 36,
          "reference": 62,
          "signature": 38,
          "source_doc": 64
        },
        "prompt": 187,
        "responses": [
          188
        ],
        "uid": "8442ebb0-fb6f-4b9f-ae0d-3bf0077e460f",
        "vars": {
          "baseMethod": 34,
          "chunkLibrary": 4,
          "chunkMethod": 5,
          "context": 61,
          "methodId": 92,
          "query": 60,
          "retrievalMethod": 36
        },
        "eval_res": {
          "items": [
            {
              "Faithfulness": "1.0",
              "Answer Relevancy": 1,
              "Context Precision": "0.8",
              "Context Relevancy": 1,
              "Answer Correctness": "0.67",
              "Chunk Overlap": true
            }
          ],
          "dtype": "KeyValue_Mixed"
        }
      },
      {
        "llm": "gpt-4o-mini",
        "metavars": {
          "LLM_0": "gpt-4o-mini",
          "answer_context": 13,
          "baseMethod": 3,
          "chunkMethod": 5,
          "docTitle": 6,
          "embeddingModel": 7,
          "method": 10,
          "reference": 12,
          "signature": 14,
          "source_doc": 15
        },
        "prompt": 189,
        "responses": [
          2
        ],
        "uid": "e5b2b334-688f-48a3-b9ba-22e095010318",
        "vars": {
          "baseMethod": 3,
          "chunkLibrary": 4,
          "chunkMethod": 5,
          "context": 11,
          "docTitle": 6,
          "embeddingModel": 7,
          "methodId": 93,
          "query": 9,
          "retrievalMethod": 10
        },
        "eval_res": {
          "items": [
            {
              "Faithfulness": "1.0",
              "Answer Relevancy": 1,
              "Context Precision": "0.5",
              "Context Relevancy": 1,
              "Answer Correctness": "0.5",
              "Chunk Overlap": true
            }
          ],
          "dtype": "KeyValue_Mixed"
        }
      },
      {
        "llm": "gpt-4o-mini",
        "metavars": {
          "LLM_0": "gpt-4o-mini",
          "answer_context": 30,
          "baseMethod": 3,
          "chunkMethod": 5,
          "embeddingModel": 7,
          "method": 10,
          "reference": 29,
          "signature": 14,
          "source_doc": 32
        },
        "prompt": 190,
        "responses": [
          191
        ],
        "uid": "b1a55fe8-d756-4029-a44a-6d1a60268943",
        "vars": {
          "baseMethod": 3,
          "chunkLibrary": 4,
          "chunkMethod": 5,
          "context": 69,
          "embeddingModel": 7,
          "methodId": 93,
          "query": 27,
          "retrievalMethod": 10
        },
        "eval_res": {
          "items": [
            {
              "Faithfulness": "0.75",
              "Answer Relevancy": 1,
              "Context Precision": "0.5",
              "Context Relevancy": 1,
              "Answer Correctness": "1.0",
              "Chunk Overlap": false
            }
          ],
          "dtype": "KeyValue_Mixed"
        }
      },
      {
        "llm": "gpt-4o-mini",
        "metavars": {
          "LLM_0": "gpt-4o-mini",
          "answer_context": 30,
          "baseMethod": 34,
          "chunkMethod": 26,
          "method": 36,
          "reference": 29,
          "signature": 53,
          "source_doc": 32
        },
        "prompt": 192,
        "responses": [
          25
        ],
        "uid": "7fc6868b-42f7-4ed7-8ddb-bb8aa6f0562d",
        "vars": {
          "baseMethod": 34,
          "chunkLibrary": 4,
          "chunkMethod": 26,
          "context": 52,
          "methodId": 92,
          "query": 27,
          "retrievalMethod": 36
        },
        "eval_res": {
          "items": [
            {
              "Faithfulness": "1.0",
              "Answer Relevancy": 1,
              "Context Precision": "0.5",
              "Context Relevancy": 1,
              "Answer Correctness": "0.83",
              "Chunk Overlap": false
            }
          ],
          "dtype": "KeyValue_Mixed"
        }
      },
      {
        "llm": "gpt-4o-mini",
        "metavars": {
          "LLM_0": "gpt-4o-mini",
          "answer_context": 63,
          "baseMethod": 3,
          "chunkMethod": 5,
          "embeddingModel": 7,
          "method": 10,
          "reference": 62,
          "signature": 14,
          "source_doc": 64
        },
        "prompt": 193,
        "responses": [
          194
        ],
        "uid": "fc04cf70-1464-499a-8a70-99c4077b8ec9",
        "vars": {
          "baseMethod": 3,
          "chunkLibrary": 4,
          "chunkMethod": 5,
          "context": 67,
          "embeddingModel": 7,
          "methodId": 93,
          "query": 60,
          "retrievalMethod": 10
        },
        "eval_res": {
          "items": [
            {
              "Faithfulness": "1.0",
              "Answer Relevancy": 1,
              "Context Precision": "0.85",
              "Context Relevancy": 1,
              "Answer Correctness": "Let's evaluate the generated answer against the reference answer.\n\n### Step 1: Identify factual claims in the generated answer.\n1. The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`.\n2. This method automatically retrieves the relevant pipeline based on the name or path to the pretrained weights.\n3. `from_pipe()` can be used to transfer components from the original pipeline to a new one.\n4. Using `from_pipe()` does not reallocate memory.\n\n### Step 2: Determine support for each claim against the reference answer.\n1. **Claim 1**: The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`.\n   - *Evaluation*: This claim is a direct statement that matches the reference answer. It is fully supported. \n   - **Score**: 1 point.\n   \n2. **Claim 2**: This method automatically retrieves the relevant pipeline based on the name or path to the pretrained weights.\n   - *Evaluation*: While this is a logical extrapolation based on the use of `from_pretrained()`, the specific functionality of automatic retrieval is not included in the reference answer. It suggests additional functionality beyond what is explicitly stated.\n   - **Score**: 0.5 points.\n   \n3. **Claim 3**: `from_pipe()` can be used to transfer components from the original pipeline to a new one.\n   - *Evaluation*: This claim introduces a method (`from_pipe()`) that is not mentioned in the reference answer. Hence, it is not supported.\n   - **Score**: 0 points.\n\n4. **Claim 4**: Using `from_pipe()` does not reallocate memory.\n   - *Evaluation*: This is also an assertion about the `from_pipe()` method that is not mentioned in the reference answer, making it unsupported.\n   - **Score**: 0 points.\n\n### Step 3: Calculate the final score.\nNow, let's total the points:\n- Claim 1: 1 point\n- Claim 2: 0.5 points\n- Claim 3: 0 points\n- Claim 4: 0 points\n\nTotal Score = 1 + 0.5 + 0 + 0 = 1.5 points.\n\nGiven that the maximum possible score is 4 points (1 for each claim), we can convert this into a score out of 1. \n\nFinal score = Total Score / Maximum Possible Score = 1.5 / 4 = 0.375.\n\nHowever, we will consider how to represent this score in the context of possible evaluation outputs which hold more relevance to the accuracy of the claims, primarily focusing on the correctness of the claims versus the reference answer directly.\n\nThus, I would assign a final answer correctness score based on the support for the correct claims.\n\nThe best representation of the final correctness that reflects the outcome accurately would be:\nSCORE: 0.375. \n\nGiven the requirement to fit strictly within a 0-1 score while reflecting the overall correctness appropriately, it should be conjectured or rounded depending on the usage of the evaluative context.\n\n##### Final output:\n```\nSCORE: 0.5\n```",
              "Chunk Overlap": true
            }
          ],
          "dtype": "KeyValue_Mixed"
        }
      },
      {
        "llm": "gpt-4o-mini",
        "metavars": {
          "LLM_0": "gpt-4o-mini",
          "answer_context": 30,
          "baseMethod": 3,
          "chunkMethod": 26,
          "embeddingModel": 7,
          "method": 10,
          "reference": 29,
          "signature": 31,
          "source_doc": 32
        },
        "prompt": 195,
        "responses": [
          196
        ],
        "uid": "ff70540b-64ca-4b91-969b-1ebc75c328eb",
        "vars": {
          "baseMethod": 3,
          "chunkLibrary": 4,
          "chunkMethod": 26,
          "context": 28,
          "embeddingModel": 7,
          "methodId": 93,
          "query": 27,
          "retrievalMethod": 10
        },
        "eval_res": {
          "items": [
            {
              "Faithfulness": "1.0",
              "Answer Relevancy": 1,
              "Context Precision": "To evaluate the context precision, let's analyze the context and its relevance to the question:\n\n1. **Identifying Relevant Parts Used in the Answer:**\n   - The context mentions: \"this is the *x86_64 - unknown - linux - musl* binary for `tokenizers`.\"\n   - The answer clearly states that the binary is designed for the \"x86_64 architecture on a Linux system using the musl C library.\"\n\n2. **Calculating Proportion of Relevant Information:**\n   - There are two main points in the relevant context: \n     - It identifies the architecture as \"x86_64\".\n     - It specifies the operating system as \"Linux\" and mentions the musl C library.\n   - The retrieved context primarily contains one clear and direct sentence relevant to the answer regarding the architecture and context for the `tokenizers-linux-x64-musl` binary.\n\n3. **Information Density and Utilization:**\n   - **Information Density:** The portion that contains relevant information is primarily the specific mention of \"x86_64 - linux - musl\", which accounts for a concise piece of relevant data.\n   - **Information Utilization:** The necessary informationâ€”architecture and systemâ€”was effectively utilized to answer the question accurately.\n\nTaking this into account:\n\n- All of the relevant context relates directly to the question at hand. Even though the entire context is longer and includes irrelevant information, the precise information used is highly relevant and necessary.\n- Since the utilized context only contains a single key phrase while the rest is irrelevant, the density is low, but utilization is fully high relative to the relevant information present.\n\nGiven the perfect relevance of the utilized information and its use in answering the question, but recognizing that the context includes additional inefficient text, I would rate the overall precision moderately high but not perfect.\n\nThus, I would assign a score of 0.8.\n\nFinal answer:\n```\nSCORE: 0.8\n```",
              "Context Relevancy": 1,
              "Answer Correctness": "0.75",
              "Chunk Overlap": false
            }
          ],
          "dtype": "KeyValue_Mixed"
        }
      },
      {
        "llm": "gpt-4o-mini",
        "metavars": {
          "LLM_0": "gpt-4o-mini",
          "answer_context": 13,
          "baseMethod": 3,
          "chunkMethod": 26,
          "docTitle": 6,
          "embeddingModel": 7,
          "method": 10,
          "reference": 12,
          "signature": 31,
          "source_doc": 15
        },
        "prompt": 197,
        "responses": [
          2
        ],
        "uid": "a7cfeb0a-d170-4fb2-80da-3e7561fd2e9b",
        "vars": {
          "baseMethod": 3,
          "chunkLibrary": 4,
          "chunkMethod": 26,
          "context": 55,
          "docTitle": 6,
          "embeddingModel": 7,
          "methodId": 93,
          "query": 9,
          "retrievalMethod": 10
        },
        "eval_res": {
          "items": [
            {
              "Faithfulness": "1.0",
              "Answer Relevancy": 1,
              "Context Precision": "0.7",
              "Context Relevancy": 1,
              "Answer Correctness": "0.75",
              "Chunk Overlap": false
            }
          ],
          "dtype": "KeyValue_Mixed"
        }
      },
      {
        "llm": "gpt-4o-mini",
        "metavars": {
          "LLM_0": "gpt-4o-mini",
          "answer_context": 22,
          "baseMethod": 34,
          "chunkMethod": 26,
          "docTitle": 18,
          "method": 36,
          "reference": 21,
          "signature": 53,
          "source_doc": 23
        },
        "prompt": 198,
        "responses": [
          199
        ],
        "uid": "c1255722-4c83-4cf2-9435-0f930f332397",
        "vars": {
          "baseMethod": 34,
          "chunkLibrary": 4,
          "chunkMethod": 26,
          "context": 76,
          "docTitle": 18,
          "methodId": 92,
          "query": 19,
          "retrievalMethod": 36
        },
        "eval_res": {
          "items": [
            {
              "Faithfulness": "Let's evaluate the claims in the provided answer based on the context.\n\n**Claims in the Answer:**\n1. \"In Transformers version 4.18.0, model checkpoints that exceed 10 GB in size are automatically sharded into smaller pieces.\"\n2. \"Each of these smaller pieces will be less than 10 GB.\"\n3. \"You can control the maximum size before sharding using the `max_shard_size` parameter.\"\n\n**Assessment of Claims:**\n1. **Claim 1:** This claim is directly supported by the context, which mentions \"sharded checkpoints since version 4.18.0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces.\" \n   - **Score:** 1 point (fully supported).\n\n2. **Claim 2:** This claim is also stated in the context where it indicates that \"model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces (each of which being of size < 10gb).\"\n   - **Score:** 1 point (fully supported).\n\n3. **Claim 3:** The context confirms this by stating, \"you can control the maximum size before sharding with the `max_shard_size` parameter.\"\n   - **Score:** 1 point (fully supported).\n\n**Total points scored:** 3 points  \n**Total number of claims:** 3\n\nTo calculate the faithfulness score:\n\\[ \\text{Faithfulness Score} = \\frac{\\text{Total Points}}{\\text{Total Number of Claims}} = \\frac{3}{3} = 1.0 \\]\n\nThus, the answer is completely faithful to the context provided, as all claims are fully supported.\n\nFinal output:\n```\nSCORE: 1.0\n```",
              "Answer Relevancy": 1,
              "Context Precision": "0.7",
              "Context Relevancy": 1,
              "Answer Correctness": "0.17",
              "Chunk Overlap": false
            }
          ],
          "dtype": "KeyValue_Mixed"
        }
      },
      {
        "llm": "gpt-4o-mini",
        "metavars": {
          "LLM_0": "gpt-4o-mini",
          "answer_context": 13,
          "baseMethod": 34,
          "chunkMethod": 26,
          "method": 36,
          "reference": 12,
          "signature": 53,
          "source_doc": 15
        },
        "prompt": 200,
        "responses": [
          201
        ],
        "uid": "f160b04e-581e-403b-8ab6-0eedd5ba80c1",
        "vars": {
          "baseMethod": 34,
          "chunkLibrary": 4,
          "chunkMethod": 26,
          "context": 57,
          "methodId": 92,
          "query": 9,
          "retrievalMethod": 36
        },
        "eval_res": {
          "items": [
            {
              "Faithfulness": "0.75",
              "Answer Relevancy": 1,
              "Context Precision": "To evaluate the precision of the retrieved context relative to the question, let's go through the process step by step.\n\n1. **Identify Used Parts of Context**: \n   The answer specifies that the default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst2-english.\" The context explicitly mentions this information when it states: \"here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english.\" This specific sentence directly answers the question.\n\n2. **Calculate Information Density and Utilization**: \n   - The part of the context that contains relevant information used to generate the answer consists of the specific sentence regarding the checkpoint.\n   - The context provided is long and includes a lot of information about tokenization and other aspects of the Transformers library pipeline that do not relate to the question. \n\n   Out of the entire provided context, only a small section (the one identifying the default checkpoint) is directly relevant to answering the question. The remainder of the context provides additional background information that is not necessary for answering the question about the default checkpoint. \n\n   If we evaluate the length:\n   - Relevant portion: about one sentence.\n   - Total context: multiple paragraphs (let's assume 7-10 sentences).\n  \n   Considering this ratio, the relevant information is approximately 1 out of a larger number of sentences.\n\n3. **Information Density & Utilization**: \n   - **Information Density**: Only a small fraction of the text is essential for answering the question. Therefore, the density of relevant information is low.\n   - **Information Utilization**: The relevant sentence was explicitly used in generating the answer, so utilization is high for that specific information.\n\nBased on this analysis:\n- The context includes one crucial piece of relevant information but is surrounded by a lot of irrelevant detail. Hence, I would rate this context's precision as below moderate but not entirely useless.\n\n**Final Score**: Given the low density of relevant information relative to the total context but the complete usage of the relevant detail, I would assign a precision score of 0.3.\n\nFinal answer:\n```\nSCORE: 0.3\n```",
              "Context Relevancy": 1,
              "Answer Correctness": "0.75",
              "Chunk Overlap": false
            }
          ],
          "dtype": "KeyValue_Mixed"
        }
      },
      {
        "llm": "gpt-4o-mini",
        "metavars": {
          "LLM_0": "gpt-4o-mini",
          "answer_context": 22,
          "baseMethod": 3,
          "chunkMethod": 26,
          "docTitle": 18,
          "embeddingModel": 7,
          "method": 10,
          "reference": 21,
          "signature": 31,
          "source_doc": 23
        },
        "prompt": 202,
        "responses": [
          203
        ],
        "uid": "d889ada5-20b4-4f5c-bee7-04ed3d0631f7",
        "vars": {
          "baseMethod": 3,
          "chunkLibrary": 4,
          "chunkMethod": 26,
          "context": 91,
          "docTitle": 18,
          "embeddingModel": 7,
          "methodId": 93,
          "query": 19,
          "retrievalMethod": 10
        },
        "eval_res": {
          "items": [
            {
              "Faithfulness": "1.0",
              "Answer Relevancy": 1,
              "Context Precision": "0.5",
              "Context Relevancy": 1,
              "Answer Correctness": "0.5",
              "Chunk Overlap": false
            }
          ],
          "dtype": "KeyValue_Mixed"
        }
      },
      {
        "llm": "gpt-4o-mini",
        "metavars": {
          "LLM_0": "gpt-4o-mini",
          "answer_context": 63,
          "baseMethod": 34,
          "chunkMethod": 26,
          "method": 36,
          "reference": 62,
          "signature": 53,
          "source_doc": 64
        },
        "prompt": 204,
        "responses": [
          205
        ],
        "uid": "a002fd40-1f88-45d2-a290-23c4dcf09fc8",
        "vars": {
          "baseMethod": 34,
          "chunkLibrary": 4,
          "chunkMethod": 26,
          "context": 79,
          "methodId": 92,
          "query": 60,
          "retrievalMethod": 36
        },
        "eval_res": {
          "items": [
            {
              "Faithfulness": "1.0",
              "Answer Relevancy": 1,
              "Context Precision": "0.5",
              "Context Relevancy": 1,
              "Answer Correctness": "0.5",
              "Chunk Overlap": false
            }
          ],
          "dtype": "KeyValue_Mixed"
        }
      },
      {
        "llm": "gpt-4o-mini",
        "metavars": {
          "LLM_0": "gpt-4o-mini",
          "answer_context": 63,
          "baseMethod": 3,
          "chunkMethod": 26,
          "embeddingModel": 7,
          "method": 10,
          "reference": 62,
          "signature": 31,
          "source_doc": 64
        },
        "prompt": 206,
        "responses": [
          207
        ],
        "uid": "4c72e706-4a97-4cbf-ac51-8dd2140e80f9",
        "vars": {
          "baseMethod": 3,
          "chunkLibrary": 4,
          "chunkMethod": 26,
          "context": 88,
          "embeddingModel": 7,
          "methodId": 93,
          "query": 60,
          "retrievalMethod": 10
        },
        "eval_res": {
          "items": [
            {
              "Faithfulness": "1.0",
              "Answer Relevancy": 1,
              "Context Precision": "0.7",
              "Context Relevancy": 1,
              "Answer Correctness": "0.5",
              "Chunk Overlap": false
            }
          ],
          "dtype": "KeyValue_Mixed"
        }
      },
      {
        "llm": "gpt-4o-mini",
        "metavars": {
          "LLM_0": "gpt-4o-mini",
          "answer_context": 49,
          "baseMethod": 3,
          "chunkMethod": 26,
          "embeddingModel": 7,
          "method": 10,
          "reference": 48,
          "signature": 31,
          "source_doc": 50
        },
        "prompt": 208,
        "responses": [
          97
        ],
        "uid": "4bc1d9ca-4e09-44bd-abff-5cfee5b00dee",
        "vars": {
          "baseMethod": 3,
          "chunkLibrary": 4,
          "chunkMethod": 26,
          "context": 85,
          "embeddingModel": 7,
          "methodId": 93,
          "query": 46,
          "retrievalMethod": 10
        },
        "eval_res": {
          "items": [
            {
              "Faithfulness": "0.875",
              "Answer Relevancy": 1,
              "Context Precision": "0.5",
              "Context Relevancy": 1,
              "Answer Correctness": "0.625",
              "Chunk Overlap": false
            }
          ],
          "dtype": "KeyValue_Mixed"
        }
      },
      {
        "llm": "gpt-4o-mini",
        "metavars": {
          "LLM_0": "gpt-4o-mini",
          "answer_context": 49,
          "baseMethod": 34,
          "chunkMethod": 26,
          "method": 36,
          "reference": 48,
          "signature": 53,
          "source_doc": 50
        },
        "prompt": 209,
        "responses": [
          210
        ],
        "uid": "2bb32e10-1e24-4b51-9f87-ed1cccbb90aa",
        "vars": {
          "baseMethod": 34,
          "chunkLibrary": 4,
          "chunkMethod": 26,
          "context": 82,
          "methodId": 92,
          "query": 46,
          "retrievalMethod": 36
        },
        "eval_res": {
          "items": [
            {
              "Faithfulness": "0.875",
              "Answer Relevancy": 1,
              "Context Precision": "1.0",
              "Context Relevancy": 1,
              "Answer Correctness": "0.375",
              "Chunk Overlap": false
            }
          ],
          "dtype": "KeyValue_Mixed"
        }
      },
      {
        "llm": "gpt-4o-mini",
        "metavars": {
          "LLM_0": "gpt-4o-mini",
          "answer_context": 49,
          "baseMethod": 3,
          "chunkMethod": 5,
          "docTitle": 72,
          "embeddingModel": 7,
          "method": 10,
          "reference": 48,
          "signature": 14,
          "source_doc": 50
        },
        "prompt": 211,
        "responses": [
          212
        ],
        "uid": "64bdd041-de19-4e64-a248-368138910621",
        "vars": {
          "baseMethod": 3,
          "chunkLibrary": 4,
          "chunkMethod": 5,
          "context": 73,
          "docTitle": 72,
          "embeddingModel": 7,
          "methodId": 93,
          "query": 46,
          "retrievalMethod": 10
        },
        "eval_res": {
          "items": [
            {
              "Faithfulness": "0.83",
              "Answer Relevancy": 1,
              "Context Precision": "1.0",
              "Context Relevancy": 1,
              "Answer Correctness": "To evaluate the correctness of the generated answer against the reference answer, let's break down the claims made in both texts.\n\n1. **Claim: \"The purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of a set of guidelines.\"**\n   - This phrase fails to align fully with the reference answer, which specifically mentions a \"set of hard-coded harmful concepts\" rather than \"a set of guidelines.\" The specific mention of \"harmful concepts\" is a crucial aspect of the Safety Checker's function, and using the term \"guidelines\" is less specific and potentially misleading.\n   - **Score: 0.5 (Partially supported with some extrapolation).**\n\n2. **Claim: \"It aims to mitigate issues associated with models like Stable Diffusion.\"**\n   - While the reference indicates the purpose is to \"mitigate the risk of generating harmful content,\" this claim about mitigating \"issues associated with models like Stable Diffusion\" is vague and does not directly correspond to the specific goal outlined in the reference answer about harmful content.\n   - **Score: 0 (Not supported or contradicted by reference).**\n\n3. **Claim: \"This contributes to the ethical development and use of the library.\"**\n   - This claim introduces a new concept (ethical development) that is not covered or supported by the reference answer. The original focus was specifically on content safety rather than the broader ethical implications, making this an unsupported addition.\n   - **Score: 0 (Not supported or contradicted by reference).**\n\nSummarizing the assessments leads to the following scores:\n- First claim: 0.5\n- Second claim: 0\n- Third claim: 0\n\nTotal points accumulated: \\(0.5 + 0 + 0 = 0.5\\).\n\nSince all claims were assessed, and given the total score of 0.5 out of a possible 3 points, the correctness score reflects how many claims were supported compared to the potential maximum.\n\nThus, the final answer correctness score is:\n```\nSCORE: 0.17\n```",
              "Chunk Overlap": true
            }
          ],
          "dtype": "KeyValue_Mixed"
        }
      }
    ],
    "upload-e9d11bb2-f4f5-43a2-8d28-4c54f345659e.json": {},
    "table-ab424041-6fa2-46b0-9121-517805638f94.json": {},
    "promptNode-d95a71d4-5df6-4689-9bb1-f18b2f9049d3_0.json": {
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\nContext: ` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `\n\nboth variants regardless. this change will improve performance for many applications. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5216 ] ( https : / / github. com / gradio - app / gradio / pull / 5216 ) [ ` 4b58ea6d ` ] ( https : / / github. com / gradio - app / gradio / commit / 4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8 ) - update i18n tokens and locale files. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )! # # 0. 2. 0 # # # features - [ # 5025 ] ( https : / / github. com / gradio -\n\nthe first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\n. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 30,
            "baseMethod": 3,
            "chunkMethod": 26,
            "embeddingModel": 7,
            "method": 10,
            "reference": 29,
            "signature": 31,
            "source_doc": 32
          },
          "prompt": 195,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\nContext: ` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `\n\nboth variants regardless. this change will improve performance for many applications. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5216 ] ( https : / / github. com / gradio - app / gradio / pull / 5216 ) [ ` 4b58ea6d ` ] ( https : / / github. com / gradio - app / gradio / commit / 4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8 ) - update i18n tokens and locale files. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )! # # 0. 2. 0 # # # features - [ # 5025 ] ( https : / / github. com / gradio -\n\nthe first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\n. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            196
          ],
          "uid": "ff70540b-64ca-4b91-969b-1ebc75c328eb",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 28,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 27,
            "retrievalMethod": 10
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\nContext: ` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `\n\nhf _ home ` is not set! this is the default way to configure where [ user - specific non - essential ( cached ) data should be written ] ( https : / / wiki. archlinux. org / title / xdg _ base _ directory ) on linux machines. if ` hf _ home ` is not set, the default home will be ` \" $ xdg _ cache _ home / huggingface \" ` instead of ` \" ~ /. cache / huggingface \" `.\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\n. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size\n\n! - - note that this file is in markdown but contain specific syntax for our doc - builder ( similar to mdx ) that may not be rendered properly in your markdown viewer. - - > # environment variables ` huggingface _ hub ` can be configured using environment variables. if you are unfamiliar with environment variable, here are generic articles about them [ on macos and linux ] ( https : / / linuxize. com / post / how - to - set - and - list - environment - variables - in - linux / ) and on [ windows ] ( https : / / phoenixnap. com / kb / windows - set - environment - variable ). this page will guide you through all environment variables specific to ` huggingface _ hub ` and their meaning. # # generic # # # hf _ inference _ endpoint to configure the inference api base url. you might want to set this variable if your organization is pointing at an api gateway rather\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 30,
            "baseMethod": 34,
            "chunkMethod": 26,
            "method": 36,
            "reference": 29,
            "signature": 53,
            "source_doc": 32
          },
          "prompt": 192,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\nContext: ` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `\n\nhf _ home ` is not set! this is the default way to configure where [ user - specific non - essential ( cached ) data should be written ] ( https : / / wiki. archlinux. org / title / xdg _ base _ directory ) on linux machines. if ` hf _ home ` is not set, the default home will be ` \" $ xdg _ cache _ home / huggingface \" ` instead of ` \" ~ /. cache / huggingface \" `.\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\n. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size\n\n! - - note that this file is in markdown but contain specific syntax for our doc - builder ( similar to mdx ) that may not be rendered properly in your markdown viewer. - - > # environment variables ` huggingface _ hub ` can be configured using environment variables. if you are unfamiliar with environment variable, here are generic articles about them [ on macos and linux ] ( https : / / linuxize. com / post / how - to - set - and - list - environment - variables - in - linux / ) and on [ windows ] ( https : / / phoenixnap. com / kb / windows - set - environment - variable ). this page will guide you through all environment variables specific to ` huggingface _ hub ` and their meaning. # # generic # # # hf _ inference _ endpoint to configure the inference api base url. you might want to set this variable if your organization is pointing at an api gateway rather\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            25
          ],
          "uid": "7fc6868b-42f7-4ed7-8ddb-bb8aa6f0562d",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 52,
            "methodId": 92,
            "query": 27,
            "retrievalMethod": 36
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\nContext: `tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n\n-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\non linux machines. \r\n\r\nIf `HF_HOME` is not set, the default home will be `\"$XDG_CACHE_HOME/huggingface\"` instead\r\nof `\"~/.cache/huggingface\"`.\n\n[on macOS and Linux](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/)\r\nand on [Windows](https://phoenixnap.com/kb/windows-set-environment-variable).\n\nattention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 30,
            "baseMethod": 34,
            "chunkMethod": 5,
            "method": 36,
            "reference": 29,
            "signature": 38,
            "source_doc": 32
          },
          "prompt": 186,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\nContext: `tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n\n-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\non linux machines. \r\n\r\nIf `HF_HOME` is not set, the default home will be `\"$XDG_CACHE_HOME/huggingface\"` instead\r\nof `\"~/.cache/huggingface\"`.\n\n[on macOS and Linux](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/)\r\nand on [Windows](https://phoenixnap.com/kb/windows-set-environment-variable).\n\nattention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            40
          ],
          "uid": "41ea9c17-ae54-417a-90b3-eef3320edcf8",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 41,
            "methodId": 92,
            "query": 27,
            "retrievalMethod": 36
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\nContext: `tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n\nlibrary, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or\n\ntokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning\n\nits unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,\n\nNow let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 30,
            "baseMethod": 3,
            "chunkMethod": 5,
            "embeddingModel": 7,
            "method": 10,
            "reference": 29,
            "signature": 14,
            "source_doc": 32
          },
          "prompt": 190,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\nContext: `tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n\nlibrary, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or\n\ntokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning\n\nits unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,\n\nNow let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            191
          ],
          "uid": "b1a55fe8-d756-4029-a44a-6d1a60268943",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 69,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 27,
            "retrievalMethod": 10
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\nContext: the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nhat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the\n\nlibrary. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in\n\n### HF_HUB_DISABLE_TELEMETRY\r\n\r\nBy default, some data is collected by HF libraries (`transformers`, `datasets`, `gradio`,..) to monitor usage, debug issues and help prioritize features.\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 13,
            "baseMethod": 34,
            "chunkMethod": 5,
            "method": 36,
            "reference": 12,
            "signature": 38,
            "source_doc": 15
          },
          "prompt": 183,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\nContext: the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nhat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the\n\nlibrary. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in\n\n### HF_HUB_DISABLE_TELEMETRY\r\n\r\nBy default, some data is collected by HF libraries (`transformers`, `datasets`, `gradio`,..) to monitor usage, debug issues and help prioritize features.\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            2
          ],
          "uid": "07647594-a268-4772-8358-783edb8c47e2",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 43,
            "methodId": 92,
            "query": 9,
            "retrievalMethod": 36
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\nContext: the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nlibrary. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in\n\nsteps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage,\n\nits unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 13,
            "baseMethod": 3,
            "chunkMethod": 5,
            "docTitle": 6,
            "embeddingModel": 7,
            "method": 10,
            "reference": 12,
            "signature": 14,
            "source_doc": 15
          },
          "prompt": 189,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\nContext: the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nlibrary. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in\n\nsteps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage,\n\nits unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            2
          ],
          "uid": "e5b2b334-688f-48a3-b9ba-22e095010318",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 11,
            "docTitle": 6,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 9,
            "retrievalMethod": 10
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\nContext: the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nto a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all\n\n##try by default, some data is collected by hf libraries ( ` transformers `, ` datasets `, ` gradio `,.. ) to monitor usage, debug issues and help prioritize features. each library defines its own policy ( i. e. which usage to monitor ) but the core implementation happens in ` huggingface _ hub ` ( see [ ` send _ telemetry ` ] ). you can set ` hf _ hub _ disable _ telemetry = 1 ` as environment variable to globally disable telemetry. # # # hf _ hub _ enable _ hf _ transfer set to ` true ` for faster uploads and downloads from the hub using ` hf _ transfer `. by default, ` huggingface _ hub ` uses the python - based ` requests. get ` and ` requests. post ` functions. although these are reliable and versatile, they may not be the most efficient choice for machines with high\n\n. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs.\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 13,
            "baseMethod": 34,
            "chunkMethod": 26,
            "method": 36,
            "reference": 12,
            "signature": 53,
            "source_doc": 15
          },
          "prompt": 200,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\nContext: the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nto a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all\n\n##try by default, some data is collected by hf libraries ( ` transformers `, ` datasets `, ` gradio `,.. ) to monitor usage, debug issues and help prioritize features. each library defines its own policy ( i. e. which usage to monitor ) but the core implementation happens in ` huggingface _ hub ` ( see [ ` send _ telemetry ` ] ). you can set ` hf _ hub _ disable _ telemetry = 1 ` as environment variable to globally disable telemetry. # # # hf _ hub _ enable _ hf _ transfer set to ` true ` for faster uploads and downloads from the hub using ` hf _ transfer `. by default, ` huggingface _ hub ` uses the python - based ` requests. get ` and ` requests. post ` functions. although these are reliable and versatile, they may not be the most efficient choice for machines with high\n\n. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs.\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            201
          ],
          "uid": "f160b04e-581e-403b-8ab6-0eedd5ba80c1",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 57,
            "methodId": 92,
            "query": 9,
            "retrievalMethod": 36
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\nContext: the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nto a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all\n\nremoved. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. to get an output linked to our classification problem, we need to use the automodelforsequenceclassification class. it works exactly as the automodel class, except that it will build a model with a classification head. there is one auto class for each common nlp task in the transformers library. here, after giving our model the two sentences, we get a tensor of size two by two : one result for each sentence and for each possible label. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third\n\n. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs.\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 13,
            "baseMethod": 3,
            "chunkMethod": 26,
            "docTitle": 6,
            "embeddingModel": 7,
            "method": 10,
            "reference": 12,
            "signature": 31,
            "source_doc": 15
          },
          "prompt": 197,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\nContext: the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nto a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all\n\nremoved. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. to get an output linked to our classification problem, we need to use the automodelforsequenceclassification class. it works exactly as the automodel class, except that it will build a model with a classification head. there is one auto class for each common nlp task in the transformers library. here, after giving our model the two sentences, we get a tensor of size two by two : one result for each sentence and for each possible label. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third\n\n. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs.\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            2
          ],
          "uid": "a7cfeb0a-d170-4fb2-80da-3e7561fd2e9b",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 55,
            "docTitle": 6,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 9,
            "retrievalMethod": 10
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\nContext: Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional\n\nattention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should\n\nclass. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 22,
            "baseMethod": 34,
            "chunkMethod": 5,
            "method": 36,
            "reference": 21,
            "signature": 38,
            "source_doc": 23
          },
          "prompt": 182,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\nContext: Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional\n\nattention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should\n\nclass. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            17
          ],
          "uid": "fac8100e-19a6-4045-b612-2587c5fde08b",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 37,
            "methodId": 92,
            "query": 19,
            "retrievalMethod": 36
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\nContext: Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do\n\n```py\r\n>>> from transformers.modeling_utils import load_sharded_checkpoint\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n\nfor a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 22,
            "baseMethod": 3,
            "chunkMethod": 5,
            "docTitle": 18,
            "embeddingModel": 7,
            "method": 10,
            "reference": 21,
            "signature": 14,
            "source_doc": 23
          },
          "prompt": 184,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\nContext: Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do\n\n```py\r\n>>> from transformers.modeling_utils import load_sharded_checkpoint\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n\nfor a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            17
          ],
          "uid": "2621bb44-b73e-40af-a553-3768d82cfc9a",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 20,
            "docTitle": 18,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 19,
            "retrievalMethod": 10
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\nContext: slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~\n\ntraining, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\n... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )\n\nsize of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] { ' total _ size ' : 433245184 } ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] { ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 22,
            "baseMethod": 3,
            "chunkMethod": 26,
            "docTitle": 18,
            "embeddingModel": 7,
            "method": 10,
            "reference": 21,
            "signature": 31,
            "source_doc": 23
          },
          "prompt": 202,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\nContext: slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~\n\ntraining, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\n... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )\n\nsize of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] { ' total _ size ' : 433245184 } ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] { ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            203
          ],
          "uid": "d889ada5-20b4-4f5c-bee7-04ed3d0631f7",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 91,
            "docTitle": 18,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 19,
            "retrievalMethod": 10
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\nContext: slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\ntraining, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when\n\nsize of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] { ' total _ size ' : 433245184 } ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] { ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre\n\n... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 22,
            "baseMethod": 34,
            "chunkMethod": 26,
            "docTitle": 18,
            "method": 36,
            "reference": 21,
            "signature": 53,
            "source_doc": 23
          },
          "prompt": 198,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\nContext: slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\ntraining, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when\n\nsize of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] { ' total _ size ' : 433245184 } ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] { ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre\n\n... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            199
          ],
          "uid": "c1255722-4c83-4cf2-9435-0f930f332397",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 76,
            "docTitle": 18,
            "methodId": 92,
            "query": 19,
            "retrievalMethod": 36
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\nContext: - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of\n\nguidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\n-->\r\n\r\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\r\n\r\n## Preamble\n\n- [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 49,
            "baseMethod": 3,
            "chunkMethod": 5,
            "docTitle": 72,
            "embeddingModel": 7,
            "method": 10,
            "reference": 48,
            "signature": 14,
            "source_doc": 50
          },
          "prompt": 211,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\nContext: - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of\n\nguidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\n-->\r\n\r\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\r\n\r\n## Preamble\n\n- [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            212
          ],
          "uid": "64bdd041-de19-4e64-a248-368138910621",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 73,
            "docTitle": 72,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 46,
            "retrievalMethod": 10
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\nContext: - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of\n\nguidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nðŸ§¨ Diffusers Experimental\r\n\r\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\r\nCurrently, the following experiments are supported:\n\nhat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the\n\n- **Encouraging safety in deployment**\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 49,
            "baseMethod": 34,
            "chunkMethod": 5,
            "method": 36,
            "reference": 48,
            "signature": 38,
            "source_doc": 50
          },
          "prompt": 185,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\nContext: - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of\n\nguidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nðŸ§¨ Diffusers Experimental\r\n\r\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\r\nCurrently, the following experiments are supported:\n\nhat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the\n\n- **Encouraging safety in deployment**\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            45
          ],
          "uid": "1846f4c6-4ed0-4505-9e24-002214fe6ed5",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 47,
            "methodId": 92,
            "query": 46,
            "retrievalMethod": 36
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\nContext: diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https\n\nhttps : / / huggingface. co / spaces / society - ethics / diffusionbiasexplorer ) to demonstrate the biases in stable diffusion interactively. in this sense, we support and encourage bias explorers and evaluations. - * * encouraging safety in deployment * * - [ * * safe stable diffusion * * ] ( https : / / huggingface. co / docs / diffusers / main / en / api / pipelines / stable _ diffusion / stable _ diffusion _ safe ) : it mitigates the well - known issue that models, like stable diffusion, that are trained on unfiltered, web - crawled datasets tend to suffer from inappropriate degeneration. related paper : [ safe latent diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / gi\n\nsame level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and\n\ndiffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 49,
            "baseMethod": 34,
            "chunkMethod": 26,
            "method": 36,
            "reference": 48,
            "signature": 53,
            "source_doc": 50
          },
          "prompt": 209,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\nContext: diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https\n\nhttps : / / huggingface. co / spaces / society - ethics / diffusionbiasexplorer ) to demonstrate the biases in stable diffusion interactively. in this sense, we support and encourage bias explorers and evaluations. - * * encouraging safety in deployment * * - [ * * safe stable diffusion * * ] ( https : / / huggingface. co / docs / diffusers / main / en / api / pipelines / stable _ diffusion / stable _ diffusion _ safe ) : it mitigates the well - known issue that models, like stable diffusion, that are trained on unfiltered, web - crawled datasets tend to suffer from inappropriate degeneration. related paper : [ safe latent diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / gi\n\nsame level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and\n\ndiffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            210
          ],
          "uid": "2bb32e10-1e24-4b51-9f87-ed1cccbb90aa",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 82,
            "methodId": 92,
            "query": 46,
            "retrievalMethod": 36
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\nContext: diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https\n\nsame level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and\n\n' s responsiveness and valuable feedback. # # scope the diffusers community will apply the following ethical guidelines to the project â€™ s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns. # # ethical guidelines the following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question. - * * transparency * * : we are committed to being transparent in managing prs, explaining our choices to users, and making technical decisions. - * * consistency * * : we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # diffusers â€™ ethical guidelines # # preamble [ diffusers ] ( https : / / huggingface. co / docs / diffusers / index ) provides pre - trained diffusion models and serves as a modular toolbox for inference and training. given its real case applications in the world and potential negative impacts on society, we think it\n\ndiffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model.\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 49,
            "baseMethod": 3,
            "chunkMethod": 26,
            "embeddingModel": 7,
            "method": 10,
            "reference": 48,
            "signature": 31,
            "source_doc": 50
          },
          "prompt": 208,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\nContext: diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https\n\nsame level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and\n\n' s responsiveness and valuable feedback. # # scope the diffusers community will apply the following ethical guidelines to the project â€™ s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns. # # ethical guidelines the following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question. - * * transparency * * : we are committed to being transparent in managing prs, explaining our choices to users, and making technical decisions. - * * consistency * * : we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # diffusers â€™ ethical guidelines # # preamble [ diffusers ] ( https : / / huggingface. co / docs / diffusers / index ) provides pre - trained diffusion models and serves as a modular toolbox for inference and training. given its real case applications in the world and potential negative impacts on society, we think it\n\ndiffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model.\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            97
          ],
          "uid": "4bc1d9ca-4e09-44bd-abff-5cfee5b00dee",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 85,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 46,
            "retrievalMethod": 10
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\nContext: ! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\nto load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,\n\nautodoc ] ] autopipelineforimage2image - all - from _ pretrained - from _ pipe # # autopipelineforinpainting [ [ autodoc ] ] autopipelineforinpainting - all - from _ pretrained - from _ pipe\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\n. bin ', ' pytorch _ model - 00002 - of - 00003. bin ', ' pytorch _ model - 00003 - of - 00003. bin ', ' pytorch _ model. bin. index. json ' ] ` ` ` on top of the configuration of the model, we see three different weights files, and an ` index. json ` file which is our index. a checkpoint like this can be fully reloaded using the [ ` ~ pretrainedmodel. from _ pretrained ` ] method : ` ` ` py > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 63,
            "baseMethod": 3,
            "chunkMethod": 26,
            "embeddingModel": 7,
            "method": 10,
            "reference": 62,
            "signature": 31,
            "source_doc": 64
          },
          "prompt": 206,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\nContext: ! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\nto load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,\n\nautodoc ] ] autopipelineforimage2image - all - from _ pretrained - from _ pipe # # autopipelineforinpainting [ [ autodoc ] ] autopipelineforinpainting - all - from _ pretrained - from _ pipe\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\n. bin ', ' pytorch _ model - 00002 - of - 00003. bin ', ' pytorch _ model - 00003 - of - 00003. bin ', ' pytorch _ model. bin. index. json ' ] ` ` ` on top of the configuration of the model, we see three different weights files, and an ` index. json ` file which is our index. a checkpoint like this can be fully reloaded using the [ ` ~ pretrainedmodel. from _ pretrained ` ] method : ` ` ` py > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            207
          ],
          "uid": "4c72e706-4a97-4cbf-ac51-8dd2140e80f9",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 88,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 60,
            "retrievalMethod": 10
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\nContext: -->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nfor a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n- from_pipe\r\n\r\n## AutoPipelineForInpainting\r\n\r\n[[autodoc]] AutoPipelineForInpainting\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 63,
            "baseMethod": 3,
            "chunkMethod": 5,
            "embeddingModel": 7,
            "method": 10,
            "reference": 62,
            "signature": 14,
            "source_doc": 64
          },
          "prompt": 193,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\nContext: -->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nfor a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n- from_pipe\r\n\r\n## AutoPipelineForInpainting\r\n\r\n[[autodoc]] AutoPipelineForInpainting\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            194
          ],
          "uid": "fc04cf70-1464-499a-8a70-99c4077b8ec9",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 67,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 60,
            "retrievalMethod": 10
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\nContext: -->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should\n\nthe checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 63,
            "baseMethod": 34,
            "chunkMethod": 5,
            "method": 36,
            "reference": 62,
            "signature": 38,
            "source_doc": 64
          },
          "prompt": 187,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\nContext: -->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should\n\nthe checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            188
          ],
          "uid": "8442ebb0-fb6f-4b9f-ae0d-3bf0077e460f",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 61,
            "methodId": 92,
            "query": 60,
            "retrievalMethod": 36
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\nContext: to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\nthe first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\n:... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir ) ` ` ` the main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in ram to the model size plus the size of the biggest shard. behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. we can load that index like any json and get a dictionary : ` ` ` py > > > import json > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ sha\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 63,
            "baseMethod": 34,
            "chunkMethod": 26,
            "method": 36,
            "reference": 62,
            "signature": 53,
            "source_doc": 64
          },
          "prompt": 204,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\nContext: to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\nthe first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\n:... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir ) ` ` ` the main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in ram to the model size plus the size of the biggest shard. behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. we can load that index like any json and get a dictionary : ` ` ` py > > > import json > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ sha\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            205
          ],
          "uid": "a002fd40-1f88-45d2-a290-23c4dcf09fc8",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 79,
            "methodId": 92,
            "query": 60,
            "retrievalMethod": 36
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\nContext: ` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `\n\nboth variants regardless. this change will improve performance for many applications. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5216 ] ( https : / / github. com / gradio - app / gradio / pull / 5216 ) [ ` 4b58ea6d ` ] ( https : / / github. com / gradio - app / gradio / commit / 4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8 ) - update i18n tokens and locale files. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )! # # 0. 2. 0 # # # features - [ # 5025 ] ( https : / / github. com / gradio -\n\nthe first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\n. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer": 29,
            "baseMethod": 3,
            "chunkMethod": 26,
            "context": 30,
            "embeddingModel": 7,
            "method": 10,
            "signature": 31,
            "source_doc": 32
          },
          "prompt": 24,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\nContext: ` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `\n\nboth variants regardless. this change will improve performance for many applications. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5216 ] ( https : / / github. com / gradio - app / gradio / pull / 5216 ) [ ` 4b58ea6d ` ] ( https : / / github. com / gradio - app / gradio / commit / 4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8 ) - update i18n tokens and locale files. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )! # # 0. 2. 0 # # # features - [ # 5025 ] ( https : / / github. com / gradio -\n\nthe first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\n. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            25
          ],
          "uid": "4a058f04-5c06-4857-89a0-f3ddc7af9a5e",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 28,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 27,
            "retrievalMethod": 10
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\nContext: ` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `\n\nhf _ home ` is not set! this is the default way to configure where [ user - specific non - essential ( cached ) data should be written ] ( https : / / wiki. archlinux. org / title / xdg _ base _ directory ) on linux machines. if ` hf _ home ` is not set, the default home will be ` \" $ xdg _ cache _ home / huggingface \" ` instead of ` \" ~ /. cache / huggingface \" `.\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\n. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size\n\n! - - note that this file is in markdown but contain specific syntax for our doc - builder ( similar to mdx ) that may not be rendered properly in your markdown viewer. - - > # environment variables ` huggingface _ hub ` can be configured using environment variables. if you are unfamiliar with environment variable, here are generic articles about them [ on macos and linux ] ( https : / / linuxize. com / post / how - to - set - and - list - environment - variables - in - linux / ) and on [ windows ] ( https : / / phoenixnap. com / kb / windows - set - environment - variable ). this page will guide you through all environment variables specific to ` huggingface _ hub ` and their meaning. # # generic # # # hf _ inference _ endpoint to configure the inference api base url. you might want to set this variable if your organization is pointing at an api gateway rather\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer": 29,
            "baseMethod": 34,
            "chunkMethod": 26,
            "context": 30,
            "method": 36,
            "signature": 53,
            "source_doc": 32
          },
          "prompt": 51,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\nContext: ` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `\n\nhf _ home ` is not set! this is the default way to configure where [ user - specific non - essential ( cached ) data should be written ] ( https : / / wiki. archlinux. org / title / xdg _ base _ directory ) on linux machines. if ` hf _ home ` is not set, the default home will be ` \" $ xdg _ cache _ home / huggingface \" ` instead of ` \" ~ /. cache / huggingface \" `.\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\n. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size\n\n! - - note that this file is in markdown but contain specific syntax for our doc - builder ( similar to mdx ) that may not be rendered properly in your markdown viewer. - - > # environment variables ` huggingface _ hub ` can be configured using environment variables. if you are unfamiliar with environment variable, here are generic articles about them [ on macos and linux ] ( https : / / linuxize. com / post / how - to - set - and - list - environment - variables - in - linux / ) and on [ windows ] ( https : / / phoenixnap. com / kb / windows - set - environment - variable ). this page will guide you through all environment variables specific to ` huggingface _ hub ` and their meaning. # # generic # # # hf _ inference _ endpoint to configure the inference api base url. you might want to set this variable if your organization is pointing at an api gateway rather\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            25
          ],
          "uid": "26d46a1b-af10-4d06-b5dd-5b7cb94aae31",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 52,
            "methodId": 92,
            "query": 27,
            "retrievalMethod": 36
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\nContext: `tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n\n-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\non linux machines. \r\n\r\nIf `HF_HOME` is not set, the default home will be `\"$XDG_CACHE_HOME/huggingface\"` instead\r\nof `\"~/.cache/huggingface\"`.\n\n[on macOS and Linux](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/)\r\nand on [Windows](https://phoenixnap.com/kb/windows-set-environment-variable).\n\nattention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer": 29,
            "baseMethod": 34,
            "chunkMethod": 5,
            "context": 30,
            "method": 36,
            "signature": 38,
            "source_doc": 32
          },
          "prompt": 39,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\nContext: `tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n\n-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\non linux machines. \r\n\r\nIf `HF_HOME` is not set, the default home will be `\"$XDG_CACHE_HOME/huggingface\"` instead\r\nof `\"~/.cache/huggingface\"`.\n\n[on macOS and Linux](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/)\r\nand on [Windows](https://phoenixnap.com/kb/windows-set-environment-variable).\n\nattention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            40
          ],
          "uid": "5cb68a4c-1da2-44b6-b322-c1b80bfd8eda",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 41,
            "methodId": 92,
            "query": 27,
            "retrievalMethod": 36
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\nContext: `tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n\nlibrary, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or\n\ntokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning\n\nits unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,\n\nNow let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer": 29,
            "baseMethod": 3,
            "chunkMethod": 5,
            "context": 30,
            "embeddingModel": 7,
            "method": 10,
            "signature": 14,
            "source_doc": 32
          },
          "prompt": 68,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\nContext: `tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n\nlibrary, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or\n\ntokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning\n\nits unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,\n\nNow let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            40
          ],
          "uid": "474520e8-334f-4b8f-8616-a1a0667d4ce8",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 69,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 27,
            "retrievalMethod": 10
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\nContext: the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nhat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the\n\nlibrary. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in\n\n### HF_HUB_DISABLE_TELEMETRY\r\n\r\nBy default, some data is collected by HF libraries (`transformers`, `datasets`, `gradio`,..) to monitor usage, debug issues and help prioritize features.\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer": 12,
            "baseMethod": 34,
            "chunkMethod": 5,
            "context": 13,
            "method": 36,
            "signature": 38,
            "source_doc": 15
          },
          "prompt": 42,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\nContext: the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nhat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the\n\nlibrary. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in\n\n### HF_HUB_DISABLE_TELEMETRY\r\n\r\nBy default, some data is collected by HF libraries (`transformers`, `datasets`, `gradio`,..) to monitor usage, debug issues and help prioritize features.\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            2
          ],
          "uid": "72348fa4-9ba4-4266-b97b-d66fd229108c",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 43,
            "methodId": 92,
            "query": 9,
            "retrievalMethod": 36
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\nContext: the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nlibrary. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in\n\nsteps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage,\n\nits unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer": 12,
            "baseMethod": 3,
            "chunkMethod": 5,
            "context": 13,
            "docTitle": 6,
            "embeddingModel": 7,
            "method": 10,
            "signature": 14,
            "source_doc": 15
          },
          "prompt": 0,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\nContext: the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nlibrary. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in\n\nsteps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage,\n\nits unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            2
          ],
          "uid": "df988bd8-3f43-4057-8f02-d40db061b111",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 11,
            "docTitle": 6,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 9,
            "retrievalMethod": 10
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\nContext: the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nto a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all\n\n##try by default, some data is collected by hf libraries ( ` transformers `, ` datasets `, ` gradio `,.. ) to monitor usage, debug issues and help prioritize features. each library defines its own policy ( i. e. which usage to monitor ) but the core implementation happens in ` huggingface _ hub ` ( see [ ` send _ telemetry ` ] ). you can set ` hf _ hub _ disable _ telemetry = 1 ` as environment variable to globally disable telemetry. # # # hf _ hub _ enable _ hf _ transfer set to ` true ` for faster uploads and downloads from the hub using ` hf _ transfer `. by default, ` huggingface _ hub ` uses the python - based ` requests. get ` and ` requests. post ` functions. although these are reliable and versatile, they may not be the most efficient choice for machines with high\n\n. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs.\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer": 12,
            "baseMethod": 34,
            "chunkMethod": 26,
            "context": 13,
            "method": 36,
            "signature": 53,
            "source_doc": 15
          },
          "prompt": 56,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\nContext: the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nto a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all\n\n##try by default, some data is collected by hf libraries ( ` transformers `, ` datasets `, ` gradio `,.. ) to monitor usage, debug issues and help prioritize features. each library defines its own policy ( i. e. which usage to monitor ) but the core implementation happens in ` huggingface _ hub ` ( see [ ` send _ telemetry ` ] ). you can set ` hf _ hub _ disable _ telemetry = 1 ` as environment variable to globally disable telemetry. # # # hf _ hub _ enable _ hf _ transfer set to ` true ` for faster uploads and downloads from the hub using ` hf _ transfer `. by default, ` huggingface _ hub ` uses the python - based ` requests. get ` and ` requests. post ` functions. although these are reliable and versatile, they may not be the most efficient choice for machines with high\n\n. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs.\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            2
          ],
          "uid": "bcf41709-f649-423f-a1d8-9890eed66149",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 57,
            "methodId": 92,
            "query": 9,
            "retrievalMethod": 36
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\nContext: the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nto a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all\n\nremoved. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. to get an output linked to our classification problem, we need to use the automodelforsequenceclassification class. it works exactly as the automodel class, except that it will build a model with a classification head. there is one auto class for each common nlp task in the transformers library. here, after giving our model the two sentences, we get a tensor of size two by two : one result for each sentence and for each possible label. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third\n\n. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs.\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer": 12,
            "baseMethod": 3,
            "chunkMethod": 26,
            "context": 13,
            "docTitle": 6,
            "embeddingModel": 7,
            "method": 10,
            "signature": 31,
            "source_doc": 15
          },
          "prompt": 54,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\nContext: the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nto a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all\n\nremoved. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. to get an output linked to our classification problem, we need to use the automodelforsequenceclassification class. it works exactly as the automodel class, except that it will build a model with a classification head. there is one auto class for each common nlp task in the transformers library. here, after giving our model the two sentences, we get a tensor of size two by two : one result for each sentence and for each possible label. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third\n\n. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs.\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            2
          ],
          "uid": "045df5bf-00f8-481f-95b6-e92f286f259d",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 55,
            "docTitle": 6,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 9,
            "retrievalMethod": 10
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\nContext: Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional\n\nattention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should\n\nclass. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer": 21,
            "baseMethod": 34,
            "chunkMethod": 5,
            "context": 22,
            "method": 36,
            "signature": 38,
            "source_doc": 23
          },
          "prompt": 33,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\nContext: Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional\n\nattention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should\n\nclass. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            17
          ],
          "uid": "ff5e6448-35dd-40e5-90e1-7b4de78c0b60",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 37,
            "methodId": 92,
            "query": 19,
            "retrievalMethod": 36
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\nContext: Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do\n\n```py\r\n>>> from transformers.modeling_utils import load_sharded_checkpoint\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n\nfor a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer": 21,
            "baseMethod": 3,
            "chunkMethod": 5,
            "context": 22,
            "docTitle": 18,
            "embeddingModel": 7,
            "method": 10,
            "signature": 14,
            "source_doc": 23
          },
          "prompt": 16,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\nContext: Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do\n\n```py\r\n>>> from transformers.modeling_utils import load_sharded_checkpoint\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n\nfor a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            17
          ],
          "uid": "88e05265-88b5-4c0a-88e6-dd0edae1cb7a",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 20,
            "docTitle": 18,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 19,
            "retrievalMethod": 10
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\nContext: slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~\n\ntraining, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\n... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )\n\nsize of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] { ' total _ size ' : 433245184 } ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] { ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer": 21,
            "baseMethod": 3,
            "chunkMethod": 26,
            "context": 22,
            "docTitle": 18,
            "embeddingModel": 7,
            "method": 10,
            "signature": 31,
            "source_doc": 23
          },
          "prompt": 89,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\nContext: slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~\n\ntraining, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\n... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )\n\nsize of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] { ' total _ size ' : 433245184 } ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] { ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            90
          ],
          "uid": "fa999fde-bec6-4182-ad32-4d2e0232f946",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 91,
            "docTitle": 18,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 19,
            "retrievalMethod": 10
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\nContext: slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\ntraining, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when\n\nsize of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] { ' total _ size ' : 433245184 } ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] { ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre\n\n... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer": 21,
            "baseMethod": 34,
            "chunkMethod": 26,
            "context": 22,
            "docTitle": 18,
            "method": 36,
            "signature": 53,
            "source_doc": 23
          },
          "prompt": 74,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\nContext: slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\ntraining, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when\n\nsize of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] { ' total _ size ' : 433245184 } ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] { ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre\n\n... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            90
          ],
          "uid": "498ac380-6972-4c7e-bdf8-e24733f82354",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 76,
            "docTitle": 18,
            "methodId": 92,
            "query": 19,
            "retrievalMethod": 36
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\nContext: - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of\n\nguidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\n-->\r\n\r\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\r\n\r\n## Preamble\n\n- [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer": 48,
            "baseMethod": 3,
            "chunkMethod": 5,
            "context": 49,
            "docTitle": 72,
            "embeddingModel": 7,
            "method": 10,
            "signature": 14,
            "source_doc": 50
          },
          "prompt": 70,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\nContext: - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of\n\nguidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\n-->\r\n\r\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\r\n\r\n## Preamble\n\n- [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            71
          ],
          "uid": "38939e65-7946-4f33-b8df-f683bc469d09",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 73,
            "docTitle": 72,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 46,
            "retrievalMethod": 10
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\nContext: - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of\n\nguidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nðŸ§¨ Diffusers Experimental\r\n\r\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\r\nCurrently, the following experiments are supported:\n\nhat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the\n\n- **Encouraging safety in deployment**\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer": 48,
            "baseMethod": 34,
            "chunkMethod": 5,
            "context": 49,
            "method": 36,
            "signature": 38,
            "source_doc": 50
          },
          "prompt": 44,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\nContext: - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of\n\nguidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nðŸ§¨ Diffusers Experimental\r\n\r\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\r\nCurrently, the following experiments are supported:\n\nhat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the\n\n- **Encouraging safety in deployment**\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            94
          ],
          "uid": "c90652e3-9a18-4acd-a9c3-8251931c0c1a",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 47,
            "methodId": 92,
            "query": 46,
            "retrievalMethod": 36
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\nContext: diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https\n\nhttps : / / huggingface. co / spaces / society - ethics / diffusionbiasexplorer ) to demonstrate the biases in stable diffusion interactively. in this sense, we support and encourage bias explorers and evaluations. - * * encouraging safety in deployment * * - [ * * safe stable diffusion * * ] ( https : / / huggingface. co / docs / diffusers / main / en / api / pipelines / stable _ diffusion / stable _ diffusion _ safe ) : it mitigates the well - known issue that models, like stable diffusion, that are trained on unfiltered, web - crawled datasets tend to suffer from inappropriate degeneration. related paper : [ safe latent diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / gi\n\nsame level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and\n\ndiffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer": 48,
            "baseMethod": 34,
            "chunkMethod": 26,
            "context": 49,
            "method": 36,
            "signature": 53,
            "source_doc": 50
          },
          "prompt": 80,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\nContext: diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https\n\nhttps : / / huggingface. co / spaces / society - ethics / diffusionbiasexplorer ) to demonstrate the biases in stable diffusion interactively. in this sense, we support and encourage bias explorers and evaluations. - * * encouraging safety in deployment * * - [ * * safe stable diffusion * * ] ( https : / / huggingface. co / docs / diffusers / main / en / api / pipelines / stable _ diffusion / stable _ diffusion _ safe ) : it mitigates the well - known issue that models, like stable diffusion, that are trained on unfiltered, web - crawled datasets tend to suffer from inappropriate degeneration. related paper : [ safe latent diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / gi\n\nsame level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and\n\ndiffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            96
          ],
          "uid": "eafaf64d-e5cb-4a9d-a0ee-87e7cd5fd512",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 82,
            "methodId": 92,
            "query": 46,
            "retrievalMethod": 36
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\nContext: diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https\n\nsame level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and\n\n' s responsiveness and valuable feedback. # # scope the diffusers community will apply the following ethical guidelines to the project â€™ s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns. # # ethical guidelines the following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question. - * * transparency * * : we are committed to being transparent in managing prs, explaining our choices to users, and making technical decisions. - * * consistency * * : we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # diffusers â€™ ethical guidelines # # preamble [ diffusers ] ( https : / / huggingface. co / docs / diffusers / index ) provides pre - trained diffusion models and serves as a modular toolbox for inference and training. given its real case applications in the world and potential negative impacts on society, we think it\n\ndiffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model.\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer": 48,
            "baseMethod": 3,
            "chunkMethod": 26,
            "context": 49,
            "embeddingModel": 7,
            "method": 10,
            "signature": 31,
            "source_doc": 50
          },
          "prompt": 83,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\nContext: diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https\n\nsame level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and\n\n' s responsiveness and valuable feedback. # # scope the diffusers community will apply the following ethical guidelines to the project â€™ s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns. # # ethical guidelines the following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question. - * * transparency * * : we are committed to being transparent in managing prs, explaining our choices to users, and making technical decisions. - * * consistency * * : we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # diffusers â€™ ethical guidelines # # preamble [ diffusers ] ( https : / / huggingface. co / docs / diffusers / index ) provides pre - trained diffusion models and serves as a modular toolbox for inference and training. given its real case applications in the world and potential negative impacts on society, we think it\n\ndiffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model.\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            97
          ],
          "uid": "18dc0895-78cf-4e67-a1e1-6b72723110f2",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 85,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 46,
            "retrievalMethod": 10
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\nContext: ! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\nto load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,\n\nautodoc ] ] autopipelineforimage2image - all - from _ pretrained - from _ pipe # # autopipelineforinpainting [ [ autodoc ] ] autopipelineforinpainting - all - from _ pretrained - from _ pipe\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\n. bin ', ' pytorch _ model - 00002 - of - 00003. bin ', ' pytorch _ model - 00003 - of - 00003. bin ', ' pytorch _ model. bin. index. json ' ] ` ` ` on top of the configuration of the model, we see three different weights files, and an ` index. json ` file which is our index. a checkpoint like this can be fully reloaded using the [ ` ~ pretrainedmodel. from _ pretrained ` ] method : ` ` ` py > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer": 62,
            "baseMethod": 3,
            "chunkMethod": 26,
            "context": 63,
            "embeddingModel": 7,
            "method": 10,
            "signature": 31,
            "source_doc": 64
          },
          "prompt": 86,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\nContext: ! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\nto load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,\n\nautodoc ] ] autopipelineforimage2image - all - from _ pretrained - from _ pipe # # autopipelineforinpainting [ [ autodoc ] ] autopipelineforinpainting - all - from _ pretrained - from _ pipe\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\n. bin ', ' pytorch _ model - 00002 - of - 00003. bin ', ' pytorch _ model - 00003 - of - 00003. bin ', ' pytorch _ model. bin. index. json ' ] ` ` ` on top of the configuration of the model, we see three different weights files, and an ` index. json ` file which is our index. a checkpoint like this can be fully reloaded using the [ ` ~ pretrainedmodel. from _ pretrained ` ] method : ` ` ` py > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            98
          ],
          "uid": "0ab44242-774a-478b-88a4-c6d27e3812f6",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 88,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 60,
            "retrievalMethod": 10
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\nContext: -->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nfor a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n- from_pipe\r\n\r\n## AutoPipelineForInpainting\r\n\r\n[[autodoc]] AutoPipelineForInpainting\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer": 62,
            "baseMethod": 3,
            "chunkMethod": 5,
            "context": 63,
            "embeddingModel": 7,
            "method": 10,
            "signature": 14,
            "source_doc": 64
          },
          "prompt": 65,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\nContext: -->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nfor a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n- from_pipe\r\n\r\n## AutoPipelineForInpainting\r\n\r\n[[autodoc]] AutoPipelineForInpainting\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            95
          ],
          "uid": "2e2c4607-c39f-4106-9224-572cc28bc142",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 67,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 60,
            "retrievalMethod": 10
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\nContext: -->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should\n\nthe checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer": 62,
            "baseMethod": 34,
            "chunkMethod": 5,
            "context": 63,
            "method": 36,
            "signature": 38,
            "source_doc": 64
          },
          "prompt": 58,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\nContext: -->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should\n\nthe checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            59
          ],
          "uid": "f3049ce6-ed64-499a-b8e8-d736acbb05fd",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 61,
            "methodId": 92,
            "query": 60,
            "retrievalMethod": 36
          }
        }
      ],
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\nContext: to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\nthe first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\n:... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir ) ` ` ` the main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in ram to the model size plus the size of the biggest shard. behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. we can load that index like any json and get a dictionary : ` ` ` py > > > import json > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ sha\nAnswer: ": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer": 62,
            "baseMethod": 34,
            "chunkMethod": 26,
            "context": 63,
            "method": 36,
            "signature": 53,
            "source_doc": 64
          },
          "prompt": 77,
          "query": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "messages": [
              {
                "content": "You are a helpful assistant.",
                "role": "system"
              },
              {
                "content": "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\nContext: to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\nthe first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\n:... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir ) ` ` ` the main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in ram to the model size plus the size of the biggest shard. behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. we can load that index like any json and get a dictionary : ` ` ` py > > > import json > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ sha\nAnswer: ",
                "role": "user"
              }
            ],
            "model": 1,
            "n": 1,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "temperature": 0,
            "top_p": 1
          },
          "responses": [
            78
          ],
          "uid": "9d780778-0bf7-4079-9e61-93f35c09dc88",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 79,
            "methodId": 92,
            "query": 60,
            "retrievalMethod": 36
          }
        }
      ]
    },
    "promptNode-d95a71d4-5df6-4689-9bb1-f18b2f9049d3.json": {
      "cache_files": {
        "promptNode-d95a71d4-5df6-4689-9bb1-f18b2f9049d3_0.json": {
          "base_model": "gpt-4",
          "emoji": "ðŸ¤–",
          "formData": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "model": "gpt-4o-mini",
            "parallel_tool_calls": true,
            "presence_penalty": 0,
            "response_format": "text",
            "seed": null,
            "shortname": "gpt-4o-mini",
            "stop": "",
            "system_msg": "You are a helpful assistant.",
            "temperature": 0,
            "tool_choice": "",
            "tools": "",
            "top_p": 1
          },
          "key": "5fce3844-27b9-41f9-9b81-447b08f10b23",
          "model": "gpt-4o-mini",
          "name": "gpt-4o-mini",
          "progress": {
            "error": 0,
            "success": 0
          },
          "settings": {
            "frequency_penalty": 0,
            "logit_bias": null,
            "max_completion_tokens": null,
            "parallel_tool_calls": true,
            "presence_penalty": 0,
            "response_format": {
              "type": "text"
            },
            "seed": null,
            "stop": [],
            "system_msg": "You are a helpful assistant.",
            "temperature": 0,
            "tool_choice": "",
            "tools": [],
            "top_p": 1
          },
          "temp": 1
        }
      },
      "responses_last_run": [
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 22,
            "baseMethod": 34,
            "chunkMethod": 5,
            "method": 36,
            "reference": 21,
            "signature": 38,
            "source_doc": 23
          },
          "prompt": 182,
          "responses": [
            17
          ],
          "uid": "fac8100e-19a6-4045-b612-2587c5fde08b",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 37,
            "methodId": 92,
            "query": 19,
            "retrievalMethod": 36
          }
        },
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 13,
            "baseMethod": 34,
            "chunkMethod": 5,
            "method": 36,
            "reference": 12,
            "signature": 38,
            "source_doc": 15
          },
          "prompt": 183,
          "responses": [
            2
          ],
          "uid": "07647594-a268-4772-8358-783edb8c47e2",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 43,
            "methodId": 92,
            "query": 9,
            "retrievalMethod": 36
          }
        },
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 22,
            "baseMethod": 3,
            "chunkMethod": 5,
            "docTitle": 18,
            "embeddingModel": 7,
            "method": 10,
            "reference": 21,
            "signature": 14,
            "source_doc": 23
          },
          "prompt": 184,
          "responses": [
            17
          ],
          "uid": "2621bb44-b73e-40af-a553-3768d82cfc9a",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 20,
            "docTitle": 18,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 19,
            "retrievalMethod": 10
          }
        },
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 49,
            "baseMethod": 34,
            "chunkMethod": 5,
            "method": 36,
            "reference": 48,
            "signature": 38,
            "source_doc": 50
          },
          "prompt": 185,
          "responses": [
            45
          ],
          "uid": "1846f4c6-4ed0-4505-9e24-002214fe6ed5",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 47,
            "methodId": 92,
            "query": 46,
            "retrievalMethod": 36
          }
        },
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 30,
            "baseMethod": 34,
            "chunkMethod": 5,
            "method": 36,
            "reference": 29,
            "signature": 38,
            "source_doc": 32
          },
          "prompt": 186,
          "responses": [
            40
          ],
          "uid": "41ea9c17-ae54-417a-90b3-eef3320edcf8",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 41,
            "methodId": 92,
            "query": 27,
            "retrievalMethod": 36
          }
        },
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 63,
            "baseMethod": 34,
            "chunkMethod": 5,
            "method": 36,
            "reference": 62,
            "signature": 38,
            "source_doc": 64
          },
          "prompt": 187,
          "responses": [
            188
          ],
          "uid": "8442ebb0-fb6f-4b9f-ae0d-3bf0077e460f",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 61,
            "methodId": 92,
            "query": 60,
            "retrievalMethod": 36
          }
        },
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 13,
            "baseMethod": 3,
            "chunkMethod": 5,
            "docTitle": 6,
            "embeddingModel": 7,
            "method": 10,
            "reference": 12,
            "signature": 14,
            "source_doc": 15
          },
          "prompt": 189,
          "responses": [
            2
          ],
          "uid": "e5b2b334-688f-48a3-b9ba-22e095010318",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 11,
            "docTitle": 6,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 9,
            "retrievalMethod": 10
          }
        },
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 30,
            "baseMethod": 3,
            "chunkMethod": 5,
            "embeddingModel": 7,
            "method": 10,
            "reference": 29,
            "signature": 14,
            "source_doc": 32
          },
          "prompt": 190,
          "responses": [
            191
          ],
          "uid": "b1a55fe8-d756-4029-a44a-6d1a60268943",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 69,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 27,
            "retrievalMethod": 10
          }
        },
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 30,
            "baseMethod": 34,
            "chunkMethod": 26,
            "method": 36,
            "reference": 29,
            "signature": 53,
            "source_doc": 32
          },
          "prompt": 192,
          "responses": [
            25
          ],
          "uid": "7fc6868b-42f7-4ed7-8ddb-bb8aa6f0562d",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 52,
            "methodId": 92,
            "query": 27,
            "retrievalMethod": 36
          }
        },
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 63,
            "baseMethod": 3,
            "chunkMethod": 5,
            "embeddingModel": 7,
            "method": 10,
            "reference": 62,
            "signature": 14,
            "source_doc": 64
          },
          "prompt": 193,
          "responses": [
            194
          ],
          "uid": "fc04cf70-1464-499a-8a70-99c4077b8ec9",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 67,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 60,
            "retrievalMethod": 10
          }
        },
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 30,
            "baseMethod": 3,
            "chunkMethod": 26,
            "embeddingModel": 7,
            "method": 10,
            "reference": 29,
            "signature": 31,
            "source_doc": 32
          },
          "prompt": 195,
          "responses": [
            196
          ],
          "uid": "ff70540b-64ca-4b91-969b-1ebc75c328eb",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 28,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 27,
            "retrievalMethod": 10
          }
        },
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 13,
            "baseMethod": 3,
            "chunkMethod": 26,
            "docTitle": 6,
            "embeddingModel": 7,
            "method": 10,
            "reference": 12,
            "signature": 31,
            "source_doc": 15
          },
          "prompt": 197,
          "responses": [
            2
          ],
          "uid": "a7cfeb0a-d170-4fb2-80da-3e7561fd2e9b",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 55,
            "docTitle": 6,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 9,
            "retrievalMethod": 10
          }
        },
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 22,
            "baseMethod": 34,
            "chunkMethod": 26,
            "docTitle": 18,
            "method": 36,
            "reference": 21,
            "signature": 53,
            "source_doc": 23
          },
          "prompt": 198,
          "responses": [
            199
          ],
          "uid": "c1255722-4c83-4cf2-9435-0f930f332397",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 76,
            "docTitle": 18,
            "methodId": 92,
            "query": 19,
            "retrievalMethod": 36
          }
        },
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 13,
            "baseMethod": 34,
            "chunkMethod": 26,
            "method": 36,
            "reference": 12,
            "signature": 53,
            "source_doc": 15
          },
          "prompt": 200,
          "responses": [
            201
          ],
          "uid": "f160b04e-581e-403b-8ab6-0eedd5ba80c1",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 57,
            "methodId": 92,
            "query": 9,
            "retrievalMethod": 36
          }
        },
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 22,
            "baseMethod": 3,
            "chunkMethod": 26,
            "docTitle": 18,
            "embeddingModel": 7,
            "method": 10,
            "reference": 21,
            "signature": 31,
            "source_doc": 23
          },
          "prompt": 202,
          "responses": [
            203
          ],
          "uid": "d889ada5-20b4-4f5c-bee7-04ed3d0631f7",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 91,
            "docTitle": 18,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 19,
            "retrievalMethod": 10
          }
        },
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 63,
            "baseMethod": 34,
            "chunkMethod": 26,
            "method": 36,
            "reference": 62,
            "signature": 53,
            "source_doc": 64
          },
          "prompt": 204,
          "responses": [
            205
          ],
          "uid": "a002fd40-1f88-45d2-a290-23c4dcf09fc8",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 79,
            "methodId": 92,
            "query": 60,
            "retrievalMethod": 36
          }
        },
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 63,
            "baseMethod": 3,
            "chunkMethod": 26,
            "embeddingModel": 7,
            "method": 10,
            "reference": 62,
            "signature": 31,
            "source_doc": 64
          },
          "prompt": 206,
          "responses": [
            207
          ],
          "uid": "4c72e706-4a97-4cbf-ac51-8dd2140e80f9",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 88,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 60,
            "retrievalMethod": 10
          }
        },
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 49,
            "baseMethod": 3,
            "chunkMethod": 26,
            "embeddingModel": 7,
            "method": 10,
            "reference": 48,
            "signature": 31,
            "source_doc": 50
          },
          "prompt": 208,
          "responses": [
            97
          ],
          "uid": "4bc1d9ca-4e09-44bd-abff-5cfee5b00dee",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 85,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 46,
            "retrievalMethod": 10
          }
        },
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 49,
            "baseMethod": 34,
            "chunkMethod": 26,
            "method": 36,
            "reference": 48,
            "signature": 53,
            "source_doc": 50
          },
          "prompt": 209,
          "responses": [
            210
          ],
          "uid": "2bb32e10-1e24-4b51-9f87-ed1cccbb90aa",
          "vars": {
            "baseMethod": 34,
            "chunkLibrary": 4,
            "chunkMethod": 26,
            "context": 82,
            "methodId": 92,
            "query": 46,
            "retrievalMethod": 36
          }
        },
        {
          "llm": "gpt-4o-mini",
          "metavars": {
            "LLM_0": "gpt-4o-mini",
            "answer_context": 49,
            "baseMethod": 3,
            "chunkMethod": 5,
            "docTitle": 72,
            "embeddingModel": 7,
            "method": 10,
            "reference": 48,
            "signature": 14,
            "source_doc": 50
          },
          "prompt": 211,
          "responses": [
            212
          ],
          "uid": "64bdd041-de19-4e64-a248-368138910621",
          "vars": {
            "baseMethod": 3,
            "chunkLibrary": 4,
            "chunkMethod": 5,
            "context": 73,
            "docTitle": 72,
            "embeddingModel": 7,
            "methodId": 93,
            "query": 46,
            "retrievalMethod": 10
          }
        }
      ]
    },
    "visNode-33c183d7-6baf-4954-a2a4-e06ce50add74.json": {},
    "chunking-9a8eac2d-45cc-460f-ad25-a59404dfd84b.json": {},
    "retrieval-e8c56605-eb3b-4a26-a4c0-6b437d2d517d.json": {},
    "join-72bbabbc-ede0-4ba5-9797-ac161da5e1e2.json": [
      {
        "vars": {
          "baseMethod": "bm25",
          "chunkLibrary": "",
          "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
          "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
          "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
          "retrievalMethod": "BM25"
        },
        "metavars": {
          "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
          "baseMethod": "bm25",
          "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
          "method": "BM25",
          "reference": "distilbert base uncased finetuned sst2 english",
          "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
          "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
        },
        "uid": "1258cfb6-9c83-4fd2-a1d6-cce00964039e",
        "prompt": "",
        "responses": [
          "the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nhat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the\n\nlibrary. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in\n\n### HF_HUB_DISABLE_TELEMETRY\r\n\r\nBy default, some data is collected by HF libraries (`transformers`, `datasets`, `gradio`,..) to monitor usage, debug issues and help prioritize features."
        ],
        "tokens": {}
      },
      {
        "vars": {
          "baseMethod": "bm25",
          "chunkLibrary": "",
          "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
          "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
          "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
          "retrievalMethod": "BM25"
        },
        "metavars": {
          "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
          "baseMethod": "bm25",
          "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
          "method": "BM25",
          "reference": "10GB",
          "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
          "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
        },
        "uid": "81892b45-ef15-49fc-a829-96ec411b949e",
        "prompt": "",
        "responses": [
          "Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional\n\nattention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should\n\nclass. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after"
        ],
        "tokens": {}
      },
      {
        "vars": {
          "baseMethod": "bm25",
          "chunkLibrary": "",
          "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
          "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
          "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
          "retrievalMethod": "BM25"
        },
        "metavars": {
          "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
          "baseMethod": "bm25",
          "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
          "method": "BM25",
          "reference": "x86_64-unknown-linux-musl",
          "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
          "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
        },
        "uid": "77068abc-c5dd-4754-bb98-018c2b7b518e",
        "prompt": "",
        "responses": [
          "`tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n\n-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\non linux machines. \r\n\r\nIf `HF_HOME` is not set, the default home will be `\"$XDG_CACHE_HOME/huggingface\"` instead\r\nof `\"~/.cache/huggingface\"`.\n\n[on macOS and Linux](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/)\r\nand on [Windows](https://phoenixnap.com/kb/windows-set-environment-variable).\n\nattention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the"
        ],
        "tokens": {}
      },
      {
        "vars": {
          "baseMethod": "bm25",
          "chunkLibrary": "",
          "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
          "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
          "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
          "retrievalMethod": "BM25"
        },
        "metavars": {
          "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
          "baseMethod": "bm25",
          "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
          "method": "BM25",
          "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
          "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
          "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
        },
        "uid": "50d23599-1adf-4309-a2b1-e0433705f7d1",
        "prompt": "",
        "responses": [
          "- [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of\n\nguidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nðŸ§¨ Diffusers Experimental\r\n\r\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\r\nCurrently, the following experiments are supported:\n\nhat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the\n\n- **Encouraging safety in deployment**"
        ],
        "tokens": {}
      },
      {
        "vars": {
          "baseMethod": "bm25",
          "chunkLibrary": "",
          "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
          "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
          "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
          "retrievalMethod": "BM25"
        },
        "metavars": {
          "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
          "baseMethod": "bm25",
          "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
          "method": "BM25",
          "reference": "from_pretrained()",
          "signature": "Overlapping Chunking (LangChain TextSplitter)-BM25",
          "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
        },
        "uid": "7ef6a74a-ce30-4a8f-a4e0-80cc345a7d83",
        "prompt": "",
        "responses": [
          "-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should\n\nthe checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two"
        ],
        "tokens": {}
      },
      {
        "vars": {
          "baseMethod": "cosine",
          "chunkLibrary": "",
          "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
          "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
          "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
          "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
          "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
          "retrievalMethod": "Cosine Similarity"
        },
        "metavars": {
          "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
          "baseMethod": "cosine",
          "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
          "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
          "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
          "method": "Cosine Similarity",
          "reference": "distilbert base uncased finetuned sst2 english",
          "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
          "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
        },
        "uid": "b5b82eaf-5c4d-46e4-aba9-253dd22dce54",
        "prompt": "",
        "responses": [
          "the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nlibrary. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in\n\nsteps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage,\n\nits unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,"
        ],
        "tokens": {}
      },
      {
        "vars": {
          "baseMethod": "cosine",
          "chunkLibrary": "",
          "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
          "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
          "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
          "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
          "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
          "retrievalMethod": "Cosine Similarity"
        },
        "metavars": {
          "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
          "baseMethod": "cosine",
          "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
          "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
          "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
          "method": "Cosine Similarity",
          "reference": "10GB",
          "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
          "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
        },
        "uid": "3135ebd4-e07f-4470-9e43-1adfbb43ba6a",
        "prompt": "",
        "responses": [
          "Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do\n\n```py\r\n>>> from transformers.modeling_utils import load_sharded_checkpoint\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n\nfor a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the"
        ],
        "tokens": {}
      },
      {
        "vars": {
          "baseMethod": "cosine",
          "chunkLibrary": "",
          "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
          "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
          "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
          "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
          "retrievalMethod": "Cosine Similarity"
        },
        "metavars": {
          "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
          "baseMethod": "cosine",
          "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
          "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
          "method": "Cosine Similarity",
          "reference": "x86_64-unknown-linux-musl",
          "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
          "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
        },
        "uid": "a3e7219f-9811-4c8a-85cf-5f8911d2f970",
        "prompt": "",
        "responses": [
          "`tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n\nlibrary, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or\n\ntokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning\n\nits unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,\n\nNow let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well"
        ],
        "tokens": {}
      },
      {
        "vars": {
          "baseMethod": "cosine",
          "chunkLibrary": "",
          "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
          "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
          "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
          "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
          "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
          "retrievalMethod": "Cosine Similarity"
        },
        "metavars": {
          "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
          "baseMethod": "cosine",
          "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
          "docTitle": "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
          "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
          "method": "Cosine Similarity",
          "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
          "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
          "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
        },
        "uid": "b8c1bb74-05e3-4bcd-8919-13aebbd68085",
        "prompt": "",
        "responses": [
          "- [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of\n\nguidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\n-->\r\n\r\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\r\n\r\n## Preamble\n\n- [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent."
        ],
        "tokens": {}
      },
      {
        "vars": {
          "baseMethod": "cosine",
          "chunkLibrary": "",
          "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
          "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
          "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
          "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
          "retrievalMethod": "Cosine Similarity"
        },
        "metavars": {
          "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
          "baseMethod": "cosine",
          "chunkMethod": "Overlapping Chunking (LangChain TextSplitter)",
          "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
          "method": "Cosine Similarity",
          "reference": "from_pretrained()",
          "signature": "Overlapping Chunking (LangChain TextSplitter)-Cosine Similarity",
          "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
        },
        "uid": "153d6a99-a273-4dd5-b732-98ff7f5f08f5",
        "prompt": "",
        "responses": [
          "-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nfor a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n- from_pipe\r\n\r\n## AutoPipelineForInpainting\r\n\r\n[[autodoc]] AutoPipelineForInpainting\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe"
        ],
        "tokens": {}
      },
      {
        "vars": {
          "baseMethod": "bm25",
          "chunkLibrary": "",
          "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
          "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
          "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
          "retrievalMethod": "BM25"
        },
        "metavars": {
          "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
          "baseMethod": "bm25",
          "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
          "method": "BM25",
          "reference": "distilbert base uncased finetuned sst2 english",
          "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
          "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
        },
        "uid": "82779f2a-1f2b-4a95-af41-2d2c7012e813",
        "prompt": "",
        "responses": [
          "the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nto a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all\n\n##try by default, some data is collected by hf libraries ( ` transformers `, ` datasets `, ` gradio `,.. ) to monitor usage, debug issues and help prioritize features. each library defines its own policy ( i. e. which usage to monitor ) but the core implementation happens in ` huggingface _ hub ` ( see [ ` send _ telemetry ` ] ). you can set ` hf _ hub _ disable _ telemetry = 1 ` as environment variable to globally disable telemetry. # # # hf _ hub _ enable _ hf _ transfer set to ` true ` for faster uploads and downloads from the hub using ` hf _ transfer `. by default, ` huggingface _ hub ` uses the python - based ` requests. get ` and ` requests. post ` functions. although these are reliable and versatile, they may not be the most efficient choice for machines with high\n\n. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs."
        ],
        "tokens": {}
      },
      {
        "vars": {
          "baseMethod": "bm25",
          "chunkLibrary": "",
          "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
          "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
          "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
          "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
          "retrievalMethod": "BM25"
        },
        "metavars": {
          "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
          "baseMethod": "bm25",
          "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
          "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
          "method": "BM25",
          "reference": "10GB",
          "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
          "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
        },
        "uid": "65b0fefe-da51-431c-8a9d-38349b2357e2",
        "prompt": "",
        "responses": [
          "slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\ntraining, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when\n\nsize of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] \\{ ' total _ size ' : 433245184 \\} ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] \\{ ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre\n\n... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )"
        ],
        "tokens": {}
      },
      {
        "vars": {
          "baseMethod": "bm25",
          "chunkLibrary": "",
          "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
          "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
          "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
          "retrievalMethod": "BM25"
        },
        "metavars": {
          "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
          "baseMethod": "bm25",
          "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
          "method": "BM25",
          "reference": "x86_64-unknown-linux-musl",
          "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
          "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
        },
        "uid": "e5e4347c-c12c-4ccc-a2c9-431809dfc69e",
        "prompt": "",
        "responses": [
          "` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `\n\nhf _ home ` is not set! this is the default way to configure where [ user - specific non - essential ( cached ) data should be written ] ( https : / / wiki. archlinux. org / title / xdg _ base _ directory ) on linux machines. if ` hf _ home ` is not set, the default home will be ` \" $ xdg _ cache _ home / huggingface \" ` instead of ` \" ~ /. cache / huggingface \" `.\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\n. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size\n\n! - - note that this file is in markdown but contain specific syntax for our doc - builder ( similar to mdx ) that may not be rendered properly in your markdown viewer. - - > # environment variables ` huggingface _ hub ` can be configured using environment variables. if you are unfamiliar with environment variable, here are generic articles about them [ on macos and linux ] ( https : / / linuxize. com / post / how - to - set - and - list - environment - variables - in - linux / ) and on [ windows ] ( https : / / phoenixnap. com / kb / windows - set - environment - variable ). this page will guide you through all environment variables specific to ` huggingface _ hub ` and their meaning. # # generic # # # hf _ inference _ endpoint to configure the inference api base url. you might want to set this variable if your organization is pointing at an api gateway rather"
        ],
        "tokens": {}
      },
      {
        "vars": {
          "baseMethod": "bm25",
          "chunkLibrary": "",
          "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
          "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
          "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
          "retrievalMethod": "BM25"
        },
        "metavars": {
          "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
          "baseMethod": "bm25",
          "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
          "method": "BM25",
          "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
          "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
          "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
        },
        "uid": "922b2750-c385-438b-97aa-cb9319433db9",
        "prompt": "",
        "responses": [
          "diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https\n\nhttps : / / huggingface. co / spaces / society - ethics / diffusionbiasexplorer ) to demonstrate the biases in stable diffusion interactively. in this sense, we support and encourage bias explorers and evaluations. - * * encouraging safety in deployment * * - [ * * safe stable diffusion * * ] ( https : / / huggingface. co / docs / diffusers / main / en / api / pipelines / stable _ diffusion / stable _ diffusion _ safe ) : it mitigates the well - known issue that models, like stable diffusion, that are trained on unfiltered, web - crawled datasets tend to suffer from inappropriate degeneration. related paper : [ safe latent diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / gi\n\nsame level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and\n\ndiffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special"
        ],
        "tokens": {}
      },
      {
        "vars": {
          "baseMethod": "bm25",
          "chunkLibrary": "",
          "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
          "methodId": "37590bb9-8313-4bdd-97f0-bb147727726c",
          "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
          "retrievalMethod": "BM25"
        },
        "metavars": {
          "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
          "baseMethod": "bm25",
          "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
          "method": "BM25",
          "reference": "from_pretrained()",
          "signature": "Overlapping Chunking (HuggingFace Tokenizers)-BM25",
          "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
        },
        "uid": "378f8a7a-4613-49ca-8ca1-e8c685f9cc5b",
        "prompt": "",
        "responses": [
          "to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\nthe first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\n:... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir ) ` ` ` the main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in ram to the model size plus the size of the biggest shard. behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. we can load that index like any json and get a dictionary : ` ` ` py > > > import json > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ sha"
        ],
        "tokens": {}
      },
      {
        "vars": {
          "baseMethod": "cosine",
          "chunkLibrary": "",
          "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
          "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
          "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
          "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
          "query": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
          "retrievalMethod": "Cosine Similarity"
        },
        "metavars": {
          "answer_context": "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
          "baseMethod": "cosine",
          "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
          "docTitle": "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
          "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
          "method": "Cosine Similarity",
          "reference": "distilbert base uncased finetuned sst2 english",
          "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
          "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md"
        },
        "uid": "a8a3e1a5-0669-4f59-81a0-b42c7641982f",
        "prompt": "",
        "responses": [
          "the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nto a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all\n\nremoved. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. to get an output linked to our classification problem, we need to use the automodelforsequenceclassification class. it works exactly as the automodel class, except that it will build a model with a classification head. there is one auto class for each common nlp task in the transformers library. here, after giving our model the two sentences, we get a tensor of size two by two : one result for each sentence and for each possible label. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third\n\n. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs."
        ],
        "tokens": {}
      },
      {
        "vars": {
          "baseMethod": "cosine",
          "chunkLibrary": "",
          "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
          "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
          "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
          "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
          "query": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
          "retrievalMethod": "Cosine Similarity"
        },
        "metavars": {
          "answer_context": "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
          "baseMethod": "cosine",
          "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
          "docTitle": "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
          "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
          "method": "Cosine Similarity",
          "reference": "10GB",
          "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
          "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md"
        },
        "uid": "c1ab4cd7-3c4b-4f60-ad2b-fbe67338c078",
        "prompt": "",
        "responses": [
          "slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~\n\ntraining, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\n... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )\n\nsize of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] \\{ ' total _ size ' : 433245184 \\} ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] \\{ ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre"
        ],
        "tokens": {}
      },
      {
        "vars": {
          "baseMethod": "cosine",
          "chunkLibrary": "",
          "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
          "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
          "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
          "query": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
          "retrievalMethod": "Cosine Similarity"
        },
        "metavars": {
          "answer_context": " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
          "baseMethod": "cosine",
          "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
          "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
          "method": "Cosine Similarity",
          "reference": "x86_64-unknown-linux-musl",
          "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
          "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md"
        },
        "uid": "a99d7353-31ef-44aa-ab99-79a44cbd6e5f",
        "prompt": "",
        "responses": [
          "` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `\n\nboth variants regardless. this change will improve performance for many applications. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5216 ] ( https : / / github. com / gradio - app / gradio / pull / 5216 ) [ ` 4b58ea6d ` ] ( https : / / github. com / gradio - app / gradio / commit / 4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8 ) - update i18n tokens and locale files. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )! # # 0. 2. 0 # # # features - [ # 5025 ] ( https : / / github. com / gradio -\n\nthe first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\n. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special"
        ],
        "tokens": {}
      },
      {
        "vars": {
          "baseMethod": "cosine",
          "chunkLibrary": "",
          "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
          "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
          "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
          "query": "What is the purpose of the Safety Checker in the Diffusers library?\n",
          "retrievalMethod": "Cosine Similarity"
        },
        "metavars": {
          "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
          "baseMethod": "cosine",
          "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
          "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
          "method": "Cosine Similarity",
          "reference": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
          "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
          "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md"
        },
        "uid": "b9b8bc0e-aecf-41d1-af14-a7950ecf216a",
        "prompt": "",
        "responses": [
          "diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https\n\nsame level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and\n\n' s responsiveness and valuable feedback. # # scope the diffusers community will apply the following ethical guidelines to the project â€™ s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns. # # ethical guidelines the following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question. - * * transparency * * : we are committed to being transparent in managing prs, explaining our choices to users, and making technical decisions. - * * consistency * * : we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # diffusers â€™ ethical guidelines # # preamble [ diffusers ] ( https : / / huggingface. co / docs / diffusers / index ) provides pre - trained diffusion models and serves as a modular toolbox for inference and training. given its real case applications in the world and potential negative impacts on society, we think it\n\ndiffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model."
        ],
        "tokens": {}
      },
      {
        "vars": {
          "baseMethod": "cosine",
          "chunkLibrary": "",
          "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
          "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
          "methodId": "d5820512-daae-46fe-b47c-7d82218a4402",
          "query": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
          "retrievalMethod": "Cosine Similarity"
        },
        "metavars": {
          "answer_context": "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
          "baseMethod": "cosine",
          "chunkMethod": "Overlapping Chunking (HuggingFace Tokenizers)",
          "embeddingModel": "sentence-transformers/all-MiniLM-L6-v2",
          "method": "Cosine Similarity",
          "reference": "from_pretrained()",
          "signature": "Overlapping Chunking (HuggingFace Tokenizers)-Cosine Similarity",
          "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md"
        },
        "uid": "9711c621-7e76-4669-ad25-06f4a8645b8f",
        "prompt": "",
        "responses": [
          "! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\nto load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,\n\nautodoc ] ] autopipelineforimage2image - all - from _ pretrained - from _ pipe # # autopipelineforinpainting [ [ autodoc ] ] autopipelineforinpainting - all - from _ pretrained - from _ pipe\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\n. bin ', ' pytorch _ model - 00002 - of - 00003. bin ', ' pytorch _ model - 00003 - of - 00003. bin ', ' pytorch _ model. bin. index. json ' ] ` ` ` on top of the configuration of the model, we see three different weights files, and an ` index. json ` file which is our index. a checkpoint like this can be fully reloaded using the [ ` ~ pretrainedmodel. from _ pretrained ` ] method : ` ` ` py > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir"
        ],
        "tokens": {}
      }
    ],
    "comment-9f37d19f-7be6-4501-8c90-ad0f8f7c55b6.json": {},
    "comment-7732d1d4-0bfe-4ca4-a26a-dc0fbc153c8a.json": {},
    "__s": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\nContext: the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nlibrary. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in\n\nsteps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage,\n\nits unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,\nAnswer: ",
      "gpt-4o-mini",
      "The default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst2-english.\"",
      "cosine",
      "",
      "Overlapping (LangChain)",
      "huggingface_course_blob_main_subtitles_en_raw_chapter2_02_inside-pipeline-pt.md.txt",
      "sentence-transformers/all-MiniLM-L6-v2",
      "cafa53f3-e774-4fba-8bae-9456534e05de",
      "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
      "Cosine Similarity",
      "the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nlibrary. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in\n\nsteps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage,\n\nits unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,",
      "distilbert base uncased finetuned sst2 english",
      "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.",
      "Overlapping (LangChain)-Cosine Similarity",
      "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\nContext: Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do\n\n```py\r\n>>> from transformers.modeling_utils import load_sharded_checkpoint\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n\nfor a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the\nAnswer: ",
      "The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB.",
      "huggingface_transformers_blob_main_docs_source_en_big_models.md.txt",
      "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
      "Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do\n\n```py\r\n>>> from transformers.modeling_utils import load_sharded_checkpoint\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n\nfor a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the",
      "10GB",
      "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Instantiating a big model\n\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\nfrom PyTorch is:\n\n1. Create your model with random weights.\n2. Load your pretrained weights.\n3. Put those pretrained weights in your random model.\n\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n\n<Tip>\n\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\nNow let's use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
      "huggingface/transformers/blob/main/docs/source/en/big_models.md",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\nContext: ` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `\n\nboth variants regardless. this change will improve performance for many applications. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5216 ] ( https : / / github. com / gradio - app / gradio / pull / 5216 ) [ ` 4b58ea6d ` ] ( https : / / github. com / gradio - app / gradio / commit / 4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8 ) - update i18n tokens and locale files. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )! # # 0. 2. 0 # # # features - [ # 5025 ] ( https : / / github. com / gradio -\n\nthe first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\n. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\nAnswer: ",
      "The `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on Linux with the musl C library.",
      "Overlapping (HuggingFace)",
      "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
      "` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `\n\nboth variants regardless. this change will improve performance for many applications. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5216 ] ( https : / / github. com / gradio - app / gradio / pull / 5216 ) [ ` 4b58ea6d ` ] ( https : / / github. com / gradio - app / gradio / commit / 4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8 ) - update i18n tokens and locale files. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )! # # 0. 2. 0 # # # features - [ # 5025 ] ( https : / / github. com / gradio -\n\nthe first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\n. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special",
      "x86_64-unknown-linux-musl",
      " `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
      "Overlapping (HuggingFace)-Cosine Similarity",
      "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\nContext: Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional\n\nattention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should\n\nclass. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after\nAnswer: ",
      "bm25",
      "8c894809-a9f1-427b-b700-75a29c286ed8",
      "BM25",
      "Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional\n\nattention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should\n\nclass. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after",
      "Overlapping (LangChain)-BM25",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\nContext: `tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n\n-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\non linux machines. \r\n\r\nIf `HF_HOME` is not set, the default home will be `\"$XDG_CACHE_HOME/huggingface\"` instead\r\nof `\"~/.cache/huggingface\"`.\n\n[on macOS and Linux](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/)\r\nand on [Windows](https://phoenixnap.com/kb/windows-set-environment-variable).\n\nattention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the\nAnswer: ",
      "The `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on Linux using the musl C library.",
      "`tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n\n-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\non linux machines. \r\n\r\nIf `HF_HOME` is not set, the default home will be `\"$XDG_CACHE_HOME/huggingface\"` instead\r\nof `\"~/.cache/huggingface\"`.\n\n[on macOS and Linux](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/)\r\nand on [Windows](https://phoenixnap.com/kb/windows-set-environment-variable).\n\nattention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\nContext: the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nhat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the\n\nlibrary. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in\n\n### HF_HUB_DISABLE_TELEMETRY\r\n\r\nBy default, some data is collected by HF libraries (`transformers`, `datasets`, `gradio`,..) to monitor usage, debug issues and help prioritize features.\nAnswer: ",
      "the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nhat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the\n\nlibrary. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in\n\n### HF_HUB_DISABLE_TELEMETRY\r\n\r\nBy default, some data is collected by HF libraries (`transformers`, `datasets`, `gradio`,..) to monitor usage, debug issues and help prioritize features.",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\nContext: - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of\n\nguidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nðŸ§¨ Diffusers Experimental\r\n\r\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\r\nCurrently, the following experiments are supported:\n\nhat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the\n\n- **Encouraging safety in deployment**\nAnswer: ",
      "The purpose of the Safety Checker in the Diffusers library is to check and compare class probabilities against a set of guidelines to ensure safety in deployment. It aims to encourage safe usage and contributions within the library.",
      "What is the purpose of the Safety Checker in the Diffusers library?\n",
      "- [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of\n\nguidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nðŸ§¨ Diffusers Experimental\r\n\r\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\r\nCurrently, the following experiments are supported:\n\nhat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the\n\n- **Encouraging safety in deployment**",
      "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
      "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\nContext: ` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `\n\nhf _ home ` is not set! this is the default way to configure where [ user - specific non - essential ( cached ) data should be written ] ( https : / / wiki. archlinux. org / title / xdg _ base _ directory ) on linux machines. if ` hf _ home ` is not set, the default home will be ` \" $ xdg _ cache _ home / huggingface \" ` instead of ` \" ~ /. cache / huggingface \" `.\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\n. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size\n\n! - - note that this file is in markdown but contain specific syntax for our doc - builder ( similar to mdx ) that may not be rendered properly in your markdown viewer. - - > # environment variables ` huggingface _ hub ` can be configured using environment variables. if you are unfamiliar with environment variable, here are generic articles about them [ on macos and linux ] ( https : / / linuxize. com / post / how - to - set - and - list - environment - variables - in - linux / ) and on [ windows ] ( https : / / phoenixnap. com / kb / windows - set - environment - variable ). this page will guide you through all environment variables specific to ` huggingface _ hub ` and their meaning. # # generic # # # hf _ inference _ endpoint to configure the inference api base url. you might want to set this variable if your organization is pointing at an api gateway rather\nAnswer: ",
      "` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `\n\nhf _ home ` is not set! this is the default way to configure where [ user - specific non - essential ( cached ) data should be written ] ( https : / / wiki. archlinux. org / title / xdg _ base _ directory ) on linux machines. if ` hf _ home ` is not set, the default home will be ` \" $ xdg _ cache _ home / huggingface \" ` instead of ` \" ~ /. cache / huggingface \" `.\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\n. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size\n\n! - - note that this file is in markdown but contain specific syntax for our doc - builder ( similar to mdx ) that may not be rendered properly in your markdown viewer. - - > # environment variables ` huggingface _ hub ` can be configured using environment variables. if you are unfamiliar with environment variable, here are generic articles about them [ on macos and linux ] ( https : / / linuxize. com / post / how - to - set - and - list - environment - variables - in - linux / ) and on [ windows ] ( https : / / phoenixnap. com / kb / windows - set - environment - variable ). this page will guide you through all environment variables specific to ` huggingface _ hub ` and their meaning. # # generic # # # hf _ inference _ endpoint to configure the inference api base url. you might want to set this variable if your organization is pointing at an api gateway rather",
      "Overlapping (HuggingFace)-BM25",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\nContext: the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nto a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all\n\nremoved. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. to get an output linked to our classification problem, we need to use the automodelforsequenceclassification class. it works exactly as the automodel class, except that it will build a model with a classification head. there is one auto class for each common nlp task in the transformers library. here, after giving our model the two sentences, we get a tensor of size two by two : one result for each sentence and for each possible label. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third\n\n. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs.\nAnswer: ",
      "the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nto a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all\n\nremoved. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. to get an output linked to our classification problem, we need to use the automodelforsequenceclassification class. it works exactly as the automodel class, except that it will build a model with a classification head. there is one auto class for each common nlp task in the transformers library. here, after giving our model the two sentences, we get a tensor of size two by two : one result for each sentence and for each possible label. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third\n\n. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs.",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\nContext: the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nto a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all\n\n##try by default, some data is collected by hf libraries ( ` transformers `, ` datasets `, ` gradio `,.. ) to monitor usage, debug issues and help prioritize features. each library defines its own policy ( i. e. which usage to monitor ) but the core implementation happens in ` huggingface _ hub ` ( see [ ` send _ telemetry ` ] ). you can set ` hf _ hub _ disable _ telemetry = 1 ` as environment variable to globally disable telemetry. # # # hf _ hub _ enable _ hf _ transfer set to ` true ` for faster uploads and downloads from the hub using ` hf _ transfer `. by default, ` huggingface _ hub ` uses the python - based ` requests. get ` and ` requests. post ` functions. although these are reliable and versatile, they may not be the most efficient choice for machines with high\n\n. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs.\nAnswer: ",
      "the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nto a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all\n\n##try by default, some data is collected by hf libraries ( ` transformers `, ` datasets `, ` gradio `,.. ) to monitor usage, debug issues and help prioritize features. each library defines its own policy ( i. e. which usage to monitor ) but the core implementation happens in ` huggingface _ hub ` ( see [ ` send _ telemetry ` ] ). you can set ` hf _ hub _ disable _ telemetry = 1 ` as environment variable to globally disable telemetry. # # # hf _ hub _ enable _ hf _ transfer set to ` true ` for faster uploads and downloads from the hub using ` hf _ transfer `. by default, ` huggingface _ hub ` uses the python - based ` requests. get ` and ` requests. post ` functions. although these are reliable and versatile, they may not be the most efficient choice for machines with high\n\n. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs.",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\nContext: -->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should\n\nthe checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\nAnswer: ",
      "The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This method downloads and caches the configuration and vocabulary associated with the specified checkpoint. It automatically retrieves the relevant pipeline based on the name or path to the pretrained weights.",
      "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
      "-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should\n\nthe checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two",
      "from_pretrained()",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n\nimage = pipeline(prompt, num_inference_steps=25).images[0]\n```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe\n",
      "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\nContext: -->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nfor a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n- from_pipe\r\n\r\n## AutoPipelineForInpainting\r\n\r\n[[autodoc]] AutoPipelineForInpainting\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe\nAnswer: ",
      "The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This method automatically retrieves the relevant pipeline based on the name or path to the pretrained weights. Additionally, to switch between tasks without reallocating memory, you can use the `from_pipe()` method.",
      "-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nfor a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n- from_pipe\r\n\r\n## AutoPipelineForInpainting\r\n\r\n[[autodoc]] AutoPipelineForInpainting\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\nContext: `tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n\nlibrary, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or\n\ntokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning\n\nits unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,\n\nNow let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well\nAnswer: ",
      "`tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n\nlibrary, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or\n\ntokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning\n\nits unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,\n\nNow let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\nContext: - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of\n\nguidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\n-->\r\n\r\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\r\n\r\n## Preamble\n\n- [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\nAnswer: ",
      "The purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of a set of guidelines to ensure safe usage of the models. It aims to mitigate issues associated with models like Stable Diffusion. This helps guide the development and contributions to the library while promoting ethical usage.",
      "huggingface_diffusers_blob_main_docs_source_en_conceptual_ethical_guidelines.md.txt",
      "- [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of\n\nguidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\n-->\r\n\r\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\r\n\r\n## Preamble\n\n- [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\nContext: slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\ntraining, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when\n\nsize of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] { ' total _ size ' : 433245184 } ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] { ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre\n\n... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )\nAnswer: ",
      "In Transformers version 4.18.0, model checkpoints that exceed 10 GB in size are automatically sharded into smaller pieces. Each of these smaller pieces will be less than 10 GB. You can also control the maximum size before sharding using the `max_shard_size` parameter.",
      "slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\ntraining, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when\n\nsize of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] \\{ ' total _ size ' : 433245184 \\} ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] \\{ ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre\n\n... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\nContext: to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\nthe first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\n:... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir ) ` ` ` the main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in ram to the model size plus the size of the biggest shard. behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. we can load that index like any json and get a dictionary : ` ` ` py > > > import json > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ sha\nAnswer: ",
      "To load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method, which automatically retrieves the relevant pipeline given the name or path to the pretrained weights. This method allows you to load a checkpoint without needing to know the specific pipeline class. Additionally, you can use the `from_pipe()` method to transfer components between pipelines if needed.",
      "to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\nthe first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\n:... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir ) ` ` ` the main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in ram to the model size plus the size of the biggest shard. behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. we can load that index like any json and get a dictionary : ` ` ` py > > > import json > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ sha",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\nContext: diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https\n\nhttps : / / huggingface. co / spaces / society - ethics / diffusionbiasexplorer ) to demonstrate the biases in stable diffusion interactively. in this sense, we support and encourage bias explorers and evaluations. - * * encouraging safety in deployment * * - [ * * safe stable diffusion * * ] ( https : / / huggingface. co / docs / diffusers / main / en / api / pipelines / stable _ diffusion / stable _ diffusion _ safe ) : it mitigates the well - known issue that models, like stable diffusion, that are trained on unfiltered, web - crawled datasets tend to suffer from inappropriate degeneration. related paper : [ safe latent diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / gi\n\nsame level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and\n\ndiffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\nAnswer: ",
      "The purpose of the Safety Checker in the Diffusers library is to mitigate inappropriate degeneration in diffusion models by checking and comparing the class probability of harmful concepts against generated images. It uses a set of hard-coded harmful concepts that are intentionally hidden to prevent reverse engineering. This mechanism helps ensure safer deployment of models trained on unfiltered datasets.",
      "diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https\n\nhttps : / / huggingface. co / spaces / society - ethics / diffusionbiasexplorer ) to demonstrate the biases in stable diffusion interactively. in this sense, we support and encourage bias explorers and evaluations. - * * encouraging safety in deployment * * - [ * * safe stable diffusion * * ] ( https : / / huggingface. co / docs / diffusers / main / en / api / pipelines / stable _ diffusion / stable _ diffusion _ safe ) : it mitigates the well - known issue that models, like stable diffusion, that are trained on unfiltered, web - crawled datasets tend to suffer from inappropriate degeneration. related paper : [ safe latent diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / gi\n\nsame level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and\n\ndiffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\nContext: diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https\n\nsame level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and\n\n' s responsiveness and valuable feedback. # # scope the diffusers community will apply the following ethical guidelines to the project â€™ s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns. # # ethical guidelines the following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question. - * * transparency * * : we are committed to being transparent in managing prs, explaining our choices to users, and making technical decisions. - * * consistency * * : we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # diffusers â€™ ethical guidelines # # preamble [ diffusers ] ( https : / / huggingface. co / docs / diffusers / index ) provides pre - trained diffusion models and serves as a modular toolbox for inference and training. given its real case applications in the world and potential negative impacts on society, we think it\n\ndiffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model.\nAnswer: ",
      "The purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of hard-coded harmful concepts against generated images to mitigate inappropriate degeneration in diffusion models. It aims to prevent the generation of harmful content by ensuring that such concepts are not present in the output. The harmful concepts are intentionally hidden to protect the integrity of the safety mechanism.",
      "diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https\n\nsame level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and\n\n' s responsiveness and valuable feedback. # # scope the diffusers community will apply the following ethical guidelines to the project â€™ s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns. # # ethical guidelines the following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question. - * * transparency * * : we are committed to being transparent in managing prs, explaining our choices to users, and making technical decisions. - * * consistency * * : we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # diffusers â€™ ethical guidelines # # preamble [ diffusers ] ( https : / / huggingface. co / docs / diffusers / index ) provides pre - trained diffusion models and serves as a modular toolbox for inference and training. given its real case applications in the world and potential negative impacts on society, we think it\n\ndiffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model.",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\nContext: ! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\nto load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,\n\nautodoc ] ] autopipelineforimage2image - all - from _ pretrained - from _ pipe # # autopipelineforinpainting [ [ autodoc ] ] autopipelineforinpainting - all - from _ pretrained - from _ pipe\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\n. bin ', ' pytorch _ model - 00002 - of - 00003. bin ', ' pytorch _ model - 00003 - of - 00003. bin ', ' pytorch _ model. bin. index. json ' ] ` ` ` on top of the configuration of the model, we see three different weights files, and an ` index. json ` file which is our index. a checkpoint like this can be fully reloaded using the [ ` ~ pretrainedmodel. from _ pretrained ` ] method : ` ` ` py > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir\nAnswer: ",
      "To load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method, which retrieves the relevant pipeline given the name or path to the pretrained weights. Additionally, to switch between tasks with the same checkpoint without reallocating memory, you can use the `from_pipe()` method. These methods simplify the process of loading checkpoints without needing to know the specific pipeline class.",
      "! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\nto load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,\n\nautodoc ] ] autopipelineforimage2image - all - from _ pretrained - from _ pipe # # autopipelineforinpainting [ [ autodoc ] ] autopipelineforinpainting - all - from _ pretrained - from _ pipe\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\n. bin ', ' pytorch _ model - 00002 - of - 00003. bin ', ' pytorch _ model - 00003 - of - 00003. bin ', ' pytorch _ model. bin. index. json ' ] ` ` ` on top of the configuration of the model, we see three different weights files, and an ` index. json ` file which is our index. a checkpoint like this can be fully reloaded using the [ ` ~ pretrainedmodel. from _ pretrained ` ] method : ` ` ` py > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise.\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\nContext: slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~\n\ntraining, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\n... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )\n\nsize of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] { ' total _ size ' : 433245184 } ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] { ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre\nAnswer: ",
      "In Transformers version 4.18.0, model checkpoints that exceed 10 GB in size are automatically sharded into smaller pieces. Each shard will be less than 10 GB. You can control the maximum size before sharding using the `max_shard_size` parameter.",
      "slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~\n\ntraining, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\n... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )\n\nsize of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] \\{ ' total _ size ' : 433245184 \\} ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] \\{ ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre",
      "136ae727-e599-426f-8c66-d057d9861f63",
      "57bde806-d5f3-459c-873d-34810dbb9a4e",
      "The purpose of the Safety Checker in the Diffusers library is to check and compare class probabilities against a set of guidelines to ensure safety in deployment. It aims to guide the development and usage of the library by encouraging safe practices.",
      "The method used to load a checkpoint for a task using `AutoPipeline` is the `from_pretrained()` method, which retrieves the relevant pipeline based on the name or path to the pretrained weights. Additionally, the `from_pipe()` method can be used to transfer components from the original pipeline to a new one without reallocating memory.",
      "The purpose of the Safety Checker in the Diffusers library is to mitigate inappropriate degeneration in diffusion models by checking and comparing the class probability of harmful concepts against generated images. It aims to prevent the generation of harmful content by using hard-coded harmful concepts that are hidden to avoid reverse engineering. This feature is part of the broader effort to ensure responsible and safe deployment of AI technologies.",
      "The purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of hard-coded harmful concepts against generated images to mitigate inappropriate degeneration in diffusion models. It aims to prevent the generation of harmful content by ensuring that such concepts are not present in the output. The harmful concepts are intentionally hidden to protect the integrity of the checker.",
      "To load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method, which retrieves the relevant pipeline given the name or path to the pretrained weights. Additionally, to switch between tasks with the same checkpoint without reallocating memory, the `from_pipe()` method can be used to transfer components from the original pipeline to the new one.",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the faithfulness of a generated answer based solely on the provided context.\n\nFaithfulness measures whether the generated answer contains only information that is supported by the provided context. An answer is considered faithful if all its factual claims are directly supported by or can be reasonably inferred from the context\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the context (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the context (assign 0 points)\n3. Calculate the faithfulness score as: (sum of points) / (total number of claims)\n\nReturn a final faithfulness score between 0 and 1, where:\n- 1.0: Perfect faithfulness (all claims fully supported)\n- 0.0: Complete unfaithfulness (no claims supported by context)\n\nProvided Context: hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.\n\nAnswer: \n```\nThe default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst2-english.\"\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "gpt-4o",
      "The provided answer contains one factual claim:\n1. \"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n\nThis claim is fully supported by the context, which states: \"Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english.\" The key information about the default checkpoint's name matches exactly between the answer and the context. \n\nSince there is only one claim and it is fully supported by the context, the faithfulness score is 1.0.\n\nSCORE: 1.0",
      "2",
      "0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\n```\n\nContext: \n```\nThe maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. **Identify the main information need(s) in the question:**\n   - The question asks for the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0.\n\n2. **Assess the context:**\n   - The context directly addresses the question by stating that the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB.\n   - The information provided is comprehensive and directly answers the question without any missing critical information.\n\n3. **Rate the overall context relevancy:**\n   - The context contains all the necessary information to fully answer the question, making it perfectly relevant.\n\nSCORE: 1.0",
      "4",
      "To evaluate the faithfulness of the generated answer, I will identify and assess the factual claims present.\n\nFactual Claim:\n1. \"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n\nAssessment:\n- This claim is fully supported by the provided context. The context specifies that the checkpoint used by default for the sentiment analysis pipeline is \"distilbert-base-uncased-finetuned-sst2-english.\"\n\nSince the claim is fully supported by the context, it is assigned a score of 1 point.\n\nThere is only one factual claim in the answer. Therefore, the faithfulness score is calculated as follows:\n\nFaithfulness Score = (sum of points) / (total number of claims) = 1 / 1 = 1.0\n\nSCORE: 1.0",
      "1",
      "6",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\n```\n\nContext: \n```\nThe default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst2-english.\"\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. **Identify the main information need(s) in the question:**\n   - The question asks for the default checkpoint used by the sentiment analysis pipeline in the Transformers library. The main information needed is the specific name or identifier of the checkpoint.\n\n2. **Assess the context:**\n   - The context provided states: \"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n   - This context directly answers the question by providing the exact name of the default checkpoint.\n   - The information is comprehensive as it gives the full name of the checkpoint without any ambiguity.\n   - There is no critical information missing; the context fully satisfies the information need of the question.\n\n3. **Rate the overall context relevancy:**\n   - The context is perfectly relevant as it contains all the necessary information to fully answer the question.\n\nSCORE: 1.0",
      "1. **Identify the main information need(s) in the question:**\n   - The question asks for the default checkpoint used by the sentiment analysis pipeline in the Transformers library. The main information needed is the specific name or identifier of the checkpoint.\n\n2. **Assess the context:**\n   - The context provided states: \"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n   - This context directly answers the question by providing the exact name of the default checkpoint used in the sentiment analysis pipeline.\n   - The information is comprehensive as it gives the full name of the checkpoint, which is the critical piece of information needed to answer the question.\n\n3. **Rate the overall context relevancy:**\n   - The context contains all the necessary information to fully answer the question without any gaps. It directly addresses the main information need by specifying the default checkpoint.\n   - Therefore, the context relevancy is perfect.\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\n```\n\nContext: \n```\nThe `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on Linux with the musl C library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. **Identify the main information need(s) in the question:**\n   - The question asks for the specific architecture that the `tokenizers-linux-x64-musl` binary is designed for.\n\n2. **Assess the context:**\n   - The context states that the `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on Linux with the musl C library.\n   - This information directly answers the question by specifying the architecture as x86_64.\n   - The context is comprehensive in that it provides the exact architecture and additional relevant details about the operating system and C library, which are pertinent to understanding the environment for which the binary is designed.\n   - There is no critical information missing; the context fully addresses the question.\n\n3. **Rate the overall context relevancy:**\n   - The context is perfectly relevant as it contains all the necessary information to fully answer the question about the architecture of the binary.\n\nSCORE: 1.0",
      "9",
      "1. **Identify the main information need(s) in the question:**\n   - The question asks for the default checkpoint used by the sentiment analysis pipeline in the Transformers library. The main information needed is the specific name or identifier of the checkpoint.\n\n2. **Assess the context:**\n   - The context provided states: \"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n   - This context directly answers the question by providing the exact name of the default checkpoint.\n   - The information is comprehensive as it fully addresses the question without any missing details.\n\n3. **Rate the overall context relevancy:**\n   - The context contains all the necessary information to fully answer the question, with no gaps or missing critical information.\n\nBased on the above evaluation, the context is perfectly relevant to the question.\n\nSCORE: 1.0",
      "5",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\n```\n\nContext: \n```\nTo load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method, which automatically retrieves the relevant pipeline given the name or path to the pretrained weights. This method allows you to load a checkpoint without needing to know the specific pipeline class. Additionally, you can use the `from_pipe()` method to transfer components between pipelines if needed.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. **Identify the main information need(s) in the question:**\n   - The question asks for the method used to load a checkpoint for a task using `AutoPipeline`.\n\n2. **Assess the context:**\n   - The context provides information directly relevant to the question by mentioning the `from_pretrained()` method as the way to load a checkpoint for a task using `AutoPipeline`.\n   - The context is comprehensive in addressing the main information need, as it explains that `from_pretrained()` automatically retrieves the relevant pipeline given the name or path to the pretrained weights. This directly answers the question.\n   - There is no critical information missing for answering the question, as the context clearly states the method used and its purpose.\n\n3. **Rate the overall context relevancy:**\n   - The context is perfectly relevant as it contains all the necessary information to fully answer the question about the method used to load a checkpoint with `AutoPipeline`.\n\nSCORE: 1.0",
      "16",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\n```\n\nContext: \n```\nThe `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on Linux using the musl C library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. **Identify the main information need(s) in the question:**\n   - The question asks for the specific architecture that the `tokenizers-linux-x64-musl` binary is designed for.\n\n2. **Assess the context:**\n   - The context states that the `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture.\n   - This directly answers the question by providing the exact architecture the binary is designed for.\n   - The context also mentions that it is for Linux using the musl C library, which, while additional, does not detract from the relevance to the main question.\n\n3. **Rate the overall context relevancy:**\n   - The context contains all the necessary information to fully answer the question about the architecture of the binary.\n   - There are no significant gaps or missing critical information regarding the architecture.\n\nGiven that the context provides a complete and direct answer to the question, the context relevancy is rated as perfect.\n\nSCORE: 1.0",
      "1. **Identify the main information need(s) in the question:**\n   - The question asks for the specific architecture that the `tokenizers-linux-x64-musl` binary is designed for.\n\n2. **Assess the context:**\n   - The context states that the `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture.\n   - This directly answers the question by providing the exact architecture the binary is designed for.\n   - The context is comprehensive in terms of the information needed to answer the question, as it specifies both the architecture (x86_64) and the operating system environment (Linux using the musl C library), which is additional relevant information.\n\n3. **Rate the overall context relevancy:**\n   - The context contains all the necessary information to fully answer the question about the architecture of the binary. There are no significant gaps or missing critical information.\n\nBased on the above analysis, the context is perfectly relevant to the question.\n\nSCORE: 1.0",
      "7",
      "1. **Identify the main information need(s) in the question:**\n   - The question asks for the specific architecture that the `tokenizers-linux-x64-musl` binary is designed for.\n\n2. **Assess the context:**\n   - The context states that the `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on Linux with the musl C library.\n   - This directly answers the question by specifying the architecture as x86_64.\n   - The context is comprehensive in terms of the architecture information, as it also mentions the operating system (Linux) and the C library (musl), which are relevant to understanding the environment for which the binary is designed.\n   - There is no critical information missing regarding the architecture, as the question is specifically about the architecture, and the context provides this information clearly.\n\n3. **Rate the overall context relevancy:**\n   - The context perfectly answers the question by providing the exact architecture information needed. There are no gaps or missing critical information.\n\nSCORE: 1.0",
      "8",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\n```\n\nContext: \n```\nIn Transformers version 4.18.0, model checkpoints that exceed 10 GB in size are automatically sharded into smaller pieces. Each shard will be less than 10 GB. You can control the maximum size before sharding using the `max_shard_size` parameter.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. **Identify the main information need(s) in the question:**\n   - The question asks for the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0.\n\n2. **Assess the context:**\n   - The context directly addresses the question by stating that in Transformers version 4.18.0, model checkpoints that exceed 10 GB in size are automatically sharded.\n   - It provides comprehensive information by specifying the threshold (10 GB) and mentioning that each shard will be less than 10 GB.\n   - There is no critical information missing, as the context also notes that the `max_shard_size` parameter can be used to control the maximum size before sharding, which is additional useful information.\n\n3. **Rate the overall context relevancy:**\n   - The context contains all the necessary information to fully answer the question, including the specific size limit and additional details about sharding. Therefore, it is perfectly relevant.\n\nSCORE: 1.0",
      "12",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\n```\n\nContext: \n```\nThe method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This method downloads and caches the configuration and vocabulary associated with the specified checkpoint. It automatically retrieves the relevant pipeline based on the name or path to the pretrained weights.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. **Identify the main information need(s) in the question:**\n   - The question asks for the specific method used to load a checkpoint for a task using `AutoPipeline`.\n\n2. **Assess the context:**\n   - The context directly answers the question by stating that the method used is `from_pretrained()`.\n   - It provides additional details about what the method does, such as downloading and caching the configuration and vocabulary, and automatically retrieving the relevant pipeline.\n   - The context is comprehensive in addressing the question, as it not only names the method but also explains its functionality.\n\n3. **Evaluate the overall context relevancy:**\n   - The context contains all the necessary information to fully answer the question. It directly names the method and provides a brief explanation of its purpose and functionality, which is exactly what the question seeks.\n\nGiven that the context perfectly addresses the question with no missing critical information, the context relevancy is rated as 1.0.\n\nSCORE: 1.0",
      "11",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\n```\n\nContext: \n```\nThe purpose of the Safety Checker in the Diffusers library is to check and compare class probabilities against a set of guidelines to ensure safety in deployment. It aims to guide the development and usage of the library by encouraging safe practices.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. **Identify the main information need(s) in the question:**\n   - The question asks about the purpose of the Safety Checker in the Diffusers library. The main information need is to understand what the Safety Checker is designed to do within this specific library.\n\n2. **Assess the context:**\n   - The context provided states that the purpose of the Safety Checker in the Diffusers library is to check and compare class probabilities against a set of guidelines to ensure safety in deployment. It also mentions that it aims to guide the development and usage of the library by encouraging safe practices.\n   - This context directly addresses the question by explaining the function of the Safety Checker, which is to ensure safety by checking class probabilities against guidelines.\n   - The context is comprehensive in that it provides a clear explanation of the Safety Checker's role and its broader aim of promoting safe practices in the library's development and usage.\n   - There does not appear to be any critical information missing for answering the question, as the context succinctly explains the purpose of the Safety Checker.\n\n3. **Rate the overall context relevancy:**\n   - The context is highly relevant and provides a complete answer to the question. It contains all the necessary information to understand the purpose of the Safety Checker in the Diffusers library.\n\nSCORE: 1.0",
      "3",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\n```\n\nContext: \n```\nThe method used to load a checkpoint for a task using `AutoPipeline` is the `from_pretrained()` method, which retrieves the relevant pipeline based on the name or path to the pretrained weights. Additionally, the `from_pipe()` method can be used to transfer components from the original pipeline to a new one without reallocating memory.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. **Identify the main information need(s) in the question:**\n   - The question asks for the method used to load a checkpoint for a task using `AutoPipeline`.\n\n2. **Assess the context:**\n   - The context states that the method used to load a checkpoint for a task using `AutoPipeline` is the `from_pretrained()` method. This directly answers the question by providing the specific method name.\n   - The context also mentions the `from_pipe()` method, which is related to transferring components between pipelines. While this is additional information, it is not directly relevant to the main question about loading a checkpoint.\n   - The context provides comprehensive information about the primary method (`from_pretrained()`) needed to answer the question. There is no critical information missing regarding the method used to load a checkpoint.\n\n3. **Rate the overall context relevancy:**\n   - The context contains all the necessary information to fully answer the question about the method used to load a checkpoint with `AutoPipeline`. Therefore, it is perfectly relevant.\n\nSCORE: 1.0",
      "10",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\n```\n\nContext: \n```\nTo load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method, which retrieves the relevant pipeline given the name or path to the pretrained weights. Additionally, to switch between tasks with the same checkpoint without reallocating memory, the `from_pipe()` method can be used to transfer components from the original pipeline to the new one.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. **Identify the main information need(s) in the question:**\n   - The question asks for the method used to load a checkpoint for a task using `AutoPipeline`.\n\n2. **Assess the context:**\n   - The context provides information about the `from_pretrained()` method, which is used to load a checkpoint for a task using `AutoPipeline`. This directly answers the question by specifying the method used.\n   - The context also mentions the `from_pipe()` method, which is related to switching tasks with the same checkpoint, but this is additional information and not directly required to answer the question.\n   - The context is comprehensive in addressing the main information need, as it clearly states the method (`from_pretrained()`) used for loading a checkpoint.\n\n3. **Evaluate the overall context relevancy:**\n   - The context contains all the necessary information to fully answer the question about the method used to load a checkpoint with `AutoPipeline`. There are no significant gaps in the information provided.\n\nBased on the above reasoning, the context is perfectly relevant to the question.\n\nSCORE: 1.0",
      "18",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\n```\n\nContext: \n```\nThe purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of hard-coded harmful concepts against generated images to mitigate inappropriate degeneration in diffusion models. It aims to prevent the generation of harmful content by ensuring that such concepts are not present in the output. The harmful concepts are intentionally hidden to protect the integrity of the checker.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. **Identify the main information need(s) in the question:**\n   - The question asks about the purpose of the Safety Checker in the Diffusers library. The main information need is to understand what the Safety Checker is designed to do within this library.\n\n2. **Assess the context:**\n   - **Relevance to the question:** The context directly addresses the purpose of the Safety Checker in the Diffusers library. It explains that the Safety Checker is used to check and compare the class probability of hard-coded harmful concepts against generated images. This is directly relevant to the question as it describes the function and intent of the Safety Checker.\n   - **Comprehensiveness of the information:** The context provides a comprehensive explanation of the Safety Checker's purpose. It mentions that the Safety Checker aims to mitigate inappropriate degeneration in diffusion models and prevent the generation of harmful content. It also notes that harmful concepts are intentionally hidden to protect the integrity of the checker.\n   - **Missing critical information:** There is no critical information missing in the context. It fully explains the purpose of the Safety Checker in the Diffusers library.\n\n3. **Rate the overall context relevancy:**\n   - The context is perfectly relevant as it contains all the necessary information to fully answer the question about the purpose of the Safety Checker in the Diffusers library.\n\nSCORE: 1.0",
      "17",
      "1. **Identify the main information need(s) in the question:**\n   - The question asks for the default checkpoint used by the sentiment analysis pipeline in the Transformers library. The main information needed is the specific name or identifier of the checkpoint.\n\n2. **Assess the context:**\n   - The context provided states: \"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n   - This context directly answers the question by providing the exact name of the default checkpoint used in the sentiment analysis pipeline.\n   - The information is comprehensive as it gives the full name of the checkpoint, which is the critical piece of information needed to answer the question.\n\n3. **Rate the overall context relevancy:**\n   - The context contains all the necessary information to fully answer the question without any gaps. It directly addresses the main information need by specifying the default checkpoint name.\n\nBased on the above evaluation, the context is perfectly relevant to the question.\n\nSCORE: 1.0",
      "13",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\n```\n\nContext: \n```\nThe purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of a set of guidelines to ensure safe usage of the models. It aims to mitigate issues associated with models like Stable Diffusion. This helps guide the development and contributions to the library while promoting ethical usage.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. **Identify the main information need(s) in the question:**\n   - The question asks about the purpose of the Safety Checker in the Diffusers library. The main information need is to understand what the Safety Checker is designed to do within the context of the Diffusers library.\n\n2. **Assess the context:**\n   - The context states that the purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of a set of guidelines to ensure safe usage of the models. It also mentions that it aims to mitigate issues associated with models like Stable Diffusion and helps guide development and contributions to the library while promoting ethical usage.\n   - This context directly addresses the question by explaining the function and goals of the Safety Checker, specifically in terms of ensuring safe and ethical usage of models, which is the core purpose of the Safety Checker.\n   - The context is comprehensive in that it covers the main aspects of the Safety Checker's purpose, including safety, ethical usage, and its role in guiding development.\n\n3. **Rate the overall context relevancy:**\n   - The context provides a complete and accurate explanation of the purpose of the Safety Checker in the Diffusers library. It does not appear to have significant gaps in the information needed to answer the question.\n\nBased on the above analysis, the context is perfectly relevant to the question.\n\nSCORE: 1.0",
      "19",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\n```\n\nContext: \n```\nThe purpose of the Safety Checker in the Diffusers library is to mitigate inappropriate degeneration in diffusion models by checking and comparing the class probability of harmful concepts against generated images. It aims to prevent the generation of harmful content by using hard-coded harmful concepts that are hidden to avoid reverse engineering. This feature is part of the broader effort to ensure responsible and safe deployment of AI technologies.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. **Identify the main information need(s) in the question:**\n   - The question asks about the purpose of the Safety Checker in the Diffusers library. The main information need is an explanation of what the Safety Checker is designed to do within the context of the Diffusers library.\n\n2. **Assess the context:**\n   - The context provided directly addresses the question by explaining that the purpose of the Safety Checker is to mitigate inappropriate degeneration in diffusion models. It does this by checking and comparing the class probability of harmful concepts against generated images.\n   - The context further elaborates that the Safety Checker aims to prevent the generation of harmful content and mentions the use of hard-coded harmful concepts to avoid reverse engineering.\n   - Additionally, it places the Safety Checker within the broader effort to ensure responsible and safe deployment of AI technologies.\n\n3. **Evaluate comprehensiveness and missing information:**\n   - The context is comprehensive in explaining the purpose of the Safety Checker. It covers the main function (mitigating inappropriate degeneration), the method (checking class probabilities), and the broader goal (ensuring responsible AI deployment).\n   - There is no critical information missing for understanding the purpose of the Safety Checker as per the question's requirements.\n\nBased on the above analysis, the context is perfectly relevant to the question, providing a complete and accurate explanation of the Safety Checker's purpose.\n\nSCORE: 1.0",
      "15",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the faithfulness of a generated answer based solely on the provided context.\n\nFaithfulness measures whether the generated answer contains only information that is supported by the provided context. An answer is considered faithful if all its factual claims are directly supported by or can be reasonably inferred from the context\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the context (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the context (assign 0 points)\n3. Calculate the faithfulness score as: (sum of points) / (total number of claims)\n\nReturn a final faithfulness score between 0 and 1, where:\n- 1.0: Perfect faithfulness (all claims fully supported)\n- 0.0: Complete unfaithfulness (no claims supported by context)\n\nProvided Context:  `tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n\n\nAnswer: \n```\nThe `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on Linux using the musl C library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "Let's evaluate the factual claims in the generated answer against the provided context.\n\n1. **Claim**: \"The `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture\"\n   - **Analysis**: The context provides \"x86_64\" as part of the description for the binary, which means it is indeed designed for the x86_64 architecture.\n   - **Score**: 1 point (Fully supported)\n\n2. **Claim**: \"on Linux\"\n   - **Analysis**: The context refers to \"linux\" as part of the identifier `tokenizers-linux-x64-musl`, implying the binary is for Linux.\n   - **Score**: 1 point (Fully supported)\n\n3. **Claim**: \"using the musl C library\"\n   - **Analysis**: The context mentions \"musl\" in \"x86_64-unknown-linux-musl\", which is indicative of the musl C library usage.\n   - **Score**: 1 point (Fully supported)\n\nEach of the claims in the answer is fully supported by the context provided. \n\nTotal points = 3 (1 point for each of the 3 claims)\nTotal claims = 3\n\nFaithfulness score = Total points / Total claims = 3 / 3 = 1.0\n\nThis gives us a perfect faithfulness score since all the claims are fully supported by the given context.\n\nSCORE: 1.0",
      "1. **Identify the main information need(s) in the question:**\n   - The question asks for the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0.\n\n2. **Assess the context:**\n   - The context directly addresses the question by stating that in Transformers version 4.18.0, model checkpoints that exceed 10 GB in size are automatically sharded.\n   - It provides comprehensive information by specifying the size limit (10 GB) and mentioning that each shard will be less than 10 GB.\n   - The context also adds extra information about the `max_shard_size` parameter, which allows control over the maximum size before sharding, although this is not directly asked in the question.\n\n3. **Rate the overall context relevancy:**\n   - The context contains all the necessary information to fully answer the question, as it directly states the maximum size (10 GB) before automatic sharding occurs in the specified version of Transformers.\n\nSCORE: 1.0",
      "14",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the faithfulness of a generated answer based solely on the provided context.\n\nFaithfulness measures whether the generated answer contains only information that is supported by the provided context. An answer is considered faithful if all its factual claims are directly supported by or can be reasonably inferred from the context\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the context (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the context (assign 0 points)\n3. Calculate the faithfulness score as: (sum of points) / (total number of claims)\n\nReturn a final faithfulness score between 0 and 1, where:\n- 1.0: Perfect faithfulness (all claims fully supported)\n- 0.0: Complete unfaithfulness (no claims supported by context)\n\nProvided Context: !--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\nWe will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n\n\n## Scope\n\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n\n\n## Ethical guidelines\n\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n\n- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâ€™s authors to have more control over its use.\n\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n\n\nAnswer: \n```\nThe purpose of the Safety Checker in the Diffusers library is to check and compare class probabilities against a set of guidelines to ensure safety in deployment. It aims to guide the development and usage of the library by encouraging safe practices.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "Let's evaluate the claims in the generated answer:\n\n1. **Claim**: \"The purpose of the Safety Checker in the Diffusers library is to check and compare class probabilities against a set of guidelines to ensure safety in deployment.\"\n\n   - **Supported Content**: The context describes the Safety Checker as a tool that \"checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated.\" The context does not specifically mention \"guidelines\" but talks about \"class probability\" and \"harmful concepts.\"\n   - **Evaluation**: The phrase \"against a set of guidelines\" is an extrapolation since the context refers specifically to \"hard-coded harmful concepts\" rather than \"guidelines.\" However, the essence of ensuring \"safety in deployment\" aligns with the context's intention to mitigate inappropriate content. Therefore, it's partially supported.\n   - **Score**: 0.5 points\n\n2. **Claim**: \"It aims to guide the development and usage of the library by encouraging safe practices.\"\n\n   - **Supported Content**: The context mentions that the community takes \"collective responsibility\" for anticipating and \"mitigating this technology's potential risks\" and an emphasis on safety through various mechanisms, but does not directly link these general aims to the Safety Checker in particular.\n   - **Evaluation**: This is an extrapolation as the context provides a purpose of the Safety Checker without stating explicitly the intention to \"guide development and usage\" directly through it. The idea of safe practices is part of the whole context but making a strong link to the Safety Checker specifically is a bit of an inference.\n   - **Score**: 0.5 points\n\nCalculating the faithfulness score:\n- Total points: 0.5 (first claim) + 0.5 (second claim) = 1.0\n- Number of claims: 2\n\nFaithfulness Score = Total Points / Number of Claims = 1.0 / 2 = 0.5\n\nSCORE: 0.5",
      "1. **Identify the main information need(s) in the question:**\n   The question asks for the specific default checkpoint used by the sentiment analysis pipeline in the Transformers library. The main information need is the name of this default checkpoint.\n\n2. **Assess the context:**\n   - The context states: \"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n   - This directly answers the question by providing the exact name of the default checkpoint.\n   - The information is comprehensive as it fully addresses the question without any gaps or missing details.\n\n3. **Rate the overall context relevancy:**\n   Since the context provides the exact answer needed to respond to the question, it is perfectly relevant.\n\nSCORE: 1.0",
      "1. **Identify the main information need(s) in the question:**\n   The question asks for the specific default checkpoint used by the sentiment analysis pipeline in the Transformers library. The main information need is the name of this default checkpoint.\n\n2. **Assess the context:**\n   - The context provided states: \"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n   - This directly answers the question by providing the exact name of the default checkpoint.\n   - The information is comprehensive as it fully addresses the question without any gaps or missing details.\n\n3. **Rate the overall context relevancy:**\n   Since the context contains all the necessary information to fully answer the question, it is perfectly relevant.\n\nSCORE: 1.0",
      "1. **Identify the main information need(s) in the question:**\n   The question asks specifically about the purpose of the Safety Checker in the Diffusers library. The main information need is to understand what the Safety Checker does and its role in ensuring safety during the use of the library.\n\n2. **Assess the context:**\n   - The context provided directly addresses the purpose of the Safety Checker by stating that it checks and compares class probabilities against guidelines to ensure safety in deployment. This is directly relevant to the question.\n   - The information is comprehensive in that it explains both the checking mechanism (comparing class probabilities) and the broader aim (to guide development and usage by encouraging safe practices).\n   - There does not appear to be critical information missing; the context sufficiently explains the purpose of the Safety Checker.\n\n3. **Rate the overall context relevancy:**\n   Given that the context provides a clear and complete explanation of the Safety Checkerâ€™s purpose, I would rate the overall context relevancy as 1.0.\n\nSCORE: 1.0",
      "1. **Identify the main information need(s) in the question**: The question seeks to find out the maximum size of a model checkpoint that triggers automatic sharding in Transformers version 4.18.0.\n\n2. **Assess the context**:\n   - The context states: \"The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB.\"\n   - This directly answers the question by providing the specific information requested: the maximum size (10GB) and the version of Transformers (4.18.0) relevant to the question.\n\n3. **Evaluation of context relevancy**:\n   - The context contains information that is directly relevant to answering the question.\n   - It is comprehensive as it provides the exact figure (10GB) and specifies the version (4.18.0), which are both critical to fully answering the question.\n   - There is no critical information missing; the context fully addresses the question.\n\nGiven this analysis, the context is perfectly relevant to the question.\n\nSCORE: 1.0",
      "1. **Identify the main information need(s) in the question:**\n   The question asks for the specific default checkpoint used by the sentiment analysis pipeline in the Transformers library. The main information need is the name of this default checkpoint.\n\n2. **Assess the context:**\n   - The context directly states that the default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst2-english.\"\n   - This information is comprehensive as it provides the exact name of the checkpoint, which is the answer to the question.\n   - There is no critical information missing, as the context fully addresses the question.\n\n3. **Rate the overall context relevancy:**\n   Since the context provides the exact answer needed to respond to the question without any gaps or missing information, it is perfectly relevant.\n\nSCORE: 1.0",
      "1. **Identify the main information need(s) in the question:**\n   The question asks specifically about the architecture for which the `tokenizers-linux-x64-musl` binary is designed. The main information need is to identify the architecture type (e.g., x86_64, ARM, etc.) associated with this binary.\n\n2. **Assess the context:**\n   - The context states that the `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on Linux using the musl C library.\n   - This directly answers the question by providing the specific architecture (x86_64) that the binary is designed for.\n   - The information is comprehensive as it not only identifies the architecture but also mentions the operating system (Linux) and the C library (musl), which adds context but is not strictly necessary to answer the question.\n   - There is no critical information missing regarding the architecture itself, as the context fully addresses the main information need.\n\n3. **Rate the overall context relevancy:**\n   Since the context provides a complete and accurate answer to the question regarding the architecture of the binary, it is highly relevant.\n\nSCORE: 1.0",
      "1. **Identify the main information need(s) in the question:**\n   The question asks for the specific default checkpoint used by the sentiment analysis pipeline in the Transformers library. The main information need is the name of this default checkpoint.\n\n2. **Assess the context:**\n   - The context directly states that the default checkpoint is \"distilbert-base-uncased-finetuned-sst2-english.\" This is exactly the information needed to answer the question.\n   - The information provided is comprehensive as it includes the full name of the checkpoint, which is sufficient to fully answer the question.\n   - There is no critical information missing, as the context provides the exact answer required.\n\n3. **Rate the overall context relevancy:**\n   Since the context contains all the necessary information to fully answer the question without any gaps or missing details, it is perfectly relevant.\n\nSCORE: 1.0",
      "1. **Identify the main information need(s) in the question**: The question seeks to find out the maximum size of a model checkpoint that triggers automatic sharding in Transformers version 4.18.0.\n\n2. **Assess the context**:\n   - The context directly states, \"The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB.\" This directly answers the question by providing the specific size limit (10GB) and the relevant version of Transformers (4.18.0).\n   - The information is comprehensive as it fully addresses the question without any ambiguity or missing details.\n\n3. **Rate the overall context relevancy**: Since the context provides a complete and accurate answer to the question without any gaps or missing information, it is perfectly relevant.\n\nSCORE: 1.0",
      "To evaluate the faithfulness of the generated answer, we need to identify the factual claims made in the answer and assess them against the provided context.\n\n1. **Claim**: \"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n\nNow, we will check if this claim is supported by the context:\n\n- The context states: \"the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english.\" \n\nThis statement in the context directly supports the claim made in the answer. The checkpoint name is correctly quoted, and the context confirms that this is indeed the default checkpoint for the sentiment analysis pipeline.\n\nSince the claim is fully supported by the context, we assign it a score of 1 point.\n\nNow, we calculate the faithfulness score:\n\n- Total claims: 1\n- Points awarded: 1\n\nFaithfulness score = (sum of points) / (total number of claims) = 1 / 1 = 1.0\n\nThus, the final faithfulness score is:\n\nSCORE: 1.0",
      "1. **Identify the main information need(s) in the question:**\n   The question asks specifically about the purpose of the Safety Checker in the Diffusers library. The main information need is to understand what the Safety Checker does and its role within the context of the Diffusers library.\n\n2. **Assess the context:**\n   - The context directly addresses the purpose of the Safety Checker by stating that it is designed to mitigate inappropriate degeneration in diffusion models and prevent the generation of harmful content. This is highly relevant to the question.\n   - The context provides comprehensive information about how the Safety Checker operates (checking and comparing class probabilities of harmful concepts against generated images) and mentions the use of hard-coded harmful concepts to avoid reverse engineering. This adds depth to the understanding of its purpose.\n   - There does not appear to be critical information missing; the context sufficiently explains the Safety Checkerâ€™s function and its importance in ensuring responsible AI deployment.\n\n3. **Rate the overall context relevancy:**\n   Given that the context fully addresses the question about the purpose of the Safety Checker, providing both a clear explanation and additional details about its operation, I would rate the overall context relevancy as perfect.\n\nSCORE: 1.0",
      "1. The main information need in the question is to identify the specific architecture that the `tokenizers-linux-x64-musl` binary is designed for.\n\n2. Assessing the context:\n   - The context states that the `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on Linux with the musl C library. This directly answers the question by specifying the architecture (x86_64) that the binary is designed for.\n   - The relevant information is comprehensive as it not only identifies the architecture but also provides additional context about the operating system (Linux) and the C library (musl) used, which may be useful for understanding the environment in which the binary operates.\n   - There is no critical information missing for answering the question, as the architecture is clearly stated.\n\n3. Based on the assessment, the context provides all necessary information to fully answer the question regarding the architecture of the binary.\n\nSCORE: 1.0",
      "To evaluate the faithfulness of the generated answer, we need to identify the factual claims made in the answer and assess them against the provided context.\n\n1. **Claim**: \"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n\nNow, we will check if this claim is supported by the context:\n\n- The context states: \"the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english.\" \n\nThis statement in the context directly supports the claim made in the answer. The checkpoint name is the same, and it is explicitly mentioned as the default for the sentiment analysis pipeline.\n\nSince the claim is fully supported by the context, we assign it a score of 1 point.\n\nNow, we calculate the faithfulness score:\n\n- Total number of claims: 1\n- Points awarded: 1\n\nFaithfulness score = (sum of points) / (total number of claims) = 1 / 1 = 1.0\n\nThus, the final faithfulness score is:\n\nSCORE: 1.0",
      "To evaluate the faithfulness of the generated answer, I will identify the factual claims made in the answer and assess their support based on the provided context.\n\n1. **Claim 1**: \"The `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture.\"\n   - This claim is fully supported by the context, which states that the binary is for `x86_64-unknown-linux-musl`. Therefore, this claim is accurate.\n   - Score: 1 point\n\n2. **Claim 2**: \"The `tokenizers-linux-x64-musl` binary is designed for Linux.\"\n   - This claim is also fully supported by the context, as it explicitly mentions that the binary is for Linux.\n   - Score: 1 point\n\n3. **Claim 3**: \"The `tokenizers-linux-x64-musl` binary is designed using the musl C library.\"\n   - This claim is fully supported by the context, which includes \"musl\" in the name of the binary, indicating that it is indeed associated with the musl C library.\n   - Score: 1 point\n\nNow, I will calculate the faithfulness score:\n- Total claims: 3\n- Points scored: 3 (1 + 1 + 1)\n\nFaithfulness score = (sum of points) / (total number of claims) = 3 / 3 = 1.0\n\nThe generated answer is completely faithful to the provided context, as all claims are fully supported.\n\nFinal answer:\nSCORE: 1.0",
      "1. **Identify the main information need(s) in the question:**\n   The question asks specifically about the method used to load a checkpoint for a task using `AutoPipeline`. The main information need is to identify the exact method and any relevant details about its functionality.\n\n2. **Assess the context:**\n   - The context directly states that the method used to load a checkpoint is `from_pretrained()`. This is exactly what the question is asking for.\n   - The context also provides additional information about what the method does, mentioning that it downloads and caches the configuration and vocabulary associated with the specified checkpoint, and that it retrieves the relevant pipeline based on the name or path to the pretrained weights. This additional information enhances the understanding of the method's functionality.\n   - There is no critical information missing that would prevent someone from understanding how to load a checkpoint using `AutoPipeline`.\n\n3. **Rate the overall context relevancy:**\n   Given that the context provides a clear and direct answer to the question, along with useful additional details, it is highly relevant.\n\nSCORE: 1.0",
      "1. **Identify the main information need(s) in the question:**\n   - The question asks specifically about the purpose of the Safety Checker in the Diffusers library. The main information need is to understand what the Safety Checker does and its role in the context of the Diffusers library.\n\n2. **Assess the context:**\n   - The context provided directly addresses the purpose of the Safety Checker by stating that it checks and compares class probabilities of harmful concepts against generated images. This is directly relevant to the question as it explains the function of the Safety Checker.\n   - The context is comprehensive in that it describes the aim of the Safety Checker, which is to mitigate inappropriate degeneration in diffusion models and prevent the generation of harmful content. It also mentions the intentional hiding of harmful concepts to protect the integrity of the checker, which adds depth to the explanation.\n   - There does not appear to be critical information missing; the context sufficiently explains the purpose and function of the Safety Checker.\n\n3. **Rate the overall context relevancy:**\n   - Given that the context provides a complete and accurate description of the purpose of the Safety Checker, I would rate the overall context relevancy as 1.0.\n\nSCORE: 1.0",
      "1. **Identify the main information need(s) in the question:**\n   The question asks specifically about the method used to load a checkpoint for a task using `AutoPipeline`. The main information need is to identify the correct method or methods associated with this process.\n\n2. **Assess the context:**\n   - The context states that the method used to load a checkpoint is the `from_pretrained()` method, which is directly relevant to the question. This method is explicitly mentioned as the primary way to load a checkpoint.\n   - The context also mentions the `from_pipe()` method, which, while related to transferring components, does not directly answer the question about loading a checkpoint. However, it provides additional context about the functionality of `AutoPipeline`.\n   - The information provided is comprehensive regarding the primary method (`from_pretrained()`), but the mention of `from_pipe()` could be seen as somewhat tangential since it does not pertain directly to loading a checkpoint.\n\n3. **Rate the overall context relevancy:**\n   The context provides the necessary information to answer the question about loading a checkpoint using `AutoPipeline`, specifically naming the `from_pretrained()` method. While it includes some additional information about `from_pipe()`, this does not detract from the relevance of the main answer. Therefore, the context is highly relevant.\n\nSCORE: 1.0",
      "1. **Identify the main information need(s) in the question:**\n   The question asks specifically about the method used to load a checkpoint for a task using `AutoPipeline`. The main information need is to identify the method or methods that facilitate this action.\n\n2. **Assess the context:**\n   - The context provides the `from_pretrained()` method as a way to load a checkpoint, which directly answers the question. It explains that this method retrieves the relevant pipeline based on the name or path to the pretrained weights, which is crucial information for understanding how to load a checkpoint.\n   - The context also mentions the `from_pipe()` method, which, while not directly related to loading a checkpoint, adds additional information about transferring components between pipelines. However, this is not necessary for answering the main question.\n   - Overall, the context is comprehensive regarding the primary method (`from_pretrained()`) needed to answer the question, and it does not omit any critical information regarding this method.\n\n3. **Rate the overall context relevancy:**\n   Since the context provides a clear and direct answer to the question about the method used to load a checkpoint, and it includes sufficient detail about the method, I would rate the overall context relevancy as perfect.\n\nSCORE: 1.0",
      "1. **Identify the main information need(s) in the question:**\n   - The question asks for the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0.\n\n2. **Assess the context:**\n   - The context states that model checkpoints exceeding 10 GB in size are automatically sharded into smaller pieces, with each shard being less than 10 GB. It also mentions the `max_shard_size` parameter, which allows control over the maximum size before sharding.\n\n   - **Relevance to the question:**\n     - The context directly answers the question by specifying that the maximum size before automatic sharding is 10 GB.\n   \n   - **Comprehensiveness of the relevant information:**\n     - The context provides a clear and concise answer to the question, including the specific threshold (10 GB) and additional information about the sharding process and the `max_shard_size` parameter.\n\n   - **Missing critical information:**\n     - There is no critical information missing; the context fully addresses the question regarding the maximum size for sharding.\n\n3. **Rate the overall context relevancy:**\n   - Given that the context provides a complete and accurate answer to the question, I would rate the overall context relevancy as 1.0.\n\nFinal answer:\nSCORE: 1.0",
      "1. **Identify the main information need(s) in the question:**\n   The question asks specifically about the method used to load a checkpoint for a task using `AutoPipeline`. The main information need is to identify the correct method or methods that facilitate this action.\n\n2. **Assess the context:**\n   - The context provides the `from_pretrained()` method as a way to load a checkpoint, which directly answers the question. This is the primary method the question is inquiring about.\n   - It also mentions the `from_pipe()` method, which is relevant in the context of switching tasks with the same checkpoint, but it is not the primary focus of the question. However, it adds useful information about managing checkpoints and memory.\n   - The context does not appear to be missing any critical information regarding the loading of a checkpoint, as it clearly states the method to be used.\n\n3. **Rate the overall context relevancy:**\n   The context is highly relevant because it directly answers the question about the method used to load a checkpoint and provides additional useful information about task switching. Therefore, it contains all necessary information to fully answer the question.\n\nSCORE: 1.0",
      "1. **Identify the main information need(s) in the question:**\n   The question asks specifically about the architecture for which the `tokenizers-linux-x64-musl` binary is designed. The main information need is to identify the architecture type (e.g., x86_64, ARM, etc.) associated with this binary.\n\n2. **Assess the context:**\n   - The context states that the `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on Linux with the musl C library.\n   - This directly answers the question by specifying that the architecture is x86_64.\n   - The information is comprehensive as it not only identifies the architecture but also provides additional context about the operating system (Linux) and the C library (musl), which may be relevant for understanding the environment in which the binary operates.\n   - There is no critical information missing that would prevent someone from understanding the architecture for which the binary is designed.\n\n3. **Rate the overall context relevancy:**\n   Since the context provides a complete and accurate answer to the question regarding the architecture of the `tokenizers-linux-x64-musl` binary, I would rate the relevancy as perfect.\n\nSCORE: 1.0",
      "1. **Identify the main information need(s) in the question:**\n   The question asks specifically about the purpose of the Safety Checker in the Diffusers library. The main information need is to understand what the Safety Checker does and its role in ensuring safe usage of the models.\n\n2. **Assess the context:**\n   - **Relevance:** The context directly addresses the purpose of the Safety Checker by stating that it checks and compares class probabilities to ensure safe usage of the models. This is directly relevant to the question.\n   - **Comprehensiveness:** The context provides a clear explanation of the Safety Checkerâ€™s function and its aim to mitigate issues associated with models like Stable Diffusion. It also mentions the broader implications for ethical usage and development contributions, which adds depth to the understanding of its purpose.\n   - **Critical Information:** The context does not appear to be missing any critical information necessary to answer the question. It succinctly covers the main aspects of the Safety Checkerâ€™s purpose.\n\n3. **Overall context relevancy rating:**\n   Given that the context provides a complete and accurate description of the Safety Checkerâ€™s purpose without any significant gaps, it warrants a high relevancy score.\n\nSCORE: 1.0",
      "1. **Identify the main information need(s) in the question:**\n   - The question asks for the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0.\n\n2. **Assess the context:**\n   - The context states that model checkpoints exceeding 10 GB in size are automatically sharded into smaller pieces, with each shard being less than 10 GB. It also mentions the `max_shard_size` parameter, which allows control over the maximum size before sharding.\n\n   - **Relevance to the question:**\n     - The context directly answers the question by specifying that the maximum size before automatic sharding is 10 GB.\n   \n   - **Comprehensiveness of the relevant information:**\n     - The context provides a clear and concise answer to the question, including the specific threshold (10 GB) and additional information about the sharding process and the `max_shard_size` parameter, which could be useful for further understanding.\n\n   - **Missing critical information:**\n     - There is no critical information missing; the context fully addresses the question regarding the maximum size for sharding.\n\n3. **Rate the overall context relevancy:**\n   - Given that the context provides a complete and accurate answer to the question, I would rate the overall context relevancy as 1.0.\n\nFinal answer:\nSCORE: 1.0",
      "To evaluate the faithfulness of the generated answer, I will identify the factual claims made in the answer and assess their support based on the provided context.\n\n### Factual Claims in the Answer:\n1. **Claim 1**: \"The purpose of the Safety Checker in the Diffusers library is to check and compare class probabilities against a set of guidelines to ensure safety in deployment.\"\n2. **Claim 2**: \"It aims to guide the development and usage of the library by encouraging safe practices.\"\n\n### Evaluation of Claims:\n1. **Claim 1**: This claim is partially supported by the context. The context states that the Safety Checker \"checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated.\" While it does mention checking and comparing probabilities, it does not explicitly state that this is done against a \"set of guidelines.\" Therefore, this claim requires some extrapolation and is not fully supported.\n   - Score: 0.5 points\n\n2. **Claim 2**: This claim is also partially supported. The context mentions that the Safety Checker is part of the efforts to \"encourage safety in deployment,\" which aligns with the idea of guiding development and usage by encouraging safe practices. However, the phrasing \"by encouraging safe practices\" is not directly stated in the context, so it requires some interpretation.\n   - Score: 0.5 points\n\n### Calculation of Faithfulness Score:\n- Total points = 0.5 (Claim 1) + 0.5 (Claim 2) = 1.0\n- Total number of claims = 2\n- Faithfulness Score = Total points / Total number of claims = 1.0 / 2 = 0.5\n\nThus, the final faithfulness score is:\n\nSCORE: 0.5",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\nContext: Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional\n\nattention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should\n\nclass. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after\nAnswer: ",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\nContext: the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nhat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the\n\nlibrary. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in\n\n### HF_HUB_DISABLE_TELEMETRY\r\n\r\nBy default, some data is collected by HF libraries (`transformers`, `datasets`, `gradio`,..) to monitor usage, debug issues and help prioritize features.\nAnswer: ",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\nContext: Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do\n\n```py\r\n>>> from transformers.modeling_utils import load_sharded_checkpoint\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n\nfor a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the\nAnswer: ",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\nContext: - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of\n\nguidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nðŸ§¨ Diffusers Experimental\r\n\r\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\r\nCurrently, the following experiments are supported:\n\nhat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the\n\n- **Encouraging safety in deployment**\nAnswer: ",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\nContext: `tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n\n-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\non linux machines. \r\n\r\nIf `HF_HOME` is not set, the default home will be `\"$XDG_CACHE_HOME/huggingface\"` instead\r\nof `\"~/.cache/huggingface\"`.\n\n[on macOS and Linux](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/)\r\nand on [Windows](https://phoenixnap.com/kb/windows-set-environment-variable).\n\nattention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the\nAnswer: ",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\nContext: -->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should\n\nthe checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\nAnswer: ",
      "The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This method downloads and caches the configuration and vocabulary associated with the specified checkpoint. It allows for easy loading of the relevant pipeline without needing to know the specific class to use.",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\nContext: the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nlibrary. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in\n\nsteps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage,\n\nits unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,\nAnswer: ",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\nContext: `tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n\nlibrary, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or\n\ntokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning\n\nits unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,\n\nNow let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well\nAnswer: ",
      "The `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture. It is specifically built for the unknown Linux environment using the musl C library.",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\nContext: ` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `\n\nhf _ home ` is not set! this is the default way to configure where [ user - specific non - essential ( cached ) data should be written ] ( https : / / wiki. archlinux. org / title / xdg _ base _ directory ) on linux machines. if ` hf _ home ` is not set, the default home will be ` \" $ xdg _ cache _ home / huggingface \" ` instead of ` \" ~ /. cache / huggingface \" `.\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\n. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size\n\n! - - note that this file is in markdown but contain specific syntax for our doc - builder ( similar to mdx ) that may not be rendered properly in your markdown viewer. - - > # environment variables ` huggingface _ hub ` can be configured using environment variables. if you are unfamiliar with environment variable, here are generic articles about them [ on macos and linux ] ( https : / / linuxize. com / post / how - to - set - and - list - environment - variables - in - linux / ) and on [ windows ] ( https : / / phoenixnap. com / kb / windows - set - environment - variable ). this page will guide you through all environment variables specific to ` huggingface _ hub ` and their meaning. # # generic # # # hf _ inference _ endpoint to configure the inference api base url. you might want to set this variable if your organization is pointing at an api gateway rather\nAnswer: ",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\nContext: -->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nfor a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n- from_pipe\r\n\r\n## AutoPipelineForInpainting\r\n\r\n[[autodoc]] AutoPipelineForInpainting\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe\nAnswer: ",
      "The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This method automatically retrieves the relevant pipeline based on the name or path to the pretrained weights. Additionally, `from_pipe()` can be used to transfer components from the original pipeline to a new one without reallocating memory.",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\nContext: ` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `\n\nboth variants regardless. this change will improve performance for many applications. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5216 ] ( https : / / github. com / gradio - app / gradio / pull / 5216 ) [ ` 4b58ea6d ` ] ( https : / / github. com / gradio - app / gradio / commit / 4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8 ) - update i18n tokens and locale files. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )! # # 0. 2. 0 # # # features - [ # 5025 ] ( https : / / github. com / gradio -\n\nthe first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\n. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\nAnswer: ",
      "The `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on a Linux system using the musl C library.",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\nContext: the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nto a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all\n\nremoved. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. to get an output linked to our classification problem, we need to use the automodelforsequenceclassification class. it works exactly as the automodel class, except that it will build a model with a classification head. there is one auto class for each common nlp task in the transformers library. here, after giving our model the two sentences, we get a tensor of size two by two : one result for each sentence and for each possible label. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third\n\n. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs.\nAnswer: ",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\nContext: slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\ntraining, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when\n\nsize of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] { ' total _ size ' : 433245184 } ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] { ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre\n\n... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )\nAnswer: ",
      "In Transformers version 4.18.0, model checkpoints that exceed 10 GB in size are automatically sharded into smaller pieces. Each of these smaller pieces will be less than 10 GB. You can control the maximum size before sharding using the `max_shard_size` parameter.",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\nContext: the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nto a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all\n\n##try by default, some data is collected by hf libraries ( ` transformers `, ` datasets `, ` gradio `,.. ) to monitor usage, debug issues and help prioritize features. each library defines its own policy ( i. e. which usage to monitor ) but the core implementation happens in ` huggingface _ hub ` ( see [ ` send _ telemetry ` ] ). you can set ` hf _ hub _ disable _ telemetry = 1 ` as environment variable to globally disable telemetry. # # # hf _ hub _ enable _ hf _ transfer set to ` true ` for faster uploads and downloads from the hub using ` hf _ transfer `. by default, ` huggingface _ hub ` uses the python - based ` requests. get ` and ` requests. post ` functions. although these are reliable and versatile, they may not be the most efficient choice for machines with high\n\n. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs.\nAnswer: ",
      "The default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst2-english.\" This checkpoint is associated with a tokenizer that processes the input sentences for sentiment classification.",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\nContext: slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~\n\ntraining, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\n... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )\n\nsize of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] { ' total _ size ' : 433245184 } ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] { ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre\nAnswer: ",
      "The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10 GB. Checkpoints larger than this size will be split into smaller pieces. You can control the maximum size before sharding using the `max_shard_size` parameter.",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\nContext: to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\nthe first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\n:... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir ) ` ` ` the main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in ram to the model size plus the size of the biggest shard. behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. we can load that index like any json and get a dictionary : ` ` ` py > > > import json > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ sha\nAnswer: ",
      "To load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method, which automatically retrieves the relevant pipeline given the name or path to the pretrained weights. This method allows you to load a checkpoint without needing to know the specific pipeline class. Additionally, you can use the `from_pipe()` method to transfer components between pipelines without reallocating memory.",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\nContext: ! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\nto load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,\n\nautodoc ] ] autopipelineforimage2image - all - from _ pretrained - from _ pipe # # autopipelineforinpainting [ [ autodoc ] ] autopipelineforinpainting - all - from _ pretrained - from _ pipe\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\n. bin ', ' pytorch _ model - 00002 - of - 00003. bin ', ' pytorch _ model - 00003 - of - 00003. bin ', ' pytorch _ model. bin. index. json ' ] ` ` ` on top of the configuration of the model, we see three different weights files, and an ` index. json ` file which is our index. a checkpoint like this can be fully reloaded using the [ ` ~ pretrainedmodel. from _ pretrained ` ] method : ` ` ` py > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir\nAnswer: ",
      "To load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method, which automatically retrieves the relevant pipeline given the name or path to the pretrained weights. Additionally, to switch between tasks with the same checkpoint without reallocating memory, the `from_pipe()` method can be used to transfer components from the original pipeline to the new one.",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\nContext: diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https\n\nsame level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and\n\n' s responsiveness and valuable feedback. # # scope the diffusers community will apply the following ethical guidelines to the project â€™ s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns. # # ethical guidelines the following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question. - * * transparency * * : we are committed to being transparent in managing prs, explaining our choices to users, and making technical decisions. - * * consistency * * : we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # diffusers â€™ ethical guidelines # # preamble [ diffusers ] ( https : / / huggingface. co / docs / diffusers / index ) provides pre - trained diffusion models and serves as a modular toolbox for inference and training. given its real case applications in the world and potential negative impacts on society, we think it\n\ndiffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model.\nAnswer: ",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\nContext: diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https\n\nhttps : / / huggingface. co / spaces / society - ethics / diffusionbiasexplorer ) to demonstrate the biases in stable diffusion interactively. in this sense, we support and encourage bias explorers and evaluations. - * * encouraging safety in deployment * * - [ * * safe stable diffusion * * ] ( https : / / huggingface. co / docs / diffusers / main / en / api / pipelines / stable _ diffusion / stable _ diffusion _ safe ) : it mitigates the well - known issue that models, like stable diffusion, that are trained on unfiltered, web - crawled datasets tend to suffer from inappropriate degeneration. related paper : [ safe latent diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / gi\n\nsame level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and\n\ndiffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\nAnswer: ",
      "The purpose of the Safety Checker in the Diffusers library is to mitigate inappropriate degeneration in diffusion models by checking and comparing the class probability of harmful concepts against generated images. It uses hard-coded harmful concepts that are intentionally hidden to prevent reverse engineering. This mechanism helps ensure safer deployment of models like stable diffusion, which may be trained on unfiltered datasets.",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If the question cannot be answered solely based on the context, just say so. Use three sentences maximum and keep the answer concise\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\nContext: - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of\n\nguidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\n-->\r\n\r\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\r\n\r\n## Preamble\n\n- [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\nAnswer: ",
      "The purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of a set of guidelines to ensure safe usage of the models. It aims to mitigate issues associated with models like Stable Diffusion. This contributes to the ethical development and use of the library.",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the faithfulness of a generated answer based solely on the provided context.\n\nFaithfulness measures whether the generated answer contains only information that is supported by the provided context. An answer is considered faithful if all its factual claims are directly supported by or can be reasonably inferred from the context\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the context (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the context (assign 0 points)\n3. Calculate the faithfulness score as: (sum of points) / (total number of claims)\n\nReturn a final faithfulness score between 0 and 1, where:\n- 1.0: Perfect faithfulness (all claims fully supported)\n- 0.0: Complete unfaithfulness (no claims supported by context)\n\nProvided Context: Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional\n\nattention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should\n\nclass. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after\n\nAnswer: \n```\nThe maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the faithfulness of the generated answer, we will identify the factual claims made in the answer and assess their support based on the provided context.\n\n**Factual Claim:**\n1. \"The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB.\"\n\n**Assessment of the Claim:**\n- The context states: \"model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces.\" This implies that the threshold for sharding is indeed 10GB. Therefore, the claim made in the answer is directly supported by the context.\n\nSince the claim is fully supported by the context, we assign it 1 point.\n\n**Calculation of the Faithfulness Score:**\n- Total number of claims: 1\n- Points assigned: 1\n\nFaithfulness Score = (sum of points) / (total number of claims) = 1 / 1 = 1.0\n\nThus, the final faithfulness score is:\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the faithfulness of a generated answer based solely on the provided context.\n\nFaithfulness measures whether the generated answer contains only information that is supported by the provided context. An answer is considered faithful if all its factual claims are directly supported by or can be reasonably inferred from the context\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the context (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the context (assign 0 points)\n3. Calculate the faithfulness score as: (sum of points) / (total number of claims)\n\nReturn a final faithfulness score between 0 and 1, where:\n- 1.0: Perfect faithfulness (all claims fully supported)\n- 0.0: Complete unfaithfulness (no claims supported by context)\n\nProvided Context: Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do\n\n```py\r\n>>> from transformers.modeling_utils import load_sharded_checkpoint\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n\nfor a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the\n\nAnswer: \n```\nThe maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the faithfulness of the generated answer, we will identify the factual claims made in the answer and assess their support based on the provided context.\n\n**Factual Claim:**\n1. \"The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB.\"\n\n**Assessment of the Claim:**\n- This claim is directly supported by the context, which states: \"model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces.\" This indicates that 10GB is indeed the threshold for automatic sharding.\n\nSince the claim is fully supported by the context, we assign it a score of 1 point.\n\n**Calculating the Faithfulness Score:**\n- Total number of claims: 1\n- Points awarded: 1\n\nFaithfulness Score = (sum of points) / (total number of claims) = 1 / 1 = 1.0\n\nThus, the final faithfulness score is:\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the faithfulness of a generated answer based solely on the provided context.\n\nFaithfulness measures whether the generated answer contains only information that is supported by the provided context. An answer is considered faithful if all its factual claims are directly supported by or can be reasonably inferred from the context\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the context (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the context (assign 0 points)\n3. Calculate the faithfulness score as: (sum of points) / (total number of claims)\n\nReturn a final faithfulness score between 0 and 1, where:\n- 1.0: Perfect faithfulness (all claims fully supported)\n- 0.0: Complete unfaithfulness (no claims supported by context)\n\nProvided Context: ` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `\n\nboth variants regardless. this change will improve performance for many applications. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5216 ] ( https : / / github. com / gradio - app / gradio / pull / 5216 ) [ ` 4b58ea6d ` ] ( https : / / github. com / gradio - app / gradio / commit / 4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8 ) - update i18n tokens and locale files. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )! # # 0. 2. 0 # # # features - [ # 5025 ] ( https : / / github. com / gradio -\n\nthe first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\n. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nAnswer: \n```\nThe `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on a Linux system using the musl C library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the faithfulness of the generated answer, we need to identify the factual claims made in the answer and assess their support based on the provided context.\n\n1. **Claim 1**: \"The `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture.\"\n   - This claim is **fully supported** by the context, which states that the binary is for \"x86_64 - unknown - linux - musl\". Therefore, this claim receives 1 point.\n\n2. **Claim 2**: \"on a Linux system using the musl C library.\"\n   - This claim is also **fully supported** by the context, which explicitly mentions \"linux - musl\". Thus, this claim also receives 1 point.\n\nBoth claims made in the answer are directly supported by the context provided. \n\nNow, we calculate the faithfulness score:\n- Total points = 1 (for Claim 1) + 1 (for Claim 2) = 2 points\n- Total number of claims = 2\n\nFaithfulness score = (sum of points) / (total number of claims) = 2 / 2 = 1.0\n\nThe generated answer is completely faithful to the context.\n\nFinal answer:\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the faithfulness of a generated answer based solely on the provided context.\n\nFaithfulness measures whether the generated answer contains only information that is supported by the provided context. An answer is considered faithful if all its factual claims are directly supported by or can be reasonably inferred from the context\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the context (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the context (assign 0 points)\n3. Calculate the faithfulness score as: (sum of points) / (total number of claims)\n\nReturn a final faithfulness score between 0 and 1, where:\n- 1.0: Perfect faithfulness (all claims fully supported)\n- 0.0: Complete unfaithfulness (no claims supported by context)\n\nProvided Context: the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nhat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the\n\nlibrary. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in\n\n### HF_HUB_DISABLE_TELEMETRY\r\n\r\nBy default, some data is collected by HF libraries (`transformers`, `datasets`, `gradio`,..) to monitor usage, debug issues and help prioritize features.\n\nAnswer: \n```\nThe default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst2-english.\"\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the faithfulness of the generated answer, we will identify the factual claims made in the answer and assess their support based on the provided context.\n\n**Factual Claims in the Answer:**\n1. \"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n\n**Assessment of Claims:**\n- The context states: \"the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english.\" \n- The generated answer claims that the default checkpoint is \"distilbert-base-uncased-finetuned-sst2-english.\" \n\nThe claim in the answer is a direct paraphrase of the information provided in the context. The context does not specify the exact formatting of the checkpoint name (e.g., hyphens vs. spaces), but it does provide the essential information that this is the default checkpoint for the sentiment analysis pipeline.\n\nSince the claim is directly supported by the context, we assign it a score of 1 point.\n\n**Calculating the Faithfulness Score:**\n- Total claims: 1\n- Points awarded: 1\n\nFaithfulness Score = (sum of points) / (total number of claims) = 1 / 1 = 1.0\n\nThus, the final faithfulness score is:\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the faithfulness of a generated answer based solely on the provided context.\n\nFaithfulness measures whether the generated answer contains only information that is supported by the provided context. An answer is considered faithful if all its factual claims are directly supported by or can be reasonably inferred from the context\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the context (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the context (assign 0 points)\n3. Calculate the faithfulness score as: (sum of points) / (total number of claims)\n\nReturn a final faithfulness score between 0 and 1, where:\n- 1.0: Perfect faithfulness (all claims fully supported)\n- 0.0: Complete unfaithfulness (no claims supported by context)\n\nProvided Context: - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of\n\nguidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nðŸ§¨ Diffusers Experimental\r\n\r\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\r\nCurrently, the following experiments are supported:\n\nhat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the\n\n- **Encouraging safety in deployment**\n\nAnswer: \n```\nThe purpose of the Safety Checker in the Diffusers library is to check and compare class probabilities against a set of guidelines to ensure safety in deployment. It aims to encourage safe usage and contributions within the library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "Let's evaluate the generated answer based on the provided context.\n\n1. **Claim 1**: \"The purpose of the Safety Checker in the Diffusers library is to check and compare class probabilities against a set of guidelines.\"\n   - This claim is **fully supported** by the context, which states that the Safety Checker checks and compares class probabilities against a set of guidelines. \n   - Score: 1 point\n\n2. **Claim 2**: \"It aims to encourage safe usage and contributions within the library.\"\n   - This claim is **partially supported**. While the context mentions that the Safety Checker is related to safety in deployment and encourages safety, it does not explicitly state that it aims to encourage safe usage and contributions. However, it can be reasonably inferred from the context that the purpose of ensuring safety would include encouraging safe usage and contributions.\n   - Score: 0.5 points\n\nNow, we calculate the faithfulness score:\n- Total points = 1 (from Claim 1) + 0.5 (from Claim 2) = 1.5\n- Total number of claims = 2\n\nFaithfulness score = Total points / Total number of claims = 1.5 / 2 = 0.75\n\nThus, the final faithfulness score is:\n\nSCORE: 0.75",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the faithfulness of a generated answer based solely on the provided context.\n\nFaithfulness measures whether the generated answer contains only information that is supported by the provided context. An answer is considered faithful if all its factual claims are directly supported by or can be reasonably inferred from the context\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the context (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the context (assign 0 points)\n3. Calculate the faithfulness score as: (sum of points) / (total number of claims)\n\nReturn a final faithfulness score between 0 and 1, where:\n- 1.0: Perfect faithfulness (all claims fully supported)\n- 0.0: Complete unfaithfulness (no claims supported by context)\n\nProvided Context: the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nlibrary. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in\n\nsteps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage,\n\nits unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,\n\nAnswer: \n```\nThe default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst2-english.\"\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the faithfulness of the generated answer, we will identify the factual claims made in the answer and assess their support based on the provided context.\n\n1. **Claim**: \"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n\nNow, we will check if this claim is supported by the context:\n\n- The context states: \"the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english.\" \n- The generated answer uses a slightly different format for the checkpoint name, adding hyphens and changing the case. However, the essential information remains the same: it identifies the same model as the default checkpoint for the sentiment analysis pipeline.\n\nSince the claim in the answer is essentially a rephrasing of the information provided in the context, it is fully supported.\n\nThus, we assign:\n- **1 point** for this claim.\n\nNow, we calculate the faithfulness score:\n- Total claims: 1\n- Points awarded: 1\n\nFaithfulness score = (sum of points) / (total number of claims) = 1 / 1 = 1.0\n\nThe generated answer is perfectly faithful to the context.\n\nFinal answer:\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the faithfulness of a generated answer based solely on the provided context.\n\nFaithfulness measures whether the generated answer contains only information that is supported by the provided context. An answer is considered faithful if all its factual claims are directly supported by or can be reasonably inferred from the context\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the context (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the context (assign 0 points)\n3. Calculate the faithfulness score as: (sum of points) / (total number of claims)\n\nReturn a final faithfulness score between 0 and 1, where:\n- 1.0: Perfect faithfulness (all claims fully supported)\n- 0.0: Complete unfaithfulness (no claims supported by context)\n\nProvided Context: -->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should\n\nthe checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\n\nAnswer: \n```\nThe method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This method downloads and caches the configuration and vocabulary associated with the specified checkpoint. It allows for easy loading of the relevant pipeline without needing to know the specific class to use.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "Let's evaluate the generated answer based on the provided context.\n\n1. **Claim 1**: \"The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`.\"\n   - This claim is fully supported by the context, which states that the method of the `AutoPipeline` class is `from_pretrained`. \n   - **Score**: 1 point\n\n2. **Claim 2**: \"This method downloads and caches the configuration and vocabulary associated with the specified checkpoint.\"\n   - This claim is also fully supported by the context, which mentions that `from_pretrained` will download and cache the configuration and vocabulary associated with a given checkpoint.\n   - **Score**: 1 point\n\n3. **Claim 3**: \"It allows for easy loading of the relevant pipeline without needing to know the specific class to use.\"\n   - This claim is fully supported by the context, which states that `AutoPipeline` is designed to make it easy to load a checkpoint without knowing the specific pipeline class to use.\n   - **Score**: 1 point\n\nNow, we sum the points:\n- Total points = 1 + 1 + 1 = 3\n- Total number of claims = 3\n\nFinally, we calculate the faithfulness score:\n- Faithfulness Score = Total points / Total claims = 3 / 3 = 1.0\n\nThe generated answer is completely faithful to the provided context.\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the faithfulness of a generated answer based solely on the provided context.\n\nFaithfulness measures whether the generated answer contains only information that is supported by the provided context. An answer is considered faithful if all its factual claims are directly supported by or can be reasonably inferred from the context\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the context (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the context (assign 0 points)\n3. Calculate the faithfulness score as: (sum of points) / (total number of claims)\n\nReturn a final faithfulness score between 0 and 1, where:\n- 1.0: Perfect faithfulness (all claims fully supported)\n- 0.0: Complete unfaithfulness (no claims supported by context)\n\nProvided Context: the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nto a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all\n\nremoved. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. to get an output linked to our classification problem, we need to use the automodelforsequenceclassification class. it works exactly as the automodel class, except that it will build a model with a classification head. there is one auto class for each common nlp task in the transformers library. here, after giving our model the two sentences, we get a tensor of size two by two : one result for each sentence and for each possible label. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third\n\n. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs.\n\nAnswer: \n```\nThe default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst2-english.\"\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the faithfulness of the generated answer, we need to identify the factual claims made in the answer and assess their support based on the provided context.\n\n1. **Claim**: \"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n\nNow, we will check if this claim is supported by the context:\n\n- The context states: \"the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english.\" \n\nThis statement in the context directly supports the claim made in the answer. The checkpoint name is consistent, and the context clearly indicates that this is the default checkpoint for the sentiment analysis pipeline.\n\nSince the claim is fully supported by the context, we assign it a score of 1 point.\n\nNow, we calculate the faithfulness score:\n\n- Total number of claims: 1\n- Points awarded: 1\n\nFaithfulness score = (sum of points) / (total number of claims) = 1 / 1 = 1.0\n\nThus, the final faithfulness score is:\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the faithfulness of a generated answer based solely on the provided context.\n\nFaithfulness measures whether the generated answer contains only information that is supported by the provided context. An answer is considered faithful if all its factual claims are directly supported by or can be reasonably inferred from the context\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the context (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the context (assign 0 points)\n3. Calculate the faithfulness score as: (sum of points) / (total number of claims)\n\nReturn a final faithfulness score between 0 and 1, where:\n- 1.0: Perfect faithfulness (all claims fully supported)\n- 0.0: Complete unfaithfulness (no claims supported by context)\n\nProvided Context: ` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `\n\nhf _ home ` is not set! this is the default way to configure where [ user - specific non - essential ( cached ) data should be written ] ( https : / / wiki. archlinux. org / title / xdg _ base _ directory ) on linux machines. if ` hf _ home ` is not set, the default home will be ` \" $ xdg _ cache _ home / huggingface \" ` instead of ` \" ~ /. cache / huggingface \" `.\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\n. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size\n\n! - - note that this file is in markdown but contain specific syntax for our doc - builder ( similar to mdx ) that may not be rendered properly in your markdown viewer. - - > # environment variables ` huggingface _ hub ` can be configured using environment variables. if you are unfamiliar with environment variable, here are generic articles about them [ on macos and linux ] ( https : / / linuxize. com / post / how - to - set - and - list - environment - variables - in - linux / ) and on [ windows ] ( https : / / phoenixnap. com / kb / windows - set - environment - variable ). this page will guide you through all environment variables specific to ` huggingface _ hub ` and their meaning. # # generic # # # hf _ inference _ endpoint to configure the inference api base url. you might want to set this variable if your organization is pointing at an api gateway rather\n\nAnswer: \n```\nThe `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on Linux with the musl C library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the faithfulness of the generated answer, we will identify the factual claims made in the answer and assess their support based on the provided context.\n\n1. **Claim 1**: \"The `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture.\"\n   - This claim is **fully supported** by the context, which states that \"`tokenizers - linux - x64 - musl` is the * * x86 _ 64 - unknown - linux - musl * * binary for `tokenizers`.\" Therefore, this claim receives 1 point.\n\n2. **Claim 2**: \"The `tokenizers-linux-x64-musl` binary is designed for Linux.\"\n   - This claim is also **fully supported** by the context, as it explicitly mentions that the binary is for Linux. Thus, this claim also receives 1 point.\n\n3. **Claim 3**: \"The `tokenizers-linux-x64-musl` binary is designed for the musl C library.\"\n   - This claim is **fully supported** by the context, which indicates that the binary is for \"musl\". Therefore, this claim receives 1 point.\n\nNow, we have three claims, all of which are fully supported by the context. \n\nCalculating the faithfulness score:\n- Total points = 1 (Claim 1) + 1 (Claim 2) + 1 (Claim 3) = 3 points\n- Total number of claims = 3\n\nFaithfulness score = (sum of points) / (total number of claims) = 3 / 3 = 1.0\n\nThus, the final faithfulness score is:\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the faithfulness of a generated answer based solely on the provided context.\n\nFaithfulness measures whether the generated answer contains only information that is supported by the provided context. An answer is considered faithful if all its factual claims are directly supported by or can be reasonably inferred from the context\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the context (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the context (assign 0 points)\n3. Calculate the faithfulness score as: (sum of points) / (total number of claims)\n\nReturn a final faithfulness score between 0 and 1, where:\n- 1.0: Perfect faithfulness (all claims fully supported)\n- 0.0: Complete unfaithfulness (no claims supported by context)\n\nProvided Context: `tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n\nlibrary, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or\n\ntokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning\n\nits unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,\n\nNow let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well\n\nAnswer: \n```\nThe `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture. It is specifically built for the unknown Linux environment using the musl C library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the faithfulness of the generated answer, I will identify the factual claims made in the answer and assess their support based on the provided context.\n\n**Factual Claims in the Answer:**\n1. The `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture.\n2. It is specifically built for the unknown Linux environment.\n3. It uses the musl C library.\n\n**Assessment of Claims:**\n1. **Claim 1:** \"The `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture.\"\n   - This claim is fully supported by the context, which states that the binary is for `x86_64-unknown-linux-musl`. Therefore, this claim receives 1 point.\n\n2. **Claim 2:** \"It is specifically built for the unknown Linux environment.\"\n   - The context mentions `x86_64-unknown-linux-musl`, which implies that it is built for an unknown Linux environment. This claim can be reasonably inferred from the context, so it receives 0.5 points.\n\n3. **Claim 3:** \"It uses the musl C library.\"\n   - The context explicitly states that the binary is for `musl`, which indicates that it uses the musl C library. This claim is fully supported, so it receives 1 point.\n\n**Calculating the Faithfulness Score:**\n- Total points = 1 (Claim 1) + 0.5 (Claim 2) + 1 (Claim 3) = 2.5 points\n- Total number of claims = 3\n\nFaithfulness Score = (Total points) / (Total number of claims) = 2.5 / 3 = 0.8333\n\nThus, the final faithfulness score is:\n\nSCORE: 0.8333",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the faithfulness of a generated answer based solely on the provided context.\n\nFaithfulness measures whether the generated answer contains only information that is supported by the provided context. An answer is considered faithful if all its factual claims are directly supported by or can be reasonably inferred from the context\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the context (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the context (assign 0 points)\n3. Calculate the faithfulness score as: (sum of points) / (total number of claims)\n\nReturn a final faithfulness score between 0 and 1, where:\n- 1.0: Perfect faithfulness (all claims fully supported)\n- 0.0: Complete unfaithfulness (no claims supported by context)\n\nProvided Context: -->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nfor a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n- from_pipe\r\n\r\n## AutoPipelineForInpainting\r\n\r\n[[autodoc]] AutoPipelineForInpainting\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe\n\nAnswer: \n```\nThe method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This method automatically retrieves the relevant pipeline based on the name or path to the pretrained weights. Additionally, `from_pipe()` can be used to transfer components from the original pipeline to a new one without reallocating memory.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the faithfulness of the generated answer, I will identify the factual claims made in the answer and assess their support based on the provided context.\n\n1. **Claim 1**: \"The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`.\"\n   - This claim is **fully supported** by the context, which states that the `from_pretrained()` method is used to automatically retrieve the relevant pipeline based on the name or path to the pretrained weights. \n   - **Score**: 1 point\n\n2. **Claim 2**: \"`from_pretrained()` automatically retrieves the relevant pipeline based on the name or path to the pretrained weights.\"\n   - This claim is also **fully supported** by the context, which explicitly describes the functionality of the `from_pretrained()` method.\n   - **Score**: 1 point\n\n3. **Claim 3**: \"Additionally, `from_pipe()` can be used to transfer components from the original pipeline to a new one without reallocating memory.\"\n   - This claim is **fully supported** by the context, which mentions that the `from_pipe()` method is used to transfer components without reallocating additional memory.\n   - **Score**: 1 point\n\nNow, I will calculate the faithfulness score:\n- Total claims: 3\n- Points awarded: 3 (1 + 1 + 1)\n\nThe faithfulness score is calculated as follows:\n\\[ \\text{Faithfulness Score} = \\frac{\\text{sum of points}}{\\text{total number of claims}} = \\frac{3}{3} = 1.0 \\]\n\nThus, the final faithfulness score is:\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the faithfulness of a generated answer based solely on the provided context.\n\nFaithfulness measures whether the generated answer contains only information that is supported by the provided context. An answer is considered faithful if all its factual claims are directly supported by or can be reasonably inferred from the context\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the context (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the context (assign 0 points)\n3. Calculate the faithfulness score as: (sum of points) / (total number of claims)\n\nReturn a final faithfulness score between 0 and 1, where:\n- 1.0: Perfect faithfulness (all claims fully supported)\n- 0.0: Complete unfaithfulness (no claims supported by context)\n\nProvided Context: `tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n\n-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\non linux machines. \r\n\r\nIf `HF_HOME` is not set, the default home will be `\"$XDG_CACHE_HOME/huggingface\"` instead\r\nof `\"~/.cache/huggingface\"`.\n\n[on macOS and Linux](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/)\r\nand on [Windows](https://phoenixnap.com/kb/windows-set-environment-variable).\n\nattention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the\n\nAnswer: \n```\nThe `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on Linux using the musl C library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the faithfulness of the generated answer, we will identify the factual claims made in the answer and assess their support based on the provided context.\n\n### Factual Claims in the Answer:\n1. The `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture.\n2. The `tokenizers-linux-x64-musl` binary is designed for Linux.\n3. The `tokenizers-linux-x64-musl` binary uses the musl C library.\n\n### Evaluation of Claims:\n1. **Claim 1**: The `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture.\n   - **Support**: This claim is fully supported by the context, which states that the binary is for `x86_64-unknown-linux-musl`. \n   - **Score**: 1 point\n\n2. **Claim 2**: The `tokenizers-linux-x64-musl` binary is designed for Linux.\n   - **Support**: This claim is also fully supported by the context, as it explicitly mentions that the binary is for Linux.\n   - **Score**: 1 point\n\n3. **Claim 3**: The `tokenizers-linux-x64-musl` binary uses the musl C library.\n   - **Support**: This claim is fully supported by the context, which includes \"musl\" in the description of the binary.\n   - **Score**: 1 point\n\n### Calculation of Faithfulness Score:\n- Total points = 1 + 1 + 1 = 3\n- Total number of claims = 3\n- Faithfulness score = Total points / Total number of claims = 3 / 3 = 1.0\n\nThe generated answer contains only information that is fully supported by the provided context.\n\nFinal answer:\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the faithfulness of a generated answer based solely on the provided context.\n\nFaithfulness measures whether the generated answer contains only information that is supported by the provided context. An answer is considered faithful if all its factual claims are directly supported by or can be reasonably inferred from the context\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the context (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the context (assign 0 points)\n3. Calculate the faithfulness score as: (sum of points) / (total number of claims)\n\nReturn a final faithfulness score between 0 and 1, where:\n- 1.0: Perfect faithfulness (all claims fully supported)\n- 0.0: Complete unfaithfulness (no claims supported by context)\n\nProvided Context: the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nto a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all\n\n##try by default, some data is collected by hf libraries ( ` transformers `, ` datasets `, ` gradio `,.. ) to monitor usage, debug issues and help prioritize features. each library defines its own policy ( i. e. which usage to monitor ) but the core implementation happens in ` huggingface _ hub ` ( see [ ` send _ telemetry ` ] ). you can set ` hf _ hub _ disable _ telemetry = 1 ` as environment variable to globally disable telemetry. # # # hf _ hub _ enable _ hf _ transfer set to ` true ` for faster uploads and downloads from the hub using ` hf _ transfer `. by default, ` huggingface _ hub ` uses the python - based ` requests. get ` and ` requests. post ` functions. although these are reliable and versatile, they may not be the most efficient choice for machines with high\n\n. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs.\n\nAnswer: \n```\nThe default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst2-english.\" This checkpoint is associated with a tokenizer that processes the input sentences for sentiment classification.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the faithfulness of the generated answer, I will identify the factual claims made in the answer and assess their support based on the provided context.\n\n1. **Claim 1**: \"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n   - This claim is **fully supported** by the context, which explicitly states that \"the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english.\" Therefore, I assign **1 point**.\n\n2. **Claim 2**: \"This checkpoint is associated with a tokenizer that processes the input sentences for sentiment classification.\"\n   - This claim is also **fully supported** by the context. The context mentions that a tokenizer is instantiated associated with the checkpoint and that it processes the input sentences. Therefore, I assign **1 point**.\n\nNow, I will calculate the faithfulness score:\n- Total points = 1 (Claim 1) + 1 (Claim 2) = 2 points\n- Total number of claims = 2\n\nFaithfulness score = (sum of points) / (total number of claims) = 2 / 2 = 1.0\n\nThe generated answer is completely faithful to the provided context.\n\nFinal answer:\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the faithfulness of a generated answer based solely on the provided context.\n\nFaithfulness measures whether the generated answer contains only information that is supported by the provided context. An answer is considered faithful if all its factual claims are directly supported by or can be reasonably inferred from the context\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the context (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the context (assign 0 points)\n3. Calculate the faithfulness score as: (sum of points) / (total number of claims)\n\nReturn a final faithfulness score between 0 and 1, where:\n- 1.0: Perfect faithfulness (all claims fully supported)\n- 0.0: Complete unfaithfulness (no claims supported by context)\n\nProvided Context: diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https\n\nhttps : / / huggingface. co / spaces / society - ethics / diffusionbiasexplorer ) to demonstrate the biases in stable diffusion interactively. in this sense, we support and encourage bias explorers and evaluations. - * * encouraging safety in deployment * * - [ * * safe stable diffusion * * ] ( https : / / huggingface. co / docs / diffusers / main / en / api / pipelines / stable _ diffusion / stable _ diffusion _ safe ) : it mitigates the well - known issue that models, like stable diffusion, that are trained on unfiltered, web - crawled datasets tend to suffer from inappropriate degeneration. related paper : [ safe latent diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / gi\n\nsame level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and\n\ndiffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nAnswer: \n```\nThe purpose of the Safety Checker in the Diffusers library is to mitigate inappropriate degeneration in diffusion models by checking and comparing the class probability of harmful concepts against generated images. It uses hard-coded harmful concepts that are intentionally hidden to prevent reverse engineering. This mechanism helps ensure safer deployment of models like stable diffusion, which may be trained on unfiltered datasets.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the faithfulness of the generated answer, I will identify the factual claims made in the answer and assess their support based on the provided context.\n\n1. **Claim 1**: \"The purpose of the Safety Checker in the Diffusers library is to mitigate inappropriate degeneration in diffusion models.\"\n   - This claim is **fully supported** by the context, which states that the safety checker mitigates inappropriate degeneration in diffusion models. (1 point)\n\n2. **Claim 2**: \"It checks and compares the class probability of harmful concepts against generated images.\"\n   - This claim is also **fully supported** by the context, which describes the function of the safety checker in checking and comparing class probabilities of harmful concepts against images. (1 point)\n\n3. **Claim 3**: \"It uses hard-coded harmful concepts that are intentionally hidden to prevent reverse engineering.\"\n   - This claim is **fully supported** by the context, which explicitly mentions that the harmful concepts are hard-coded and hidden to prevent reverse engineering. (1 point)\n\n4. **Claim 4**: \"This mechanism helps ensure safer deployment of models like stable diffusion, which may be trained on unfiltered datasets.\"\n   - This claim is **partially supported**. While the context discusses the safety checker and its role in ensuring safer deployment, it does not explicitly state that it helps ensure safer deployment of models like stable diffusion. However, it can be reasonably inferred that the safety checker contributes to safer deployment. (0.5 points)\n\nNow, I will calculate the faithfulness score:\n- Total points = 1 + 1 + 1 + 0.5 = 3.5\n- Total number of claims = 4\n- Faithfulness score = 3.5 / 4 = 0.875\n\nThus, the final faithfulness score is:\n\nSCORE: 0.875",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the faithfulness of a generated answer based solely on the provided context.\n\nFaithfulness measures whether the generated answer contains only information that is supported by the provided context. An answer is considered faithful if all its factual claims are directly supported by or can be reasonably inferred from the context\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the context (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the context (assign 0 points)\n3. Calculate the faithfulness score as: (sum of points) / (total number of claims)\n\nReturn a final faithfulness score between 0 and 1, where:\n- 1.0: Perfect faithfulness (all claims fully supported)\n- 0.0: Complete unfaithfulness (no claims supported by context)\n\nProvided Context: - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of\n\nguidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\n-->\r\n\r\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\r\n\r\n## Preamble\n\n- [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\nAnswer: \n```\nThe purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of a set of guidelines to ensure safe usage of the models. It aims to mitigate issues associated with models like Stable Diffusion. This contributes to the ethical development and use of the library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the faithfulness of the generated answer, I will identify the factual claims made in the answer and assess their support based on the provided context.\n\n1. **Claim 1**: \"The purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of a set of guidelines.\"\n   - This claim is **fully supported** by the context, which states that the Safety Checker checks and compares the class probability of a set of guidelines. \n   - Score: 1 point\n\n2. **Claim 2**: \"It aims to mitigate issues associated with models like Stable Diffusion.\"\n   - This claim is **partially supported**. The context mentions that Safe Stable Diffusion mitigates issues associated with models like Stable Diffusion, but it does not explicitly state that the Safety Checker itself aims to mitigate these issues. However, it can be reasonably inferred that the Safety Checker contributes to this goal.\n   - Score: 0.5 points\n\n3. **Claim 3**: \"This contributes to the ethical development and use of the library.\"\n   - This claim is **partially supported**. The context discusses ethical guidelines and the commitment to safe usage, but it does not explicitly state that the Safety Checker contributes to ethical development. However, it can be reasonably inferred that ensuring safe usage aligns with ethical development.\n   - Score: 0.5 points\n\nNow, I will calculate the faithfulness score:\n- Total points = 1 (Claim 1) + 0.5 (Claim 2) + 0.5 (Claim 3) = 2 points\n- Total number of claims = 3\n\nFaithfulness score = Total points / Total number of claims = 2 / 3 = 0.6667\n\nThus, the final faithfulness score is:\n\nSCORE: 0.6667",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the faithfulness of a generated answer based solely on the provided context.\n\nFaithfulness measures whether the generated answer contains only information that is supported by the provided context. An answer is considered faithful if all its factual claims are directly supported by or can be reasonably inferred from the context\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the context (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the context (assign 0 points)\n3. Calculate the faithfulness score as: (sum of points) / (total number of claims)\n\nReturn a final faithfulness score between 0 and 1, where:\n- 1.0: Perfect faithfulness (all claims fully supported)\n- 0.0: Complete unfaithfulness (no claims supported by context)\n\nProvided Context: diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https\n\nsame level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and\n\n' s responsiveness and valuable feedback. # # scope the diffusers community will apply the following ethical guidelines to the project â€™ s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns. # # ethical guidelines the following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question. - * * transparency * * : we are committed to being transparent in managing prs, explaining our choices to users, and making technical decisions. - * * consistency * * : we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # diffusers â€™ ethical guidelines # # preamble [ diffusers ] ( https : / / huggingface. co / docs / diffusers / index ) provides pre - trained diffusion models and serves as a modular toolbox for inference and training. given its real case applications in the world and potential negative impacts on society, we think it\n\ndiffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model.\n\nAnswer: \n```\nThe purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of hard-coded harmful concepts against generated images to mitigate inappropriate degeneration in diffusion models. It aims to prevent the generation of harmful content by ensuring that such concepts are not present in the output. The harmful concepts are intentionally hidden to protect the integrity of the checker.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the faithfulness of the generated answer, I will identify the factual claims made in the answer and assess their support based on the provided context.\n\n1. **Claim 1**: \"The purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of hard-coded harmful concepts against generated images.\"\n   - This claim is fully supported by the context, which states that the safety checker checks and compares the class probability of harmful concepts against generated images. \n   - **Score**: 1 point\n\n2. **Claim 2**: \"It aims to prevent the generation of harmful content by ensuring that such concepts are not present in the output.\"\n   - This claim is also fully supported by the context, which implies that the purpose of the safety checker is to mitigate inappropriate degeneration in diffusion models, which includes preventing harmful content.\n   - **Score**: 1 point\n\n3. **Claim 3**: \"The harmful concepts are intentionally hidden to protect the integrity of the checker.\"\n   - This claim is directly supported by the context, which states that the harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n   - **Score**: 1 point\n\nNow, I will calculate the faithfulness score:\n- Total claims: 3\n- Points scored: 3 (1 + 1 + 1)\n\nThe faithfulness score is calculated as follows:\n\\[ \\text{Faithfulness Score} = \\frac{\\text{Total Points}}{\\text{Total Claims}} = \\frac{3}{3} = 1.0 \\]\n\nThus, the generated answer is fully faithful to the provided context.\n\nFinal answer:\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the faithfulness of a generated answer based solely on the provided context.\n\nFaithfulness measures whether the generated answer contains only information that is supported by the provided context. An answer is considered faithful if all its factual claims are directly supported by or can be reasonably inferred from the context\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the context (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the context (assign 0 points)\n3. Calculate the faithfulness score as: (sum of points) / (total number of claims)\n\nReturn a final faithfulness score between 0 and 1, where:\n- 1.0: Perfect faithfulness (all claims fully supported)\n- 0.0: Complete unfaithfulness (no claims supported by context)\n\nProvided Context: slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\ntraining, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when\n\nsize of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] { ' total _ size ' : 433245184 } ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] { ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre\n\n... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )\n\nAnswer: \n```\nIn Transformers version 4.18.0, model checkpoints that exceed 10 GB in size are automatically sharded into smaller pieces. Each of these smaller pieces will be less than 10 GB. You can control the maximum size before sharding using the `max_shard_size` parameter.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "Let's evaluate the claims made in the generated answer based on the provided context.\n\n1. **Claim 1**: \"In Transformers version 4.18.0, model checkpoints that exceed 10 GB in size are automatically sharded into smaller pieces.\"\n   - This claim is **fully supported** by the context, which states: \"sharded checkpoints since version 4.18.0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces.\"\n   - **Score**: 1 point\n\n2. **Claim 2**: \"Each of these smaller pieces will be less than 10 GB.\"\n   - This claim is also **fully supported** by the context, which mentions that the sharded pieces are \"each of which being of size < 10gb.\"\n   - **Score**: 1 point\n\n3. **Claim 3**: \"You can control the maximum size before sharding using the `max_shard_size` parameter.\"\n   - This claim is **fully supported** by the context, which states: \"you can control the maximum size before sharding with the `max_shard_size` parameter.\"\n   - **Score**: 1 point\n\nNow, let's calculate the faithfulness score:\n\n- Total claims: 3\n- Points awarded: 3 (1 + 1 + 1)\n\nFaithfulness score = (sum of points) / (total number of claims) = 3 / 3 = 1.0\n\nThe generated answer is completely faithful to the provided context.\n\nFinal answer:\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the faithfulness of a generated answer based solely on the provided context.\n\nFaithfulness measures whether the generated answer contains only information that is supported by the provided context. An answer is considered faithful if all its factual claims are directly supported by or can be reasonably inferred from the context\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the context (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the context (assign 0 points)\n3. Calculate the faithfulness score as: (sum of points) / (total number of claims)\n\nReturn a final faithfulness score between 0 and 1, where:\n- 1.0: Perfect faithfulness (all claims fully supported)\n- 0.0: Complete unfaithfulness (no claims supported by context)\n\nProvided Context: slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~\n\ntraining, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\n... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )\n\nsize of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] { ' total _ size ' : 433245184 } ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] { ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre\n\nAnswer: \n```\nThe maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10 GB. Checkpoints larger than this size will be split into smaller pieces. You can control the maximum size before sharding using the `max_shard_size` parameter.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "Let's evaluate the claims made in the generated answer based on the provided context.\n\n1. **Claim 1**: \"The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10 GB.\"\n   - This claim is **fully supported** by the context, which states: \"model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces.\" Therefore, this claim receives **1 point**.\n\n2. **Claim 2**: \"Checkpoints larger than this size will be split into smaller pieces.\"\n   - This claim is also **fully supported** by the context, as it directly follows from the first claim. The context explicitly states that checkpoints larger than 10 GB are automatically sharded. Thus, this claim also receives **1 point**.\n\n3. **Claim 3**: \"You can control the maximum size before sharding using the `max_shard_size` parameter.\"\n   - This claim is **fully supported** by the context, which mentions: \"you can control the maximum size before sharding with the `max_shard_size` parameter.\" Therefore, this claim receives **1 point**.\n\nNow, we have three claims, all of which are fully supported by the context. \n\nTo calculate the faithfulness score:\n- Total points = 1 (Claim 1) + 1 (Claim 2) + 1 (Claim 3) = 3 points\n- Total number of claims = 3\n\nFaithfulness score = Total points / Total number of claims = 3 / 3 = 1.0\n\nThus, the final faithfulness score is:\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the faithfulness of a generated answer based solely on the provided context.\n\nFaithfulness measures whether the generated answer contains only information that is supported by the provided context. An answer is considered faithful if all its factual claims are directly supported by or can be reasonably inferred from the context\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the context (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the context (assign 0 points)\n3. Calculate the faithfulness score as: (sum of points) / (total number of claims)\n\nReturn a final faithfulness score between 0 and 1, where:\n- 1.0: Perfect faithfulness (all claims fully supported)\n- 0.0: Complete unfaithfulness (no claims supported by context)\n\nProvided Context: ! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\nto load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,\n\nautodoc ] ] autopipelineforimage2image - all - from _ pretrained - from _ pipe # # autopipelineforinpainting [ [ autodoc ] ] autopipelineforinpainting - all - from _ pretrained - from _ pipe\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\n. bin ', ' pytorch _ model - 00002 - of - 00003. bin ', ' pytorch _ model - 00003 - of - 00003. bin ', ' pytorch _ model. bin. index. json ' ] ` ` ` on top of the configuration of the model, we see three different weights files, and an ` index. json ` file which is our index. a checkpoint like this can be fully reloaded using the [ ` ~ pretrainedmodel. from _ pretrained ` ] method : ` ` ` py > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir\n\nAnswer: \n```\nTo load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method, which automatically retrieves the relevant pipeline given the name or path to the pretrained weights. Additionally, to switch between tasks with the same checkpoint without reallocating memory, the `from_pipe()` method can be used to transfer components from the original pipeline to the new one.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the faithfulness of the generated answer, I will identify the factual claims made in the answer and assess their support based on the provided context.\n\n### Factual Claims in the Answer:\n1. \"To load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method.\"\n2. \"The `from_pretrained()` method automatically retrieves the relevant pipeline given the name or path to the pretrained weights.\"\n3. \"To switch between tasks with the same checkpoint without reallocating memory, the `from_pipe()` method can be used.\"\n4. \"The `from_pipe()` method transfers components from the original pipeline to the new one.\"\n\n### Assessment of Claims:\n1. **Claim 1**: \"To load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method.\"\n   - **Support**: This claim is fully supported by the context, which states that the `from_pretrained()` method is used to load a checkpoint for a task.\n   - **Score**: 1 point\n\n2. **Claim 2**: \"The `from_pretrained()` method automatically retrieves the relevant pipeline given the name or path to the pretrained weights.\"\n   - **Support**: This claim is also fully supported by the context, which explicitly mentions that the `from_pretrained()` method retrieves the relevant pipeline.\n   - **Score**: 1 point\n\n3. **Claim 3**: \"To switch between tasks with the same checkpoint without reallocating memory, the `from_pipe()` method can be used.\"\n   - **Support**: This claim is fully supported by the context, which describes the use of the `from_pipe()` method for switching tasks without reallocating memory.\n   - **Score**: 1 point\n\n4. **Claim 4**: \"The `from_pipe()` method transfers components from the original pipeline to the new one.\"\n   - **Support**: This claim is fully supported by the context, which states that the `from_pipe()` method is used to transfer components from the original pipeline.\n   - **Score**: 1 point\n\n### Calculation of Faithfulness Score:\n- Total points = 1 + 1 + 1 + 1 = 4\n- Total number of claims = 4\n- Faithfulness Score = Total points / Total number of claims = 4 / 4 = 1.0\n\nThe generated answer is completely faithful to the provided context, as all claims are fully supported.\n\nFinal answer:\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the faithfulness of a generated answer based solely on the provided context.\n\nFaithfulness measures whether the generated answer contains only information that is supported by the provided context. An answer is considered faithful if all its factual claims are directly supported by or can be reasonably inferred from the context\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the context (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the context (assign 0 points)\n3. Calculate the faithfulness score as: (sum of points) / (total number of claims)\n\nReturn a final faithfulness score between 0 and 1, where:\n- 1.0: Perfect faithfulness (all claims fully supported)\n- 0.0: Complete unfaithfulness (no claims supported by context)\n\nProvided Context: to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\nthe first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\n:... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir ) ` ` ` the main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in ram to the model size plus the size of the biggest shard. behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. we can load that index like any json and get a dictionary : ` ` ` py > > > import json > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ sha\n\nAnswer: \n```\nTo load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method, which automatically retrieves the relevant pipeline given the name or path to the pretrained weights. This method allows you to load a checkpoint without needing to know the specific pipeline class. Additionally, you can use the `from_pipe()` method to transfer components between pipelines without reallocating memory.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "Let's evaluate the generated answer based on the provided context.\n\n### Factual Claims in the Generated Answer:\n1. \"To load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method.\"\n2. \"This method automatically retrieves the relevant pipeline given the name or path to the pretrained weights.\"\n3. \"This method allows you to load a checkpoint without needing to know the specific pipeline class.\"\n4. \"Additionally, you can use the `from_pipe()` method to transfer components between pipelines without reallocating memory.\"\n\n### Evaluation of Each Claim:\n1. **Claim 1**: \"To load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method.\"\n   - **Support**: Fully supported by the context, which states that the `from_pretrained()` method is used to load a checkpoint.\n   - **Score**: 1 point\n\n2. **Claim 2**: \"This method automatically retrieves the relevant pipeline given the name or path to the pretrained weights.\"\n   - **Support**: Fully supported by the context, which explicitly mentions that the `from_pretrained()` method retrieves the relevant pipeline.\n   - **Score**: 1 point\n\n3. **Claim 3**: \"This method allows you to load a checkpoint without needing to know the specific pipeline class.\"\n   - **Support**: Fully supported by the context, which states that you can load a checkpoint without knowing the specific pipeline class.\n   - **Score**: 1 point\n\n4. **Claim 4**: \"Additionally, you can use the `from_pipe()` method to transfer components between pipelines without reallocating memory.\"\n   - **Support**: Fully supported by the context, which describes the use of the `from_pipe()` method for transferring components without reallocating memory.\n   - **Score**: 1 point\n\n### Calculation of the Faithfulness Score:\n- Total Points: 1 + 1 + 1 + 1 = 4\n- Total Number of Claims: 4\n- Faithfulness Score: 4 / 4 = 1.0\n\nThe generated answer is completely faithful to the provided context, as all claims are fully supported.\n\nFinal answer:\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant a generated answer is to the given question, regardless of factual accuracy\n\nAnswer Relevancy measures whether the generated answer directly addresses what was asked in the question. An answer is highly relevant if it provides information that specifically responds to the query's intent and information needs.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. Assess whether the answer:\n   - Directly addresses all aspects of the question (assign 1 point)\n   - Only partially addresses the question (assign 0.5 points)\n   - Fails to address the question or is off-topic (assign 0 points)\n\n\nReturn a final answer relevancy score between 0 and 1, where:\n- 1.0: Perfect relevancy (completely addresses the question with focus)\n- 0.0: Complete irrelevancy (does not address the question at all)\n\n```\nWhat is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\n```\n\nAnswer: \n```\nThe maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. The main information need in the question is to determine the specific size limit (maximum size) of a model checkpoint that triggers automatic sharding in Transformers version 4.18.0.\n\n2. The answer provided directly addresses the question by stating that the maximum size is 10GB. It clearly provides the requested information and is focused on the specific version of Transformers mentioned, which is 4.18.0.\n\nGiven that the answer completely answers the question, it meets the full criteria for relevancy.\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant a generated answer is to the given question, regardless of factual accuracy\n\nAnswer Relevancy measures whether the generated answer directly addresses what was asked in the question. An answer is highly relevant if it provides information that specifically responds to the query's intent and information needs.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. Assess whether the answer:\n   - Directly addresses all aspects of the question (assign 1 point)\n   - Only partially addresses the question (assign 0.5 points)\n   - Fails to address the question or is off-topic (assign 0 points)\n\n\nReturn a final answer relevancy score between 0 and 1, where:\n- 1.0: Perfect relevancy (completely addresses the question with focus)\n- 0.0: Complete irrelevancy (does not address the question at all)\n\n```\nWhat architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\n```\n\nAnswer: \n```\nThe `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on a Linux system using the musl C library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. The main information need in the question is to identify the specific architecture for which the `tokenizers-linux-x64-musl` binary is designed.\n\n2. The provided answer explicitly states that the binary is designed for the x86_64 architecture on a Linux system using the musl C library. This directly addresses the question by providing the exact architecture requested.\n\nGiven that the answer comprehensively and accurately responds to the question without any ambiguity, I would assign it a score of 1.0 for perfect relevancy.\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant a generated answer is to the given question, regardless of factual accuracy\n\nAnswer Relevancy measures whether the generated answer directly addresses what was asked in the question. An answer is highly relevant if it provides information that specifically responds to the query's intent and information needs.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. Assess whether the answer:\n   - Directly addresses all aspects of the question (assign 1 point)\n   - Only partially addresses the question (assign 0.5 points)\n   - Fails to address the question or is off-topic (assign 0 points)\n\n\nReturn a final answer relevancy score between 0 and 1, where:\n- 1.0: Perfect relevancy (completely addresses the question with focus)\n- 0.0: Complete irrelevancy (does not address the question at all)\n\n```\nWhat is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\n```\n\nAnswer: \n```\nThe maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10 GB. Checkpoints larger than this size will be split into smaller pieces. You can control the maximum size before sharding using the `max_shard_size` parameter.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. The main information need in the question is to identify the maximum size of a model checkpoint in Transformers version 4.18.0 before it is automatically sharded.\n\n2. The generated answer directly responds to this need by providing the specific maximum size of a model checkpoint (10 GB) and elaborates on the consequence of exceeding that size (that checkpoints larger than this will be split). It also mentions that users can control the sharding behavior with the `max_shard_size` parameter, which adds helpful context relevant to the user's potential concerns regarding model checkpoint management.\n\nGiven that the answer thoroughly addresses the question and provides additional relevant context, it completely fulfills the information need.\n\nFinal score: 1.0\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant a generated answer is to the given question, regardless of factual accuracy\n\nAnswer Relevancy measures whether the generated answer directly addresses what was asked in the question. An answer is highly relevant if it provides information that specifically responds to the query's intent and information needs.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. Assess whether the answer:\n   - Directly addresses all aspects of the question (assign 1 point)\n   - Only partially addresses the question (assign 0.5 points)\n   - Fails to address the question or is off-topic (assign 0 points)\n\n\nReturn a final answer relevancy score between 0 and 1, where:\n- 1.0: Perfect relevancy (completely addresses the question with focus)\n- 0.0: Complete irrelevancy (does not address the question at all)\n\n```\nWhat architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\n```\n\nAnswer: \n```\nThe `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on Linux with the musl C library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "In analyzing the question, \"What architecture is the `tokenizers-linux-x64-musl` binary designed for?\", the main information need is to identify the specific architecture that this binary targets.\n\nThe provided answer states, \"The `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on Linux with the musl C library.\" This directly addresses the question by specifying the architecture (x86_64) that the binary is designed for. Additionally, it includes relevant context about the environment (Linux and the musl C library) without straying from the core question.\n\nSince the answer aligns clearly with what the question asks, I would assign a score of 1.0 for perfect relevancy.\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant a generated answer is to the given question, regardless of factual accuracy\n\nAnswer Relevancy measures whether the generated answer directly addresses what was asked in the question. An answer is highly relevant if it provides information that specifically responds to the query's intent and information needs.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. Assess whether the answer:\n   - Directly addresses all aspects of the question (assign 1 point)\n   - Only partially addresses the question (assign 0.5 points)\n   - Fails to address the question or is off-topic (assign 0 points)\n\n\nReturn a final answer relevancy score between 0 and 1, where:\n- 1.0: Perfect relevancy (completely addresses the question with focus)\n- 0.0: Complete irrelevancy (does not address the question at all)\n\n```\nWhat method is used to load a checkpoint for a task using `AutoPipeline`?\n\n```\n\nAnswer: \n```\nTo load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method, which automatically retrieves the relevant pipeline given the name or path to the pretrained weights. This method allows you to load a checkpoint without needing to know the specific pipeline class. Additionally, you can use the `from_pipe()` method to transfer components between pipelines without reallocating memory.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "The main information need in the question is to understand the method used for loading a checkpoint within the context of `AutoPipeline`. The question is specifically asking for the method itself that facilitates this action.\n\nThe generated answer directly addresses the method to load a checkpoint by stating the use of `from_pretrained()` and briefly explaining its functionality regarding loading a checkpoint without needing to know the specific pipeline class. It also mentions `from_pipe()` as an additional method for handling component transfers, though this isn't directly necessary to answer the original question about loading a checkpoint.\n\nSince the answer provides a clear method (`from_pretrained()`), which aligns with the query's intent and includes relevant context about its usage, it fully addresses the question posed.\n\nTherefore, I will assign a score of 1.0 for perfect relevancy.\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant a generated answer is to the given question, regardless of factual accuracy\n\nAnswer Relevancy measures whether the generated answer directly addresses what was asked in the question. An answer is highly relevant if it provides information that specifically responds to the query's intent and information needs.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. Assess whether the answer:\n   - Directly addresses all aspects of the question (assign 1 point)\n   - Only partially addresses the question (assign 0.5 points)\n   - Fails to address the question or is off-topic (assign 0 points)\n\n\nReturn a final answer relevancy score between 0 and 1, where:\n- 1.0: Perfect relevancy (completely addresses the question with focus)\n- 0.0: Complete irrelevancy (does not address the question at all)\n\n```\nWhat is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\n```\n\nAnswer: \n```\nThe default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst2-english.\"\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. The main information need in the question is to identify the default checkpoint used in the sentiment analysis pipeline within the Transformers library.\n\n2. The answer directly addresses this question by providing the specific default checkpoint name, \"distilbert-base-uncased-finetuned-sst2-english.\" It completely aligns with the query's intent and information needs, offering a precise response regarding what was asked.\n\nGiven that the answer is entirely relevant and directly addresses the question, I would assign the maximum relevancy score.\n\nSCORE: 1.0",
      "Let's evaluate the generated answer based on the provided context.\n\n1. **Identifying Factual Claims:**\n   - The answer claims: \"The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB.\"\n\n2. **Assessment of Claims:**\n   - The claim states that in Transformers version 4.18.0, the maximum size for a model checkpoint before it is automatically sharded is 10GB. This is explicitly stated in the context provided.\n\n3. **Score Calculation:**\n   - Since the claim is fully supported by the context, it is given 1 point.\n\nIn total, there is 1 factual claim which is fully supported, yielding a faithfulness score of:\n\n\\[\n\\text{Faithfulness Score} = \\frac{\\text{points}}{\\text{total claims}} = \\frac{1}{1} = 1.0\n\\]\n\nThus, the final faithfulness score is:\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant a generated answer is to the given question, regardless of factual accuracy\n\nAnswer Relevancy measures whether the generated answer directly addresses what was asked in the question. An answer is highly relevant if it provides information that specifically responds to the query's intent and information needs.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. Assess whether the answer:\n   - Directly addresses all aspects of the question (assign 1 point)\n   - Only partially addresses the question (assign 0.5 points)\n   - Fails to address the question or is off-topic (assign 0 points)\n\n\nReturn a final answer relevancy score between 0 and 1, where:\n- 1.0: Perfect relevancy (completely addresses the question with focus)\n- 0.0: Complete irrelevancy (does not address the question at all)\n\n```\nWhat is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\n```\n\nAnswer: \n```\nThe default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst2-english.\" This checkpoint is associated with a tokenizer that processes the input sentences for sentiment classification.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "The main information need in the question is to identify the default checkpoint used by the sentiment analysis pipeline in the Transformers library. The answer provided directly addresses this need by stating the specific checkpoint name: \"distilbert-base-uncased-finetuned-sst2-english.\" It goes further to explain that this checkpoint is linked with a tokenizer for sentiment classification, which, while additional information, does not detract from the core answer.\n\nThe answer fully addresses the question without any ambiguity or extraneous information that could confuse the reader. Since the main aspect of the question is sufficiently covered, I will assign a score of 1.0 for perfect relevancy.\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant a generated answer is to the given question, regardless of factual accuracy\n\nAnswer Relevancy measures whether the generated answer directly addresses what was asked in the question. An answer is highly relevant if it provides information that specifically responds to the query's intent and information needs.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. Assess whether the answer:\n   - Directly addresses all aspects of the question (assign 1 point)\n   - Only partially addresses the question (assign 0.5 points)\n   - Fails to address the question or is off-topic (assign 0 points)\n\n\nReturn a final answer relevancy score between 0 and 1, where:\n- 1.0: Perfect relevancy (completely addresses the question with focus)\n- 0.0: Complete irrelevancy (does not address the question at all)\n\n```\nWhat is the purpose of the Safety Checker in the Diffusers library?\n\n```\n\nAnswer: \n```\nThe purpose of the Safety Checker in the Diffusers library is to check and compare class probabilities against a set of guidelines to ensure safety in deployment. It aims to encourage safe usage and contributions within the library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. The main information need in the question is to understand the specific purpose of the Safety Checker in the Diffusers library. This entails looking for an explanation of what the Safety Checker does and how it contributes to safety within the usage of the library.\n\n2. The answer provided directly addresses the question by clearly stating that the Safety Checker is designed to check and compare class probabilities against set guidelines for safety. It also highlights its purpose in promoting safe use and contributions within the library, which aligns with the intent of the question.\n\nGiven that the answer responds comprehensively to the question about the purpose of the Safety Checker without omitting important details or going off-topic, it merits a score of 1.0 for perfect relevancy.\n\nSCORE: 1.0",
      "To evaluate the relevancy of the generated answer to the question, we first identify the main information need in the question. The question asks specifically for the default checkpoint used by the sentiment analysis pipeline in the Transformers library. Consequently, the key aspects of the query include:\n\n1. The term \"default checkpoint\" â€“ indicating a specific model or resource used by the sentiment analysis pipeline.\n2. The context of the \"Transformers library\" â€“ necessitating the answer to be specifically related to this library.\n\nThe answer provided is:\n\"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n\nThis answer directly addresses both aspects of the question:\n1. It clearly states what the default checkpoint is (\"distilbert-base-uncased-finetuned-sst2-english\").\n2. It confirms that this checkpoint is associated with the sentiment analysis pipeline within the context of the Transformers library.\n\nSince the answer completely addresses the question with specific and relevant information without any extraneous details, it scores a perfect relevance.\n\nSCORE: 1.0",
      "Let's analyze the generated answer claim by claim based on the provided context.\n\nClaim 1: \"The `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture\"\n- This claim is fully supported by the context, which states, \"this is the * * x86 _ 64 - unknown - linux - musl * * binary for `tokenizers`.\" Thus, we assign 1 point for this claim.\n\nClaim 2: \"on Linux with the musl C library\"\n- This claim is also fully supported by the context. The context explicitly mentions \"linux - musl\" along with the binary being for the x86_64 architecture. Therefore, we can reasonably infer that it is for Linux with the musl C library. We assign another 1 point for this claim.\n\nNow, let's sum the points:\n- Total points = 1 (for claim 1) + 1 (for claim 2) = 2 points\n- Total number of claims = 2\n\nNow, we calculate the faithfulness score:\n- Faithfulness score = Total points / Total number of claims = 2 / 2 = 1.0\n\nThus, the generated answer is fully faithful to the provided context.\n\nFinal answer:\nSCORE: 1.0",
      "1. The main information need in the question is to identify the default checkpoint associated with the sentiment analysis pipeline in the Transformers library. The user is looking for a specific name or label of this checkpoint.\n\n2. The provided answer directly and clearly states the default checkpoint used in the sentiment analysis pipeline of the Transformers library: \"distilbert-base-uncased-finetuned-sst2-english.\" It completely addresses the query by providing the exact information requested without any additional irrelevant details.\n\nBased on this evaluation, the answer addresses all aspects of the question thoroughly and directly.\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant a generated answer is to the given question, regardless of factual accuracy\n\nAnswer Relevancy measures whether the generated answer directly addresses what was asked in the question. An answer is highly relevant if it provides information that specifically responds to the query's intent and information needs.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. Assess whether the answer:\n   - Directly addresses all aspects of the question (assign 1 point)\n   - Only partially addresses the question (assign 0.5 points)\n   - Fails to address the question or is off-topic (assign 0 points)\n\n\nReturn a final answer relevancy score between 0 and 1, where:\n- 1.0: Perfect relevancy (completely addresses the question with focus)\n- 0.0: Complete irrelevancy (does not address the question at all)\n\n```\nWhat architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\n```\n\nAnswer: \n```\nThe `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on Linux using the musl C library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. The main information need in the question is to identify the architecture for which the `tokenizers-linux-x64-musl` binary is designed. The question specifically asks for the architecture type and references the binary name, emphasizing the relevance of architecture-specific information.\n\n2. The generated answer states that the `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on Linux using the musl C library. This directly addresses the question by providing the specific architecture (x86_64) relevant to the binary in question, along with additional context about the operating system and library type. \n\nSince the answer effectively responds to the query's intent and information needs by specifying the architecture, it can be assessed as fully relevant.\n\nFinal assessment:\n- The answer provides a complete response that directly specifies the architecture of the binary in question.\n\nSCORE: 1.0",
      "Let's analyze the factual claims made in the generated answer against the provided context.\n\n1. Claim: \"The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10 GB.\"\n   - This claim is fully supported by the context provided. The context states that \"model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces.\" This information directly confirms the claim.\n\n2. Claim: \"Checkpoints larger than this size will be split into smaller pieces.\"\n   - This claim is also fully supported by the context. The context explicitly mentions that checkpoints exceeding the 10 GB size will be automatically sharded, thus confirming that they will be split into smaller pieces.\n\n3. Claim: \"You can control the maximum size before sharding using the `max_shard_size` parameter.\"\n   - This claim is fully supported as well. The context indicates, \"you can control the maximum size before sharding with the `max_shard_size` parameter,\" which confirms the claim.\n\nNow, summing up the points:\n- All three claims are fully supported, hence we assign 1 point to each.\n- Total points = 3\n- Total number of claims = 3\n\nThe faithfulness score is calculated as:\nScore = (Total points) / (Total number of claims) = 3 / 3 = 1.0\n\nThus, the final faithfulness score is:\n\nSCORE: 1.0",
      "Let's evaluate the generated answer based on the provided context.\n\n1. **Identifying Factual Claims:**\n   - Claim 1: \"The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`.\"\n   - Claim 2: \"`from_pretrained()` automatically retrieves the relevant pipeline based on the name or path to the pretrained weights.\"\n   - Claim 3: \"`from_pipe()` can be used to transfer components from the original pipeline to a new one without reallocating memory.\"\n\n2. **Assessing Claims Against the Context:**\n   - **Claim 1:** This claim is fully supported by the context because it states that `from_pretrained()` is used for loading checkpoints. (1 point)\n   - **Claim 2:** This is also fully supported as the context clearly explains that `from_pretrained()` automatically retrieves the relevant pipeline based on the name or path to the pretrained weights. (1 point)\n   - **Claim 3:** This claim is fully supported by the context as it indicates that `from_pipe()` can indeed be used to transfer components without reallocating additional memory. (1 point)\n\n3. **Calculating Faithfulness Score:**\n   - Total number of claims = 3\n   - Total points = 1 + 1 + 1 = 3\n   - Faithfulness score = Total points / Total number of claims = 3 / 3 = 1.0\n\nBased on the assessment, the final faithfulness score is:\n\nSCORE: 1.0",
      "Let's evaluate the claims made in the generated answer regarding the provided context about Transformers version 4.18.0 and model checkpoints.\n\n1. **Claim**: \"The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB.\"\n\nNow, let's check the context to see if this claim is supported:\n\n- The context states: \"model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces.\" \n- This indicates that 10GB is indeed the threshold for sharding, confirming the accuracy of the claim.\n\nSince the claim is directly supported by the context without the need for any additional inference, it is \"fully supported.\"\n\nNow, we can assign points based on our evaluation:\n\n- The claim is fully supported by the context: **1 point**\n\nNext, we calculate the faithfulness score:\n\n- Total points = 1\n- Total claims = 1\n- Faithfulness score = (Total points) / (Total claims) = 1 / 1 = 1.0\n\nGiven that all claims made in the generated answer are fully supported by the provided context, the final faithfulness score is:\n\nSCORE: 1.0",
      "Let's evaluate the generated answer based on the provided context:\n\n1. **Factual Claims Identification:**\n   - The answer states: \"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n\n2. **Assessment of Claims:**\n   - The claim about the default checkpoint being \"distilbert-base-uncased-finetuned-sst2-english\" is directly referenced in the provided context, which states: \"the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english.\" \n\n   This claim is explicitly supported by the context as it quotes the same checkpoint.\n\n3. **Scoring Claims:**\n   - The claim is fully supported by the context (1 point).\n\nGiven that there is only one factual claim and it is fully supported:\n\nFaithfulness score calculation:\n- Total Points: 1\n- Total Number of Claims: 1\n\nScore = (Total Points) / (Total Number of Claims) = 1 / 1 = 1.0\n\nThis indicates perfect faithfulness since all claims in the answer are supported by the context.\n\nFinal answer:\nSCORE: 1.0",
      "To evaluate the generated answer for faithfulness based on the provided context, we need to analyze the factual claims made in the answer and see how they align with the context given.\n\n1. **Identifying factual claims in the generated answer:**\n   - The first claim is that \"the default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n\n2. **Determining support for the claim:**\n   - This claim refers to the \"default checkpoint\" used in the sentiment analysis pipeline, specifically naming the checkpoint as \"distilbert-base-uncased-finetuned-sst2-english.\"\n   - The provided context explicitly states: \"the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english.\" Although the formatting differs slightly (use of hyphens and casing), the essence of the claim correlates directly with the factual information in the context.\n\nSince the claim made is fully supported by the provided context, it can be assigned a score of 1 point.\n\n3. **Calculating the faithfulness score:**\n   - Total claims = 1\n   - Fully supported claims = 1 (1 point)\n   \n   Therefore, the faithfulness score is calculated as follows:\n   \\[\n   \\text{Faithfulness Score} = \\frac{\\text{sum of points}}{\\text{total number of claims}} = \\frac{1}{1} = 1.0\n   \\]\n\nHence, the final faithfulness score for the generated answer is:\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant a generated answer is to the given question, regardless of factual accuracy\n\nAnswer Relevancy measures whether the generated answer directly addresses what was asked in the question. An answer is highly relevant if it provides information that specifically responds to the query's intent and information needs.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. Assess whether the answer:\n   - Directly addresses all aspects of the question (assign 1 point)\n   - Only partially addresses the question (assign 0.5 points)\n   - Fails to address the question or is off-topic (assign 0 points)\n\n\nReturn a final answer relevancy score between 0 and 1, where:\n- 1.0: Perfect relevancy (completely addresses the question with focus)\n- 0.0: Complete irrelevancy (does not address the question at all)\n\n```\nWhat is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\n```\n\nAnswer: \n```\nIn Transformers version 4.18.0, model checkpoints that exceed 10 GB in size are automatically sharded into smaller pieces. Each of these smaller pieces will be less than 10 GB. You can control the maximum size before sharding using the `max_shard_size` parameter.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the relevancy of the provided answer to the question, we first need to identify the main information need in the question.\n\n**Main Information Needs**:\n1. The maximum size of a model checkpoint in Transformers version 4.18.0 before it is automatically sharded.\n\nNow, assessing the answer provided:\n\n1. The answer states that model checkpoints that exceed 10 GB are automatically sharded. This directly addresses the maximum size aspect of the question.\n2. The answer also mentions that each smaller piece after sharding will be less than 10 GB, which is supplementary information relevant to the original query.\n3. Lastly, the answer includes information about controlling the maximum size before sharding using the `max_shard_size` parameter. While this is useful, it is not directly necessary to answer the main question, but it does not detract from the relevance either.\n\nOverall, the answer fully addresses the main information need by clearly stating the threshold (10 GB) at which model checkpoints are automatically sharded.\n\nGiven this direct relevance to the query, I would assign a score of 1.0 for the answer.\n\nFinal score: \nSCORE: 1.0",
      "To evaluate the relevancy of the answer to the question, I will first identify the main information needs in the question:\n\n1. The question asks for a specific value: the maximum size of a model checkpoint that triggers automatic sharding in a particular software version (Transformers version 4.18.0). This requires a direct and precise response regarding that size.\n\nNow, assessing the provided answer:\n\n- The answer states, \"The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB.\" \n- This response directly addresses both aspects of the question: it specifies what the maximum size is (10GB) and explicitly mentions the version of Transformers (4.18.0) relevant to the context.\n\nGiven that the answer directly and fully responds to the question without introducing unrelated information or ambiguity, it aligns perfectly with the information needs outlined in the question.\n\nBased on this evaluation, I would assign a perfect relevancy score.\n\nSCORE: 1.0",
      "1. The main information need in the question is to identify the specific default checkpoint utilized by the sentiment analysis pipeline in the Transformers library.\n\n2. Assessment of the context:\n   - The provided context directly answers the question by specifying the default checkpoint: \"distilbert-base-uncased-finetuned-sst2-english.\"\n   - The information is complete and comprehensive in relation to what was asked; it includes not just the name of the checkpoint but also relevant details that indicate it is fine-tuned for sentiment analysis in the English language.\n   - There is no critical information missing, as the context fully satisfies the question's requirements about the default checkpoint.\n\nGiven that the context contains all necessary information to fully answer the question, the overall context relevancy is rated as 1.0.\n\nFinal answer:\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant a generated answer is to the given question, regardless of factual accuracy\n\nAnswer Relevancy measures whether the generated answer directly addresses what was asked in the question. An answer is highly relevant if it provides information that specifically responds to the query's intent and information needs.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. Assess whether the answer:\n   - Directly addresses all aspects of the question (assign 1 point)\n   - Only partially addresses the question (assign 0.5 points)\n   - Fails to address the question or is off-topic (assign 0 points)\n\n\nReturn a final answer relevancy score between 0 and 1, where:\n- 1.0: Perfect relevancy (completely addresses the question with focus)\n- 0.0: Complete irrelevancy (does not address the question at all)\n\n```\nWhat method is used to load a checkpoint for a task using `AutoPipeline`?\n\n```\n\nAnswer: \n```\nThe method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This method automatically retrieves the relevant pipeline based on the name or path to the pretrained weights. Additionally, `from_pipe()` can be used to transfer components from the original pipeline to a new one without reallocating memory.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the relevancy of the answer provided, we first identify the main information need from the question. The question asks specifically for the method used to load a checkpoint for a task using `AutoPipeline`. This implies an interest in identifying which particular method or functions can be employed to perform this operation.\n\nNow, let's analyze the answer:\n\n1. **Directly mentions the method**: The answer specifically states that the method `from_pretrained()` is used to load a checkpoint, which directly addresses the main information need.\n2. **Additional relevant information**: The answer also includes the method `from_pipe()`, describing its purpose in relation to `AutoPipeline`. While this additional information is relevant to the context, it is not explicitly asked for in the question.\n\nSince the answer directly addresses the method needed to load a checkpoint (which is the core aspect of the question), it is highly relevant. There is no ambiguity or deviation from the asked question, making the answer complete in its response.\n\nDue to the directness and clarity of the provided answer regarding the specific method for loading a checkpoint, I would assign a score of 1.0.\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant a generated answer is to the given question, regardless of factual accuracy\n\nAnswer Relevancy measures whether the generated answer directly addresses what was asked in the question. An answer is highly relevant if it provides information that specifically responds to the query's intent and information needs.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. Assess whether the answer:\n   - Directly addresses all aspects of the question (assign 1 point)\n   - Only partially addresses the question (assign 0.5 points)\n   - Fails to address the question or is off-topic (assign 0 points)\n\n\nReturn a final answer relevancy score between 0 and 1, where:\n- 1.0: Perfect relevancy (completely addresses the question with focus)\n- 0.0: Complete irrelevancy (does not address the question at all)\n\n```\nWhat method is used to load a checkpoint for a task using `AutoPipeline`?\n\n```\n\nAnswer: \n```\nThe method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This method downloads and caches the configuration and vocabulary associated with the specified checkpoint. It allows for easy loading of the relevant pipeline without needing to know the specific class to use.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the relevance of the generated answer to the question, let's analyze the components involved.\n\n1. **Main Information Need(s) in the Question:**\n   The question asks for a method to load a checkpoint using `AutoPipeline`. It seeks specific information about the method name and potentially how it functions in the context of `AutoPipeline`.\n\n2. **Assessment of the Answer:**\n   The answer directly states that the method is `from_pretrained()`, which is exactly what the question asks for. It also provides additional relevant context about what this method does, explaining that it downloads configuration and vocabulary associated with the checkpoint, and facilitates loading the pipeline without knowing the specific class to use. This additional detail does not stray from the question's intent; instead, it enhances understanding.\n\nSince the answer directly addresses the core of the question and expands on it without being off-topic, it fully meets the requirements.\n\nGiven this analysis:\n- The answer directly addresses all aspects of the question by naming the method and briefly explaining its purpose in loading a checkpoint.\n\nThus, I assign a relevancy score of 1.0.\n\nFinal answer:\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant a generated answer is to the given question, regardless of factual accuracy\n\nAnswer Relevancy measures whether the generated answer directly addresses what was asked in the question. An answer is highly relevant if it provides information that specifically responds to the query's intent and information needs.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. Assess whether the answer:\n   - Directly addresses all aspects of the question (assign 1 point)\n   - Only partially addresses the question (assign 0.5 points)\n   - Fails to address the question or is off-topic (assign 0 points)\n\n\nReturn a final answer relevancy score between 0 and 1, where:\n- 1.0: Perfect relevancy (completely addresses the question with focus)\n- 0.0: Complete irrelevancy (does not address the question at all)\n\n```\nWhat architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\n```\n\nAnswer: \n```\nThe `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture. It is specifically built for the unknown Linux environment using the musl C library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the relevancy of the generated answer to the question, let's break down the question and the answer.\n\n1. **Main Information Needs of the Question:**\n   - The question asks specifically about the architecture that the `tokenizers-linux-x64-musl` binary is designed for. The key focus is on identifying the system architecture associated with this binary.\n\n2. **Assessment of the Answer:**\n   - The answer specifies that the `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture. This directly addresses the primary information need of the question.\n   - Additionally, it provides supplementary information by mentioning it is built for an \"unknown Linux environment using the musl C library.\" While this additional information is interesting, the core requirement of identifying the architecture is fully satisfied.\n\nBased on this assessment, the answer addresses all aspects of the question by clearly stating the architecture and providing context. Therefore, the answer is highly relevant.\n\nSCORE: 1.0",
      "1. The main information need in the question is to identify the default checkpoint used by the sentiment analysis pipeline in the Transformers library.\n\n2. Assessing the context:\n   - The context provides a direct answer to the question by stating that the default checkpoint is \"distilbert-base-uncased-finetuned-sst2-english.\" This is precisely what the question asks for.\n   - The information presented is comprehensive in that it fully specifies the checkpoint, which is the key detail needed to answer the question.\n   - There are no critical pieces of information missing; the context correctly and succinctly answers the inquiry.\n\n3. Given that the context contains all relevant information necessary to answer the question completely and accurately:\n   - I rate the overall context relevancy as 1.0.\n\nSCORE: 1.0",
      "To evaluate the faithfulness of the generated answer, we first identify the factual claims present in it. The answer makes a single claim:\n\n1. The default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst2-english.\"\n\nNow, we need to check if this claim is supported by the provided context.\n\nIn the context, it is mentioned: \"the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english.\" This matches exactly with the claim made in the answer, except for the minor difference in punctuation (hyphens and capitalization).\n\nSince the claim is directly supported by the context without any need for extrapolation or interpretation, we assign it a score of 1 point.\n\nNow, we summarize our findings:\n- The total number of claims is 1.\n- The total points scored from assessing the claims is 1.\n\nThus, the faithfulness score is calculated as follows:\nFaithfulness Score = (Sum of Points) / (Total Number of Claims) = 1 / 1 = 1.0\n\nBased on this assessment, the final faithfulness score is:\n\nSCORE: 1.0",
      "1. **Information Need**: The question asks for the maximum size of a model checkpoint that triggers automatic sharding in a specific version of the Transformers library, version 4.18.0. The primary need is to identify this size limit.\n\n2. **Context Assessment**:\n   - The provided context directly answers the question by stating that the maximum size of a model checkpoint before automatic sharding occurs in Transformers version 4.18.0 is 10GB. \n   - The relevant information is comprehensive as it specifies the size limit and the version of the library, which is essential for accurately answering the question.\n   - There are no critical pieces of information missing since the context succinctly delivers the exact answer needed.\n\n3. **Overall Context Relevancy Rating**: Since the context fully addresses the question with the precise information provided, I rate the overall context relevancy as perfect.\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\n```\n\nContext: \n```\nThe default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst2-english.\" This checkpoint is associated with a tokenizer that processes the input sentences for sentiment classification.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. The main information need in the question is to identify the default checkpoint used by the sentiment analysis pipeline in the Transformers library.\n\n2. Assessing the provided context:\n   - The context explicitly states that the default checkpoint used by the sentiment analysis pipeline is \"distilbert-base-uncased-finetuned-sst2-english.\" This is the exact answer to the question, making it highly relevant.\n   - The context also provides additional information about the checkpoint being associated with a tokenizer that processes input sentences for sentiment classification. This supplementary detail enhances the comprehensiveness of the response but is not strictly necessary to answer the question directly.\n   - There is no critical information missing regarding the specific default checkpoint requested; it is clearly answered.\n\nGiven that the context fully answers the question and includes relevant supplementary information, I rate the overall context relevancy as 1.0.\n\nSCORE: 1.0",
      "1. The main information need in the question is to identify the specific maximum size limit for a model checkpoint before it is automatically sharded in the specified version of the Transformers library (version 4.18.0).\n\n2. Assessing the provided context:\n   - The context contains the information that the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB.\n   - This piece of information is directly relevant to the question as it provides the exact limit being asked for.\n   - The information is complete and answers the question without any gaps or omissions regarding the size limit.\n\n3. Rating the overall context relevancy:\n   - The context perfectly answers the question with all the necessary information provided, which leads to a SCORE of 1.0.\n\nTherefore, my final answer is:\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the precision of the retrieved context relative to the question\n\nContext Precision measures how efficiently the retrieved information is used to answer the question. High precision means most of the retrieved context was necessary and used in generating the answer, with minimal irrelevant information.\n\nEvaluation instructions:\n1. Identify which parts of the context were actually used to generate the answer.\n2. Calculate the proportion of the context that contains information relevant to and used in the answer.\n3. Consider both:\n   - Information density (how much of the context is relevant)\n   - Information utilization (how much of the relevant information was used)\n\n4. Rate the context precision on a scale from 0 to 1:\n   - 1.0: Perfect precision (all retrieved context is relevant and used efficiently)\n   - 0.5: Moderate precision (about half the context is relevant and used)\n   - 0.0: No precision (context is entirely irrelevant)\n\n```\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\n```\nContext: the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nlibrary. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in\n\nsteps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage,\n\nits unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,\n\nAnswer: \n```\nThe default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst2-english.\"\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the context precision, let's break down the context and analyze it against the question.\n\n1. **Identify Relevant Parts of the Context:** \n   - The sentence that directly addresses the question is: \"the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english.\"\n   - This part explicitly answers the question about the default checkpoint.\n\n2. **Assess Information Density and Utilization:** \n   - Relevant Information: The mentioned sentence is closely related to the question and provides a direct answer. \n   - Additional context: The rest of the text talks about tokenization and other methods related to the Transformers library and sentiment analysis but does not specifically answer the question regarding the checkpoint.\n\n3. **Calculate Proportion of Relevant Context Used:**\n   - Total context length: There are several sentences presented as context (roughly 6-7 sentences).\n   - Relevant context: Only the first sentence directly provides the needed information. \n   - Therefore, relevant context accounts for approximately 1 out of 6 or 7 sentences.\n\n4. **Final Evaluation:** \n   - Information Density: The relevant portion is about 14% of the context, indicating a low density.\n   - Information Utilization: The directly relevant information (the checkpoint name) was utilized completely in the answer.\n   - As such, while the critical information was used, a significant portion of the context is irrelevant.\n\nGiven that only a small part of the context is relevant and used efficiently, I would rate the precision as moderate.\n\nSCORE: 0.5",
      "Let's analyze the generated answer for faithfulness based on the provided context. \n\n### Factual Claims in the Answer:\n1. The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`.\n2. This method downloads and caches the configuration and vocabulary associated with the specified checkpoint.\n3. It allows for easy loading of the relevant pipeline without needing to know the specific class to use.\n\n### Evaluation of Claims:\n1. Claim 1: \"The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`.\"\n   - This claim is fully supported by the context, as it explicitly states that \"the method of this class is from_pretrained.\"\n   - **Score: 1 point**\n  \n2. Claim 2: \"This method downloads and caches the configuration and vocabulary associated with the specified checkpoint.\"\n   - This claim is also fully supported by the context, which mentions that `from_pretrained` will \"download and cache the configuration and the vocabulary associated to a given checkpoint.\"\n   - **Score: 1 point**\n  \n3. Claim 3: \"It allows for easy loading of the relevant pipeline without needing to know the specific class to use.\"\n   - This claim is fully supported by the context, which explicitly states that `AutoPipeline` makes it easy for users to load a checkpoint for a task without knowing the specific pipeline class.\n   - **Score: 1 point**\n\n### Calculation of the Faithfulness Score:\n- Total number of claims: 3\n- Total points earned: 3 (1 + 1 + 1)\n\nFaithfulness Score = (Total points) / (Total number of claims) = 3 / 3 = 1.0\n\n### Final Answer:\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\n```\n\nContext: \n```\nThe purpose of the Safety Checker in the Diffusers library is to check and compare class probabilities against a set of guidelines to ensure safety in deployment. It aims to encourage safe usage and contributions within the library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. **Identify the main information need in the question**: The question asks for the specific purpose of the Safety Checker in the Diffusers library. The key information needed is a clear explanation of what the Safety Checker does, what it checks, and its intended goals in the context of deployment and usage.\n\n2. **Assess the context**:\n   - **Relevance**: The context directly addresses the purpose of the Safety Checker by stating that it checks and compares class probabilities against a set of guidelines for safety in deployment. This information is pertinent and provides insight into what the Safety Checker is intended to do.\n   - **Comprehensiveness**: The context provides a solid understanding of the function of the Safety Checker, mentioning both the checking of probabilities and the encouragement of safe usage and contributions within the library. This covers the main aspects required to understand its purpose.\n   - **Critical Information**: There is no critical information missing; the context effectively captures the essence of the Safety Checkerâ€™s role within the Diffusers library.\n\nGiven that the context thoroughly explains the Safety Checkerâ€™s purpose and does not leave out important details, the overall relevancy can be rated as perfect.\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\n```\n\nContext: \n```\nThe `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on a Linux system using the musl C library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. **Identify the main information need(s) in the question**:\n   The question is asking specifically about the architecture that the `tokenizers-linux-x64-musl` binary is intended for.\n\n2. **Assess the context**:\n   - The context provides clear information stating that the `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on a Linux system using the musl C library. \n   - Since the question is explicitly inquiring about the architecture, the information that it is designed for the x86_64 architecture is directly relevant and explicitly answers the question.\n   - There are no gaps in the information provided that would hinder understanding or completeness concerning the architecture, as it gives the necessary details without omission.\n\n3. **Rate the overall context relevancy**:\n   Given that the context perfectly answers the specific question about the architecture of the binary, I would rate it with the highest score.\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\n```\n\nContext: \n```\nThe method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This method downloads and caches the configuration and vocabulary associated with the specified checkpoint. It allows for easy loading of the relevant pipeline without needing to know the specific class to use.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. **Identify the main information need(s) in the question**:\n   The main information need is to identify the specific method used to load a checkpoint in `AutoPipeline`, as well as any pertinent details about its function.\n\n2. **Assess the retrieved context**:\n   - **Relevance**: The context directly answers the question by identifying the method `from_pretrained()` to load a checkpoint. It also provides information about what this method does, including its role in downloading and caching the configuration and vocabulary associated with the checkpoint.\n   - **Comprehensiveness**: The relevant information is quite comprehensive. It not only names the method, but it also explains its purpose in the context of using `AutoPipeline`, indicating that it helps load the relevant pipeline without needing to know the specific class.\n   - **Critical Information**: There doesn't appear to be any critical information missing that would impede a complete understanding of how to load a checkpoint using `AutoPipeline`.\n\n3. **Overall context relevancy rating**: The context provided is highly relevant, contains all necessary information to fully answer the question, and gives additional context about the method's functionality in `AutoPipeline`.\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the precision of the retrieved context relative to the question\n\nContext Precision measures how efficiently the retrieved information is used to answer the question. High precision means most of the retrieved context was necessary and used in generating the answer, with minimal irrelevant information.\n\nEvaluation instructions:\n1. Identify which parts of the context were actually used to generate the answer.\n2. Calculate the proportion of the context that contains information relevant to and used in the answer.\n3. Consider both:\n   - Information density (how much of the context is relevant)\n   - Information utilization (how much of the relevant information was used)\n\n4. Rate the context precision on a scale from 0 to 1:\n   - 1.0: Perfect precision (all retrieved context is relevant and used efficiently)\n   - 0.5: Moderate precision (about half the context is relevant and used)\n   - 0.0: No precision (context is entirely irrelevant)\n\n```\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\n```\nContext: `tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n\n-->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\non linux machines. \r\n\r\nIf `HF_HOME` is not set, the default home will be `\"$XDG_CACHE_HOME/huggingface\"` instead\r\nof `\"~/.cache/huggingface\"`.\n\n[on macOS and Linux](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/)\r\nand on [Windows](https://phoenixnap.com/kb/windows-set-environment-variable).\n\nattention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the\n\nAnswer: \n```\nThe `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on Linux using the musl C library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the precision of the retrieved context with respect to the question, we first identify the relevant parts of the context that contributed to answering the question.\n\n### Step 1: Identify Relevant Parts of the Context\nThe context contains the line: \n\n- \"This is the **x86_64-unknown-linux-musl** binary for `tokenizers`\".\n\nThis line provides key information needed to answer the question about the architecture for which the `tokenizers-linux-x64-musl` binary is designed. Specifically, \"x86_64\" indicates the architecture, \"linux\" indicates the operating system, and \"musl\" indicates the C library used.\n\n### Step 2: Information Density and Utilization\n- **Information Density**: The relevant information (the architecture being x86_64) is condensed in the first line. There is no additional relevant information that pertains to the architecture in the broader context, as the rest primarily discusses unrelated features of `AutoPipeline` and environment settings.\n  \n- **Information Utilization**: The answer provided directly cites \"x86_64 architecture on Linux using the musl C library,\" which is fully derived from the key phrase in the context. The most pertinent information about the architecture (x86_64) is effectively used in the answer.\n\n### Conclusion\nThe relevant context accurately provides the information needed to generate the answer. Given this analysis:\n\n- The relevant information (about the architecture) is highly concentrated in a small part of the retrieved context.\n- The relevant information is fully utilized in answering the question.\n\nHence, the assessment of context precision would be a score of:\n\nSCORE: 1.0",
      "1. **Identify the main information need(s) in the question**:\n   - The question is asking for the specific default checkpoint that is used by the sentiment analysis pipeline in the Transformers library.\n\n2. **Assessment of the context**:\n   - The provided context states: \"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n   - This information directly answers the question by naming the exact default checkpoint used.\n   - The information is comprehensive, as it includes the full name of the checkpoint, which is necessary for a complete answer.\n   - There is no critical information missing; everything needed to answer the question is present.\n\n3. **Rate the overall context relevancy**:\n   - Since the context perfectly matches the information need of the question and provides all required details without gaps, it deserves a rating of 1.0.\n\nFinal answer:\nSCORE: 1.0",
      "Let's evaluate the claims in the generated answer based on the provided context.\n\n**Claim 1**: \"The `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture.\"\n- This claim is fully supported by the context, which identifies `tokenizers-linux-x64-musl` as the binary for **x86_64-unknown-linux-musl**.\n- **Score for Claim 1**: 1 point\n\n**Claim 2**: \"It is specifically built for the unknown Linux environment using the musl C library.\"\n- The context refers to the binary as `x86_64-unknown-linux-musl`, implying it is built for an unknown Linux environment and specifically mentions the musl C library.\n- While the term \"unknown Linux environment\" is not explicitly stated in the context, \"unknown\" is inferred from \"unknown-linux-musl\". Thus, the claim is partially supported by inference.\n- **Score for Claim 2**: 0.5 points\n\nNow, we summarize the scoring:\n- Total points: 1 (for Claim 1) + 0.5 (for Claim 2) = 1.5 points\n- Total number of claims: 2\n\nWe calculate the faithfulness score as follows:\n\\[ \\text{Faithfulness Score} = \\frac{\\text{Total Points}}{\\text{Total Claims}} = \\frac{1.5}{2} = 0.75 \\]\n\nThus, the final faithfulness score is:\n\nSCORE: 0.75",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\n```\n\nContext: \n```\nThe `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture. It is specifically built for the unknown Linux environment using the musl C library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. The main information need in the question is to identify the architecture that the `tokenizers-linux-x64-musl` binary is designed for.\n\n2. Evaluating the provided context:\n   - The context states that the `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture. This directly answers the question about which architecture the binary supports.\n   - The context also mentions that it is specifically built for an unknown Linux environment using the musl C library. While this detail is informative, the primary need is about the architecture type.\n   - There are no critical pieces of information missing regarding the architecture itself, as the context effectively includes the necessary detail that the binary is for the x86_64 architecture.\n\n3. The context is highly relevant to answering the question, providing complete and direct information pertaining to the architecture of the binary.\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the precision of the retrieved context relative to the question\n\nContext Precision measures how efficiently the retrieved information is used to answer the question. High precision means most of the retrieved context was necessary and used in generating the answer, with minimal irrelevant information.\n\nEvaluation instructions:\n1. Identify which parts of the context were actually used to generate the answer.\n2. Calculate the proportion of the context that contains information relevant to and used in the answer.\n3. Consider both:\n   - Information density (how much of the context is relevant)\n   - Information utilization (how much of the relevant information was used)\n\n4. Rate the context precision on a scale from 0 to 1:\n   - 1.0: Perfect precision (all retrieved context is relevant and used efficiently)\n   - 0.5: Moderate precision (about half the context is relevant and used)\n   - 0.0: No precision (context is entirely irrelevant)\n\n```\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\n```\nContext: Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional\n\nattention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should\n\nclass. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after\n\nAnswer: \n```\nThe maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the context precision relative to the question, let's follow the instructions step by step.\n\n1. **Identify used parts of the context**: \n   The relevant information used in the answer is:\n   - \"model checkpoints that end up taking more than 10GB of space are automatically sharded\"\n\n   The answer provided states that the maximum size of a model checkpoint before it is automatically sharded is 10GB, which directly corresponds to this part of the context.\n\n2. **Calculate the proportion of relevant context**:\n   - The entire context includes several topics, some of which are directly relevant to the question (the sharding point) while others delve into unrelated subjects (like the attention mask, tokenization step, using pre-trained models, etc.).\n   - The specific sentence that addresses the sharding concerns the maximum size being mentioned explicitly. \n\n   The total length of the context provided is considerable, but only one sentence relates directly to the question asked.\n\n3. **Information density and utilization**:\n   - **Information density**: Out of the entire context, only a fraction (one specific sentence) pertains to the question.\n   - **Information utilization**: The relevant information was accurately utilized to provide the answer.\n\nGiven that only a single piece of information (the 10GB threshold) from a larger block of potentially irrelevant information was used, but the relevant information is indeed correct and efficiently used, we can assess the precision.\n\n4. **Rate the context precision**:\n   - Since approximately only one sentence out of many is relevant (which we can estimate as about 1 out of 5 or so sentences), the context holds some relevance. Therefore, the density is low, but the utilization is complete for that relevant segment.\n\nTaking both into account, I would rate the precision moderately since only a minimal amount of context was needed for the answer, but it was directly used effectively.\n\nUltimately, I would assign the score based on the above considerations.\n\nSCORE: 0.5",
      "1. The main information need in the question is to identify the architecture that the `tokenizers-linux-x64-musl` binary is designed for.\n\n2. Assessing the context:\n   - The context explicitly states that the `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on Linux. This directly answers the question regarding the architecture.\n   - The information provided is comprehensive as it specifies both the architecture (x86_64) and the operating system (Linux) used in conjunction with the musl C library, which adds useful details about the environment in which the binary operates.\n   - There is no critical information missing related to what architecture is being targeted, as the key point (x86_64) is clearly stated.\n\n3. Given that the context provides all the necessary information to fully answer the question about the architecture of the binary, I would rate the overall context relevancy as perfect.\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\n```\n\nContext: \n```\nThe purpose of the Safety Checker in the Diffusers library is to mitigate inappropriate degeneration in diffusion models by checking and comparing the class probability of harmful concepts against generated images. It uses hard-coded harmful concepts that are intentionally hidden to prevent reverse engineering. This mechanism helps ensure safer deployment of models like stable diffusion, which may be trained on unfiltered datasets.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. **Identify the main information need(s) in the question:**\n   - The question seeks to understand the specific purpose of the Safety Checker in the Diffusers library.\n\n2. **Assess the context:**\n   - **Direct relevance:** The context provides a clear explanation of what the Safety Checker does, specifically mentioning its role in mitigating inappropriate degeneration in diffusion models and how it achieves this by checking class probabilities against harmful concepts in generated images.\n   - **Comprehensive relevant information:** The context thoroughly describes the mechanism of the Safety Checker, including its use of hard-coded harmful concepts and its importance for the safe deployment of models like stable diffusion.\n   - **Missing information:** There doesn't appear to be critical information missing; the explanation is comprehensive and addresses the purpose outlined in the question.\n\n3. **Rating overall context relevancy:**\n   The context is highly relevant as it directly answers the question by explaining the purpose of the Safety Checker in a detailed and clear manner.\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the precision of the retrieved context relative to the question\n\nContext Precision measures how efficiently the retrieved information is used to answer the question. High precision means most of the retrieved context was necessary and used in generating the answer, with minimal irrelevant information.\n\nEvaluation instructions:\n1. Identify which parts of the context were actually used to generate the answer.\n2. Calculate the proportion of the context that contains information relevant to and used in the answer.\n3. Consider both:\n   - Information density (how much of the context is relevant)\n   - Information utilization (how much of the relevant information was used)\n\n4. Rate the context precision on a scale from 0 to 1:\n   - 1.0: Perfect precision (all retrieved context is relevant and used efficiently)\n   - 0.5: Moderate precision (about half the context is relevant and used)\n   - 0.0: No precision (context is entirely irrelevant)\n\n```\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\n```\nContext: the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nto a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all\n\nremoved. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. to get an output linked to our classification problem, we need to use the automodelforsequenceclassification class. it works exactly as the automodel class, except that it will build a model with a classification head. there is one auto class for each common nlp task in the transformers library. here, after giving our model the two sentences, we get a tensor of size two by two : one result for each sentence and for each possible label. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third\n\n. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs.\n\nAnswer: \n```\nThe default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst2-english.\"\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the context precision for the provided question, we can follow these steps:\n\n1. **Identify Relevant Parts of the Context**: The section of the context that is vital for answering the question directly states: \"the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english.\" This phrase explicitly mentions the default checkpoint relevant to the question.\n\n2. **Calculate Context Proportion**:\n   - **Relevant information**: The relevant information is concentrated in a single part of the context, where the default checkpoint is mentioned. \n   - **Total context length**: The total context provided is fairly extensive, consisting of multiple sentences covering different aspects of the process in the Transformers library.\n   - **Information density**: There is only one concise piece of information concerning the question. The rest of the context primarily discusses tokenization and model processing.\n\n3. **Information Utilization**: \n   - In generating the answer, the exact wording from the identified relevant part was effectively utilized as it directly answered the question without requiring additional elaboration.\n\nGiven this analysis:\n- The relevant portion contains only one coherent sentence within a broad context, leading to low information density.\n- However, since the relevant information was fully utilized in forming the answer, this improves the score slightly.\n\nConsidering the balance between information density and utilization, I would assign a score of around 0.7 â€“ indicating that while the relevant information was utilized well, the information density is low in relation to the entire context available.\n\nThus, the final formatted score is:\n\nSCORE: 0.7",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant a generated answer is to the given question, regardless of factual accuracy\n\nAnswer Relevancy measures whether the generated answer directly addresses what was asked in the question. An answer is highly relevant if it provides information that specifically responds to the query's intent and information needs.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. Assess whether the answer:\n   - Directly addresses all aspects of the question (assign 1 point)\n   - Only partially addresses the question (assign 0.5 points)\n   - Fails to address the question or is off-topic (assign 0 points)\n\n\nReturn a final answer relevancy score between 0 and 1, where:\n- 1.0: Perfect relevancy (completely addresses the question with focus)\n- 0.0: Complete irrelevancy (does not address the question at all)\n\n```\nWhat method is used to load a checkpoint for a task using `AutoPipeline`?\n\n```\n\nAnswer: \n```\nTo load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method, which automatically retrieves the relevant pipeline given the name or path to the pretrained weights. Additionally, to switch between tasks with the same checkpoint without reallocating memory, the `from_pipe()` method can be used to transfer components from the original pipeline to the new one.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the relevancy of the answer to the question, I will first identify the main information needs expressed in the query:\n\n1. The primary need is to know the method used to load a checkpoint for a task utilizing the `AutoPipeline`.\n2. The question implicitly seeks specific details about this method, such as its name and perhaps its functionality.\n\nNow, assessing the provided answer:\n\n- The answer directly addresses the question by identifying the `from_pretrained()` method, clearly stating that it is used to load a checkpoint for a task with `AutoPipeline`. This directly aligns with the main information need of the question.\n- Additionally, it also mentions the `from_pipe()` method, which, although not explicitly asked for, is relevant as it provides useful information regarding the management of checkpoints and tasks in `AutoPipeline`. This extra information does not detract from the answer's primary focus.\n- Overall, the answer is clear, focused, and responds directly to the query about loading checkpoints in `AutoPipeline`.\n\nSince the answer fully addresses the main information need of the question, it merits a score reflecting perfect relevancy.\n\nFinal score: \nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the precision of the retrieved context relative to the question\n\nContext Precision measures how efficiently the retrieved information is used to answer the question. High precision means most of the retrieved context was necessary and used in generating the answer, with minimal irrelevant information.\n\nEvaluation instructions:\n1. Identify which parts of the context were actually used to generate the answer.\n2. Calculate the proportion of the context that contains information relevant to and used in the answer.\n3. Consider both:\n   - Information density (how much of the context is relevant)\n   - Information utilization (how much of the relevant information was used)\n\n4. Rate the context precision on a scale from 0 to 1:\n   - 1.0: Perfect precision (all retrieved context is relevant and used efficiently)\n   - 0.5: Moderate precision (about half the context is relevant and used)\n   - 0.0: No precision (context is entirely irrelevant)\n\n```\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\n```\nContext: Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do\n\n```py\r\n>>> from transformers.modeling_utils import load_sharded_checkpoint\r\n\r\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\r\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n\nfor a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the\n\nAnswer: \n```\nThe maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the context precision for the provided question, I will analyze the content of the context against the question's requirements.\n\n1. **Identifying Relevant Parts of Context**:\n   The key information from the context is: \"model checkpoints that end up taking more than 10GB of space are automatically sharded\". This directly answers the question about the maximum size of a model checkpoint before it is sharded.\n\n2. **Utilization of Relevant Information**:\n   The answer relies solely on the phrase about the 10GB limit. There are no other parts of the context required to understand or support this information. The rest of the context discusses sharded checkpoints, their advantages, and a specific example of saving a model, which is not necessary for answering the question.\n\n3. **Calculating Proportion of Context**:\n   - The relevant statement is about 10GB.\n   - The overall length of the context seems to be significantly larger, but only a small portion (the sentence about 10GB) is relevant.\n   - Therefore, we have high information density concerning the relevant part (it is a direct answer), but low overall context utilization (as most of the remaining context is irrelevant).\n\n4. **Rating the Context Precision**:\n   Since the majority of the context is irrelevant, and only a specific statement directly used in generating the answer, I would assign a score reflecting that disparity.\n\nGiven the analysis:\n- Information density shows that 1 out of several sentences addresses the question, giving a low overall context relevance ratio.\n- Information utilization rates high for the relevant sentence but highlights an overall low because of the amount of irrelevant content.\n\nThus, I rate the context precision as follows:\n- Relevancy: Only one sentence is relevant.\n- Utilization: That sentence is used entirely, but the context as a whole remains cluttered with unnecessary details.\n\n**Final Score**: I would assign a precision score of 0.5, as half of the context could theoretically be relevant if focused properly on the specific answer provided.\n\nTherefore, the final output is:\nSCORE: 0.5",
      "To evaluate the faithfulness of the generated answer, we need to break down the factual claims within it and compare them to the provided context.\n\n### Factual Claims in the Answer:\n1. \"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n2. \"This checkpoint is associated with a tokenizer that processes the input sentences for sentiment classification.\"\n\n### Evaluation of Each Claim:\n1. **Claim 1**: \"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n   - **Support**: This claim is directly supported by the context. The context states, \"the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english.\"\n   - **Score**: 1 point\n\n2. **Claim 2**: \"This checkpoint is associated with a tokenizer that processes the input sentences for sentiment classification.\"\n   - **Support**: The context mentions \"we instantiate a tokenizer associated with that checkpoint,\" which infers that the checkpoint is related to a tokenizer used for processing input sentences. However, it is not explicitly stated that this tokenizer processes the sentences specifically for sentiment classification; that detail is inferred.\n   - **Score**: 0.5 points (partially supported through reasonable inference)\n\n### Calculation of the Faithfulness Score:\n- Total factual claims: 2\n- Points assigned: 1 (for Claim 1) + 0.5 (for Claim 2) = 1.5\n\n**Faithfulness Score Calculation**:\n\\[ \\text{Faithfulness Score} = \\frac{\\text{Total Points}}{\\text{Total Claims}} = \\frac{1.5}{2} = 0.75 \\]\n\nThus, the final faithfulness score based on the assessment of the claims and their support from the context is:\n\nSCORE: 0.75",
      "To evaluate the relevancy of the provided context regarding the question about the purpose of the Safety Checker in the Diffusers library, we can break down the task as follows:\n\n1. **Identify the main information need**: The question is inquiring specifically about the purpose of the Safety Checker within the Diffusers library, which suggests that we are looking for a clear statement or explanation of what this component does and why it is important.\n\n2. **Assess the context**:\n   - The provided context directly addresses the question by stating that the Safety Checker checks and compares class probabilities of harmful concepts against generated images. This is highly relevant as it highlights how the Safety Checker functions to mitigate inappropriate content generation.\n   - The context elaborates that its aim is to prevent harmful content from being generated, which adds to the comprehensiveness regarding its purpose.\n   - It also mentions that harmful concepts are hidden to protect the integrity of the Safety Checker, providing insight into the mechanisms and considerations involved in its operation.\n\nIn summary, the retrieved context thoroughly explains the purpose of the Safety Checker, including key functions and goals. There are no apparent critical gaps, and the information is directly related to the question asked.\n\nGiven all of the above, the overall context relevancy rate is evaluated as:\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\n```\n\nContext: \n```\nTo load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method, which automatically retrieves the relevant pipeline given the name or path to the pretrained weights. Additionally, to switch between tasks with the same checkpoint without reallocating memory, the `from_pipe()` method can be used to transfer components from the original pipeline to the new one.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. **Identify the main information need(s) in the question**:\n   The main information need in the question is to determine the specific method used to load a checkpoint for a task when utilizing `AutoPipeline`. The question specifically asks for the method involved in this process.\n\n2. **Assessment of the context**:\n   - The context states that the method to load a checkpoint is the `from_pretrained()` method, which directly answers the question. This is the primary piece of information needed.\n   - Additionally, the context provides supplementary information about the `from_pipe()` method, which is not directly asked in the question but adds value by elaborating on how to manage tasks without reallocating memory. This supplementary detail can be considered relevant but is not strictly necessary to answer the main question.\n\n3. **Overall Context Relevancy Rating**:\n   - The context contains information that directly answers the question regarding the method for loading a checkpoint, making it highly relevant.\n   - The explanation is clear and provides a full response to the asked question. Hence, there are no significant gaps in the information provided, and no critical information is missing.\n\nGiven this evaluation, the appropriate score for the context relevancy is:\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the precision of the retrieved context relative to the question\n\nContext Precision measures how efficiently the retrieved information is used to answer the question. High precision means most of the retrieved context was necessary and used in generating the answer, with minimal irrelevant information.\n\nEvaluation instructions:\n1. Identify which parts of the context were actually used to generate the answer.\n2. Calculate the proportion of the context that contains information relevant to and used in the answer.\n3. Consider both:\n   - Information density (how much of the context is relevant)\n   - Information utilization (how much of the relevant information was used)\n\n4. Rate the context precision on a scale from 0 to 1:\n   - 1.0: Perfect precision (all retrieved context is relevant and used efficiently)\n   - 0.5: Moderate precision (about half the context is relevant and used)\n   - 0.0: No precision (context is entirely irrelevant)\n\n```\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\n```\nContext: -->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should\n\nthe checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\n\nAnswer: \n```\nThe method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This method downloads and caches the configuration and vocabulary associated with the specified checkpoint. It allows for easy loading of the relevant pipeline without needing to know the specific class to use.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To assess the precision of the retrieved context relative to the question about the method used to load a checkpoint for a task using `AutoPipeline`, let's dissect the provided context step by step.\n\n1. Identifying relevant parts of the context:\n   - **Key point**: The context mentions `from_pretrained()` as the method used for loading a checkpoint: \"method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint.\" This directly answers the question.\n   - The context also states: \"make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use,\" which supports the claim about the utility of `from_pretrained()`.\n   - The mention of the sentiment analysis pipeline and tokenizer details appear to be less relevant for directly answering the question about the method used.\n\n2. Calculating the proportion of relevant context:\n   - The relevant part of the context that was used for generating the answer consists mainly of the explanation of the `from_pretrained()` method.\n   - Additional context provides explanation about its utility (loading without needing specific class knowledge) but is less critical.\n   - The detail about the default checkpoint for sentiment analysis is not necessary to answer the specific question and could be considered irrelevant.\n\n3. Evaluating information density and utilization:\n   - Information density is reasonably high for the relevant part concerning `from_pretrained()`, which is central to the question.\n   - Information utilization is also high, as the critical information about `from_pretrained()` was effectively used to craft the answer.\n\nGiven the breakdown, most of the relevant context pertaining to the answer is succinct and utilized appropriately in responding to the question. However, some additional less relevant context is included that doesn't impact understanding the answer.\n\nConsidering all these points, I would rate the context precision as follows:\n- Relevant information is approximately 70% of the total retrieved context.\n- Of that relevant information, about 90% was effectively utilized in generating the answer.\n\nThus, the overall precision would land closer to a score of 0.8, accounting for a combination of high relevance and effective utilization with minor inclusion of irrelevant details.\n\nFinal score based on the evaluation is:\n\nSCORE: 0.8",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the precision of the retrieved context relative to the question\n\nContext Precision measures how efficiently the retrieved information is used to answer the question. High precision means most of the retrieved context was necessary and used in generating the answer, with minimal irrelevant information.\n\nEvaluation instructions:\n1. Identify which parts of the context were actually used to generate the answer.\n2. Calculate the proportion of the context that contains information relevant to and used in the answer.\n3. Consider both:\n   - Information density (how much of the context is relevant)\n   - Information utilization (how much of the relevant information was used)\n\n4. Rate the context precision on a scale from 0 to 1:\n   - 1.0: Perfect precision (all retrieved context is relevant and used efficiently)\n   - 0.5: Moderate precision (about half the context is relevant and used)\n   - 0.0: No precision (context is entirely irrelevant)\n\n```\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\n```\nContext: ` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `\n\nboth variants regardless. this change will improve performance for many applications. thanks [ @ pngwn ] ( https : / / github. com / pngwn )! - [ # 5216 ] ( https : / / github. com / gradio - app / gradio / pull / 5216 ) [ ` 4b58ea6d ` ] ( https : / / github. com / gradio - app / gradio / commit / 4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8 ) - update i18n tokens and locale files. thanks [ @ hannahblair ] ( https : / / github. com / hannahblair )! # # 0. 2. 0 # # # features - [ # 5025 ] ( https : / / github. com / gradio -\n\nthe first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\n. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nAnswer: \n```\nThe `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on a Linux system using the musl C library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the context precision, let's analyze the context and its relevance to the question:\n\n1. **Identifying Relevant Parts Used in the Answer:**\n   - The context mentions: \"this is the *x86_64 - unknown - linux - musl* binary for `tokenizers`.\"\n   - The answer clearly states that the binary is designed for the \"x86_64 architecture on a Linux system using the musl C library.\"\n\n2. **Calculating Proportion of Relevant Information:**\n   - There are two main points in the relevant context: \n     - It identifies the architecture as \"x86_64\".\n     - It specifies the operating system as \"Linux\" and mentions the musl C library.\n   - The retrieved context primarily contains one clear and direct sentence relevant to the answer regarding the architecture and context for the `tokenizers-linux-x64-musl` binary.\n\n3. **Information Density and Utilization:**\n   - **Information Density:** The portion that contains relevant information is primarily the specific mention of \"x86_64 - linux - musl\", which accounts for a concise piece of relevant data.\n   - **Information Utilization:** The necessary informationâ€”architecture and systemâ€”was effectively utilized to answer the question accurately.\n\nTaking this into account:\n\n- All of the relevant context relates directly to the question at hand. Even though the entire context is longer and includes irrelevant information, the precise information used is highly relevant and necessary.\n- Since the utilized context only contains a single key phrase while the rest is irrelevant, the density is low, but utilization is fully high relative to the relevant information present.\n\nGiven the perfect relevance of the utilized information and its use in answering the question, but recognizing that the context includes additional inefficient text, I would rate the overall precision moderately high but not perfect.\n\nThus, I would assign a score of 0.8.\n\nFinal answer:\n```\nSCORE: 0.8\n```",
      "To evaluate the faithfulness of the generated answer, I will identify the factual claims made and assess their support based on the provided context.\n\n1. Claim: \"The `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture\"\n   - This claim is **fully supported** by the context, which explicitly states that the binary is for `x86_64-unknown-linux-musl`. Hence, this implies it is designed for that architecture.\n   - Score: 1 point\n\n2. Claim: \"on Linux using the musl C library\"\n   - This claim is also **fully supported** by the context. The term \"musl\" in the binary name indicates that it is built with the musl C library, and the context clearly denotes that it is for Linux.\n   - Score: 1 point\n\nAfter evaluating both claims, I find that both are fully supported by the provided context, leading to a total score of 2 points (1 point for each claim).\n\nNow I will calculate the faithfulness score:\n- Total claims = 2\n- Sum of points = 2\n\nFaithfulness Score = (Sum of points) / (Total number of claims) = 2 / 2 = 1.0\n\nTherefore, the generated answer is completely faithful to the context.\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the correctness of a generated answer based on a reference answer.\n\nAnswer Correctness measures how closely the generated answer matches the reference answer. An answer is considered correct if it contains all the necessary information from the reference answer and is free of errors.\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the reference answer (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the reference answer (assign 0 points)\n\n\nReturn a final answer correctness score between 0 and 1, where:\n- 1.0: Perfect correctness (all claims fully supported)\n- 0.0: Complete incorrectness (no claims supported by reference)\n\n```\nReference Answer: x86_64-unknown-linux-musl\n```\n\nAnswer: \n```\nThe `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on Linux using the musl C library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "Let's evaluate the generated answer in relation to the reference answer.\n\n1. The generated answer states that the `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on Linux using the musl C library.\n2. The reference answer is simply \"x86_64-unknown-linux-musl\".\n\nNow, we can identify the claims made in the generated answer:\n\n- **Claim 1**: The binary is designed for the x86_64 architecture.\n  - This claim is supported by the reference answer, which explicitly states \"x86_64\".\n  - **Score**: 1 point.\n\n- **Claim 2**: The binary is designed for Linux.\n  - The reference answer suggests that this is indeed the case since it indicates a Linux environment (as inferred from the \"linux\" portion).\n  - **Score**: 1 point.\n\n- **Claim 3**: The binary uses the musl C library.\n  - The reference answer includes \"musl\", which directly supports this claim.\n  - **Score**: 1 point.\n\nCombining these results, all claims made in the generated answer are fully supported by the reference answer. Therefore, the answer correctness score is:\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the correctness of a generated answer based on a reference answer.\n\nAnswer Correctness measures how closely the generated answer matches the reference answer. An answer is considered correct if it contains all the necessary information from the reference answer and is free of errors.\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the reference answer (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the reference answer (assign 0 points)\n\n\nReturn a final answer correctness score between 0 and 1, where:\n- 1.0: Perfect correctness (all claims fully supported)\n- 0.0: Complete incorrectness (no claims supported by reference)\n\n```\nReference Answer: distilbert base uncased finetuned sst2 english\n```\n\nAnswer: \n```\nThe default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst2-english.\"\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the correctness of the generated answer compared to the reference answer, we need to analyze both texts for factual claims.\n\n1. **Claim from the generated answer**: \"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n\n   - This claim is factual and identifies a specific model checkpoint associated with the Transformers library for sentiment analysis. \n   - The reference answer, \"distilbert base uncased finetuned sst2 english,\" mentions the same model but does so in a slightly different format (using dashes, capital letters, and different separation).\n   - Although the wording and formatting differ slightly between the two answers, the underlying information about the model (DistilBERT for sentiment analysis) is exactly the same.\n\nGiven these points, the single main claim in the generated answer is fully supported by the reference answer, even if there are minor formatting differences. Therefore, it is assigned a score of 1 point.\n\nThe generated answer has precisely matched the important factual information of the reference answer, which leads to the following assessment:\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant a generated answer is to the given question, regardless of factual accuracy\n\nAnswer Relevancy measures whether the generated answer directly addresses what was asked in the question. An answer is highly relevant if it provides information that specifically responds to the query's intent and information needs.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. Assess whether the answer:\n   - Directly addresses all aspects of the question (assign 1 point)\n   - Only partially addresses the question (assign 0.5 points)\n   - Fails to address the question or is off-topic (assign 0 points)\n\n\nReturn a final answer relevancy score between 0 and 1, where:\n- 1.0: Perfect relevancy (completely addresses the question with focus)\n- 0.0: Complete irrelevancy (does not address the question at all)\n\n```\nWhat is the purpose of the Safety Checker in the Diffusers library?\n\n```\n\nAnswer: \n```\nThe purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of a set of guidelines to ensure safe usage of the models. It aims to mitigate issues associated with models like Stable Diffusion. This contributes to the ethical development and use of the library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the relevancy of the generated answer to the question, I will follow these steps:\n\n1. **Identify the main information need in the question:** \n   The question is asking specifically for the purpose of the Safety Checker in the Diffusers library. The main information need is to understand what the Safety Checker does and why it is included in the library.\n\n2. **Assess whether the answer directly addresses the question:**\n   - The answer provided explains that the Safety Checker is designed to check and compare class probabilities according to a set of guidelines. This directly relates to the purpose of the Safety Checker.\n   - It mentions the goal of ensuring safe usage of models and mitigating issues related to models like Stable Diffusion, which are relevant to understanding the rationale behind the Safety Checker's purpose.\n   - Additionally, it highlights the ethical development and use of the library, which further provides context for why the Safety Checker is important.\n\nSince the answer comprehensively addresses the purpose of the Safety Checker and includes relevant details, it directly fulfills the information need posed by the question.\n\nGiven this analysis, I would assign a relevancy score of 1.0, as the answer perfectly addresses the question with a clear and focused response.\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the precision of the retrieved context relative to the question\n\nContext Precision measures how efficiently the retrieved information is used to answer the question. High precision means most of the retrieved context was necessary and used in generating the answer, with minimal irrelevant information.\n\nEvaluation instructions:\n1. Identify which parts of the context were actually used to generate the answer.\n2. Calculate the proportion of the context that contains information relevant to and used in the answer.\n3. Consider both:\n   - Information density (how much of the context is relevant)\n   - Information utilization (how much of the relevant information was used)\n\n4. Rate the context precision on a scale from 0 to 1:\n   - 1.0: Perfect precision (all retrieved context is relevant and used efficiently)\n   - 0.5: Moderate precision (about half the context is relevant and used)\n   - 0.0: No precision (context is entirely irrelevant)\n\n```\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\n```\nContext: the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two\n\nmethod of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment\n\nhat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the\n\nlibrary. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in\n\n### HF_HUB_DISABLE_TELEMETRY\r\n\r\nBy default, some data is collected by HF libraries (`transformers`, `datasets`, `gradio`,..) to monitor usage, debug issues and help prioritize features.\n\nAnswer: \n```\nThe default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst2-english.\"\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the context precision, let's first break down the context and the answer provided.\n\n### Identifying Relevant Parts of the Context\n\nThe relevant portion of the context is:\n- \"the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english.\"\n\nThe answer states:\n- \"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n\n### Analysis\n\n1. **Information Density**:\n   - The context does contain information explicitly stating the default checkpoint for the sentiment analysis pipeline, which is essential to answer the question.\n   - However, there is surrounding information in the context that does not pertain directly to the question (e.g., details about the tokenizer, methods from the class, and the note on telemetry). \n\n2. **Information Utilization**:\n   - The specific detail regarding the checkpoint is directly quoted in the answer. The answer used only the relevant part without needing any extraneous information provided in the context.\n   - Thus, the relevant information that was used to construct the answer is present and utilized effectively.\n\n### Calculation of Precision\n- The relevant context used for the answer was a very clear statement of the default checkpoint.\n- The entire context provided is somewhat distracting, with minor details about methods and telemetry that do not directly contribute to the answer.\n\nBased on this:\n- Out of the total retrieved context, only a small fraction (one distinct sentence) provides the required information to answer the question. The rest is irrelevant.\n- Therefore, the information density is low as the rest of the context is largely irrelevant. However, what was relevant was fully utilized.\n\n### Final Rating\nGiven the analysis:\n- Due to the low density of relevant context (only 1 sentence out of multiple sentences being relevant), I would rate the precision as moderate, but with the information used to answer effectively. Thus, I would land on the lower side of moderate precision.\n\nFinal score justification:\n- Information density: Low (due to irrelevant details) \n- Information utilization: High (the relevant information was used effectively).\n\nFinal score: **0.5**. \n\nThus, the output is:\n```\nSCORE: 0.5\n```",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the precision of the retrieved context relative to the question\n\nContext Precision measures how efficiently the retrieved information is used to answer the question. High precision means most of the retrieved context was necessary and used in generating the answer, with minimal irrelevant information.\n\nEvaluation instructions:\n1. Identify which parts of the context were actually used to generate the answer.\n2. Calculate the proportion of the context that contains information relevant to and used in the answer.\n3. Consider both:\n   - Information density (how much of the context is relevant)\n   - Information utilization (how much of the relevant information was used)\n\n4. Rate the context precision on a scale from 0 to 1:\n   - 1.0: Perfect precision (all retrieved context is relevant and used efficiently)\n   - 0.5: Moderate precision (about half the context is relevant and used)\n   - 0.0: No precision (context is entirely irrelevant)\n\n```\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\n```\nContext: diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https\n\nhttps : / / huggingface. co / spaces / society - ethics / diffusionbiasexplorer ) to demonstrate the biases in stable diffusion interactively. in this sense, we support and encourage bias explorers and evaluations. - * * encouraging safety in deployment * * - [ * * safe stable diffusion * * ] ( https : / / huggingface. co / docs / diffusers / main / en / api / pipelines / stable _ diffusion / stable _ diffusion _ safe ) : it mitigates the well - known issue that models, like stable diffusion, that are trained on unfiltered, web - crawled datasets tend to suffer from inappropriate degeneration. related paper : [ safe latent diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / gi\n\nsame level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and\n\ndiffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nAnswer: \n```\nThe purpose of the Safety Checker in the Diffusers library is to mitigate inappropriate degeneration in diffusion models by checking and comparing the class probability of harmful concepts against generated images. It uses hard-coded harmful concepts that are intentionally hidden to prevent reverse engineering. This mechanism helps ensure safer deployment of models like stable diffusion, which may be trained on unfiltered datasets.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the precision of the retrieved context regarding the Safety Checker in the Diffusers library, let's assess the context provided in relation to the written answer.\n\n### Step 1: Identify Relevant Parts of the Context\nThe main part of the context that relates to the question includes:\n- \"it checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated.\"\n- \"the harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\"\n- \"it mitigates the well-known issue that models, like stable diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration.\"\n\nThese sentences directly address the purpose of the Safety Checker and provide crucial information that was utilized in crafting the answer.\n\nOther parts of the context discuss different aspects of the Diffusers library, such as licensing, accessibility, and reproducibility. However, this information is not relevant to the specific question about the Safety Checker, which focuses solely on its purpose.\n\n### Step 2: Calculate Proportions\n1. **Total relevant context:** There are 3 major sentences that directly address the Safety Checker.\n2. **Total context size:** The overall provided context is lengthy and includes multiple topics, but the specific parts relevant to the question can be distilled into about 5 significant sentences.\n\n### Information Density and Utilization\n- **Information density:** The relevant portions (3 sentences) represent approximately 3 out of 5 sentences specifically dedicated to the Safety Checker context, which may be seen as high density.\n- **Information utilization:** The complete relevant sentences were directly used to construct the answer, meaning that the system efficiently utilized all relevant information.\n\n### Conclusion \nThe context provided is mostly relevant and almost entirely used to generate an accurate and detailed answer to the question. Given that only a few sentences were necessary to answer the question, and those sentences were both relevant and effectively utilized, I would assign a high precision score.\n\nBased on this assessment, I would rate the context precision as follows:\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\n```\n\nContext: \n```\nThe purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of a set of guidelines to ensure safe usage of the models. It aims to mitigate issues associated with models like Stable Diffusion. This contributes to the ethical development and use of the library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. **Identify the main information need(s) in the question:**\n   The question seeks to understand the specific purpose of the Safety Checker in the Diffusers library. The focus is on what the Safety Checker does and its functions related to ensuring safety and ethical usage of the models offered in that library.\n\n2. **Assessing the context:**\n   - The provided context states that the Safety Checker is designed to \"check and compare the class probability of a set of guidelines\" aimed at ensuring safe usage of the models. This directly addresses the question by explicitly stating the purpose of the Safety Checker.\n   - It goes further to explain that the Safety Checker aims to mitigate issues related to models like Stable Diffusion, which adds to the comprehensiveness of the relevant information provided.\n   - Additionally, it mentions the contribution of the Safety Checker to \"ethical development and use of the library,\" which is critical to understanding the broader implications of the Safety Checker beyond its technical functionality.\n\n3. **Evaluation:**\n   The context provides a succinct yet complete description of the Safety Checker's purpose in the Diffusers library. It covers the core function (checking class probability against guidelines), the rationale (to ensure safe usage and mitigate model-related issues), and the ethical considerations. There are no significant gaps or missing information that would impede understanding the purpose.\n\nOverall, the context is perfectly relevant as it contains all the necessary information needed to fully answer the question.\n\nSCORE: 1.0",
      "Let's break down the evaluation of the generated answer based on the provided context.\n\n1. **Identifying factual claims:**\n   - Claim 1: \"The `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture.\"\n   - Claim 2: \"The binary is designed for a Linux system.\"\n   - Claim 3: \"The binary is using the musl C library.\"\n\n2. **Evaluating each claim against the context:**\n   - Claim 1: \"The `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture.\"\n     - This claim is supported as the context mentions \"x86_64\" in relation to the `tokenizers-linux-x64-musl` binary, confirming that it is indeed for the x86_64 architecture. This is fully supported. (1 point)\n\n   - Claim 2: \"The binary is designed for a Linux system.\"\n     - The context includes \"linux\" in the designation of the binary, indicating that it is a binary for Linux systems. This is also fully supported. (1 point)\n\n   - Claim 3: \"The binary is using the musl C library.\"\n     - The context explicitly mentions \"musl\" alongside the binary name, indicating that it is designed to use the musl C library. This is fully supported. (1 point)\n\n3. **Calculating the faithfulness score:**\n   - Total claims: 3\n   - Points awarded: 3 (1 point for each claim)\n\nThe faithfulness score is calculated as: (3 points) / (3 total claims) = 1.0\n\nThis shows perfect faithfulness as all claims are fully supported by the context.\n\nFINAL SCORE: \nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the precision of the retrieved context relative to the question\n\nContext Precision measures how efficiently the retrieved information is used to answer the question. High precision means most of the retrieved context was necessary and used in generating the answer, with minimal irrelevant information.\n\nEvaluation instructions:\n1. Identify which parts of the context were actually used to generate the answer.\n2. Calculate the proportion of the context that contains information relevant to and used in the answer.\n3. Consider both:\n   - Information density (how much of the context is relevant)\n   - Information utilization (how much of the relevant information was used)\n\n4. Rate the context precision on a scale from 0 to 1:\n   - 1.0: Perfect precision (all retrieved context is relevant and used efficiently)\n   - 0.5: Moderate precision (about half the context is relevant and used)\n   - 0.0: No precision (context is entirely irrelevant)\n\n```\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\n```\nContext: -->\r\n\r\n# AutoPipeline\r\n\r\n`AutoPipeline` is designed to:\r\n\r\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nfor a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n\n- from_pipe\r\n\r\n## AutoPipelineForInpainting\r\n\r\n[[autodoc]] AutoPipelineForInpainting\r\n\t- all\r\n\t- from_pretrained\r\n\t- from_pipe\n\nAnswer: \n```\nThe method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This method automatically retrieves the relevant pipeline based on the name or path to the pretrained weights. Additionally, `from_pipe()` can be used to transfer components from the original pipeline to a new one without reallocating memory.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the precision of the retrieved context relative to the question, we need to analyze the context provided and how it correlates to the answer given.\n\n1. **Identify Parts of the Context Used in the Answer**:\n   - The answer states: \"The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`.\" This is clearly a key piece of information.\n   - The answer also mentions: \"Additionally, `from_pipe()` can be used to transfer components from the original pipeline to a new one without reallocating memory.\" This adds context to how checkpoints can be managed but is secondary to the primary inquiry about loading a checkpoint.\n\n2. **Calculate Proportion of Context That is Relevant**:\n   - The relevant parts of the context are:\n     - \"use the `from_pretrained()` method.\"\n     - Mentions of the `from_pipe()` method are relevant but not central to the answer's main focus.\n   - The stated context overview refers to `AutoPipeline` and its functionalities, covering both methods. Therefore, the relevant information focuses on those two methods.\n\n3. **Information Density and Utilization**:\n   - The context provides a clear definition and uses the terms relevant to the question directly.\n   - In total, there are 6 sentences in the context. Out of these, the sentences specifically informing about `from_pretrained()` and briefly mentioning `from_pipe()` provide most of the pertinent information needed to answer the question.\n   - The effective relevant sentences might be counted as approximately 2 sentences out of 6, suggesting a density of 2/6 or 1/3, which, given that more contextual sentences are used to confirm understanding, raises the score slightly.\n\n4. **Rating**:\n   - Given that all relevant information directly related to loading a checkpoint is present, and that most of it is effectively used in crafting the answer, we arrive at a good but not perfect density of utilized context.\n\nThus, interpreting the density and utilization alongside coherence, I would assess the context precision as follows:\n\n- There is a high relevance and utilization of context, particularly regarding loading a checkpoint; thus, I rate the context precision at 0.85.\n\nFinal Answer:\nSCORE: 0.85",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the correctness of a generated answer based on a reference answer.\n\nAnswer Correctness measures how closely the generated answer matches the reference answer. An answer is considered correct if it contains all the necessary information from the reference answer and is free of errors.\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the reference answer (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the reference answer (assign 0 points)\n\n\nReturn a final answer correctness score between 0 and 1, where:\n- 1.0: Perfect correctness (all claims fully supported)\n- 0.0: Complete incorrectness (no claims supported by reference)\n\n```\nReference Answer: 10GB\n```\n\nAnswer: \n```\nThe maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "Let's analyze the claims in the generated answer in relation to the reference answer.\n\n1. **Claim - \"The maximum size of a model checkpoint before it is automatically sharded\"**:\n   - The generated answer discusses a maximum size for a model checkpoint, which is a specific aspect of model management in the Transformers library.\n   - It continues to specify \"10GB\" as this maximum size.\n\n2. **Claim - \"Transformers version 4.18.0\"**:\n   - The mention of \"Transformers version 4.18.0\" provides context regarding the specific version of the library being discussed. However, the reference answer does not provide information about the version; it simply states a size.\n\nNow evaluating the factual claims:\n- The generated answer states that the maximum size is \"10GB,\" which is exactly the same as in the reference answer. This claim is fully supported, so it scores 1 point.\n- However, the detail about \"Transformers version 4.18.0\" is not mentioned in the reference answer. As a result, this portion does not receive any support or is not required for correctness in relation to the reference, thus it does not contribute to scoring.\n\nIn summation:\n- The claim about the maximum size (10GB) is fully supported, scoring 1 point.\n- The version context is not referenced in the original answer, leading to no additional support for this detail.\n\nConsidering these points, the overall score results from the fully supported claim of 1 point and the irrelevant addition of the version detail, not affecting the maximum size claim.\n\nTherefore, the final answer correctness score is:\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the precision of the retrieved context relative to the question\n\nContext Precision measures how efficiently the retrieved information is used to answer the question. High precision means most of the retrieved context was necessary and used in generating the answer, with minimal irrelevant information.\n\nEvaluation instructions:\n1. Identify which parts of the context were actually used to generate the answer.\n2. Calculate the proportion of the context that contains information relevant to and used in the answer.\n3. Consider both:\n   - Information density (how much of the context is relevant)\n   - Information utilization (how much of the relevant information was used)\n\n4. Rate the context precision on a scale from 0 to 1:\n   - 1.0: Perfect precision (all retrieved context is relevant and used efficiently)\n   - 0.5: Moderate precision (about half the context is relevant and used)\n   - 0.0: No precision (context is entirely irrelevant)\n\n```\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\n```\nContext: slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~\n\ntraining, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\n... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )\n\nsize of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] { ' total _ size ' : 433245184 } ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] { ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre\n\nAnswer: \n```\nThe maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10 GB. Checkpoints larger than this size will be split into smaller pieces. You can control the maximum size before sharding using the `max_shard_size` parameter.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the precision of the retrieved context relative to the question, I will analyze the context to see how much of it was relevant and how much was utilized to answer the question.\n\n1. **Identification of Used Context**:\n   - The relevant information directly addressing the question is: \"model checkpoints that end up taking more than 10gb of space are automatically sharded.\"\n   - The answer specifically states that the maximum size is 10 GB and that checkpoints larger than this will be sharded. This specific piece of information was clearly utilized in generating the answer.\n\n2. **Calculation of Relevant Context**:\n   - Looking at the context provided, it contains several segments:\n     - There is repetition in the content discussing sharded checkpoints and mentioning the 10 GB threshold.\n     - Some content discusses the `max_shard_size` parameter, which is also relevant but secondary.\n     - Other parts of the context delve into technical details about loading models and initialization that do not directly answer the question.\n   - Only the mention of 10 GB is crucial for answering the question about the maximum size before sharding.\n\n3. **Information Density**:\n   - Among the entire passage, the repeated lines about the 10 GB sharding capacity are essential and relevant, while much of the other context regarding model loading and metadata is not directly applicable.\n   - The relevant information (10 GB and the explanation of sharding) occupies a small proportion of the entire context.\n\n4. **Information Utilization**:\n   - The relevant information used from the context (the 10 GB detail) is efficiently used as it directly answers the question. Although the context may include additional relevant concepts, only that specific information about the 10 GB limit was necessary to form the complete answer.\n\n**Assessment**:\nGiven the retrieved context, the proportion of the text that was relevant is low, but the utilized information (10 GB) was effectively used and was precisely what was needed. The presence of largely irrelevant information decreases the overall context precision, but the relevant portion was utilized well.\n\n**Final Score**: The score reflects that while relevant information was efficiently used, there was substantial irrelevant context present. Therefore, I will rate the precision as moderate.\n\nFinal Answer: \nSCORE: 0.5",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the correctness of a generated answer based on a reference answer.\n\nAnswer Correctness measures how closely the generated answer matches the reference answer. An answer is considered correct if it contains all the necessary information from the reference answer and is free of errors.\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the reference answer (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the reference answer (assign 0 points)\n\n\nReturn a final answer correctness score between 0 and 1, where:\n- 1.0: Perfect correctness (all claims fully supported)\n- 0.0: Complete incorrectness (no claims supported by reference)\n\n```\nReference Answer: distilbert base uncased finetuned sst2 english\n```\n\nAnswer: \n```\nThe default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst2-english.\" This checkpoint is associated with a tokenizer that processes the input sentences for sentiment classification.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the correctness of the generated answer in comparison to the reference answer, let's break down the claims present in the generated answer.\n\n1. **Claim about the default checkpoint**: The generated answer states that \"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\" The reference answer states \"distilbert base uncased finetuned sst2 english.\" These are essentially the same, with the generated answer providing a more complete name (including the \"base\" and hyphenation) and its context (mentioning it is used by the sentiment analysis pipeline). Therefore, this claim is fully supported. **Score: 1 point.**\n\n2. **Claim about the tokenizer**: The answer includes that \"This checkpoint is associated with a tokenizer that processes the input sentences for sentiment classification.\" While it is common knowledge that a model checkpoint is associated with a tokenizer, this specific detail is not mentioned in the reference answer. Hence, while the information may be accurate, it is an extrapolation and not something directly supported by the reference. **Score: 0.5 points.**\n\nNow, calculating the overall score:\n- Fully supported claims: 1 point\n- Partially supported claims: 0.5 points \n\nTotal points out of a maximum of 2 points = (1 + 0.5) / 2 = 0.75\n\nThus, the final answer correctness score is:\n\nSCORE: 0.75",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the precision of the retrieved context relative to the question\n\nContext Precision measures how efficiently the retrieved information is used to answer the question. High precision means most of the retrieved context was necessary and used in generating the answer, with minimal irrelevant information.\n\nEvaluation instructions:\n1. Identify which parts of the context were actually used to generate the answer.\n2. Calculate the proportion of the context that contains information relevant to and used in the answer.\n3. Consider both:\n   - Information density (how much of the context is relevant)\n   - Information utilization (how much of the relevant information was used)\n\n4. Rate the context precision on a scale from 0 to 1:\n   - 1.0: Perfect precision (all retrieved context is relevant and used efficiently)\n   - 0.5: Moderate precision (about half the context is relevant and used)\n   - 0.0: No precision (context is entirely irrelevant)\n\n```\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\n```\nContext: - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of\n\nguidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\nðŸ§¨ Diffusers Experimental\r\n\r\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\r\nCurrently, the following experiments are supported:\n\nhat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the\n\n- **Encouraging safety in deployment**\n\nAnswer: \n```\nThe purpose of the Safety Checker in the Diffusers library is to check and compare class probabilities against a set of guidelines to ensure safety in deployment. It aims to encourage safe usage and contributions within the library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the precision of the retrieved context in relation to the question about the Safety Checker in the Diffusers library, letâ€™s break down our analysis:\n\n1. **Identifying Used Information**: \n   - The answer states that \"The purpose of the Safety Checker in the Diffusers library is to check and compare class probabilities against a set of guidelines to ensure safety in deployment.\" \n   - It also mentions that it \"aims to encourage safe usage and contributions within the library.\"\n\n2. **Relevant Parts of the Context**: \n   - The context snippet reveals that the Safety Checker \"checks and compares the class probability of a set of guidelines\" and mentions \"safety in deployment.\"\n   - The context snippet emphasizes \"guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library,\" which ties into the purpose stated in the answer.\n\n3. **Information Density**:\n   - The relevant information is concentrated within the first part of the context about the Safety Checker, specifically its role in checking class probabilities and ensuring safety, which directly answers the question.\n   - The portion about \"Diffusers Experimental\" does not pertain to the Safety Checker and is irrelevant to the question.\n\n4. **Information Utilization**:\n   - The answer efficiently uses the relevant parts of the context. It directly reflects the function of the Safety Checker and its intent, including both safety checks and encouraging safe usage, as highlighted in the context.\n\n5. **Calculation of Proportion**:\n   - The relevant context (about Safety Checker) is a small segment of the longer context, which also includes irrelevant information about other experiments.\n   - The relevant information takes up about half of the context snippet provided in relation to the total length of the text.\n\nConsidering the evaluation:\n- **Information density** is high for the relevant segment but low for the entire context, as part of the context discussed experiments unrelated to the question.\n- **Information utilization** is high because the answer accurately captures the core purpose described in the relevant context.\n\nBased on these observations, I would rate the overall context precision as follows:\n- The relevant information is significant but not all of it (due to the presence of irrelevant experimental data), leading to moderately efficient utilization.\n\nThus, to summarize:\n- I would rate the context precision at **0.5**, as about half of the context is relevant and used effectively to generate the answer, while part of the context material is irrelevant.\n\nFinal Output: \nSCORE: 0.5",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant a generated answer is to the given question, regardless of factual accuracy\n\nAnswer Relevancy measures whether the generated answer directly addresses what was asked in the question. An answer is highly relevant if it provides information that specifically responds to the query's intent and information needs.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. Assess whether the answer:\n   - Directly addresses all aspects of the question (assign 1 point)\n   - Only partially addresses the question (assign 0.5 points)\n   - Fails to address the question or is off-topic (assign 0 points)\n\n\nReturn a final answer relevancy score between 0 and 1, where:\n- 1.0: Perfect relevancy (completely addresses the question with focus)\n- 0.0: Complete irrelevancy (does not address the question at all)\n\n```\nWhat is the purpose of the Safety Checker in the Diffusers library?\n\n```\n\nAnswer: \n```\nThe purpose of the Safety Checker in the Diffusers library is to mitigate inappropriate degeneration in diffusion models by checking and comparing the class probability of harmful concepts against generated images. It uses hard-coded harmful concepts that are intentionally hidden to prevent reverse engineering. This mechanism helps ensure safer deployment of models like stable diffusion, which may be trained on unfiltered datasets.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the relevancy of the generated answer to the question about the purpose of the Safety Checker in the Diffusers library, we first identify the main information need. The question specifically asks for the purpose of the Safety Checker, which implies a request for its function and role within the Diffusers library.\n\nNow, let's assess the provided answer:\n\n1. The answer explicitly states that the purpose of the Safety Checker is \"to mitigate inappropriate degeneration in diffusion models,\" which directly addresses the question.\n2. It further elaborates that the check is done by \"checking and comparing the class probability of harmful concepts against generated images,\" explaining how it functions.\n3. The additional details about hard-coded harmful concepts and the goal of safer deployment of models like stable diffusion add clarity and depth to the answer but remain relevant to explaining the purpose.\n\nSince the answer effectively and comprehensively communicates the purpose of the Safety Checker, covering its function and significance in the context mentioned in the question, it can be considered a complete and focused response.\n\nGiven that the answer addresses all aspects of the question directly and provides pertinent information elaborating on the purpose of the Safety Checker, I assign it a relevancy score of 1.0.\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the precision of the retrieved context relative to the question\n\nContext Precision measures how efficiently the retrieved information is used to answer the question. High precision means most of the retrieved context was necessary and used in generating the answer, with minimal irrelevant information.\n\nEvaluation instructions:\n1. Identify which parts of the context were actually used to generate the answer.\n2. Calculate the proportion of the context that contains information relevant to and used in the answer.\n3. Consider both:\n   - Information density (how much of the context is relevant)\n   - Information utilization (how much of the relevant information was used)\n\n4. Rate the context precision on a scale from 0 to 1:\n   - 1.0: Perfect precision (all retrieved context is relevant and used efficiently)\n   - 0.5: Moderate precision (about half the context is relevant and used)\n   - 0.0: No precision (context is entirely irrelevant)\n\n```\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\n```\nContext: `tokenizers-linux-x64-musl`\r\n\r\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n\nlibrary, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or\n\ntokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning\n\nits unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained,\n\nNow let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well\n\nAnswer: \n```\nThe `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture. It is specifically built for the unknown Linux environment using the musl C library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the context precision, let's analyze the provided context along with the question and the answer.\n\n### Question:\nWhat architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\n### Context:\n1. **Content Analysis**:\n   - The first part states that \"`tokenizers-linux-x64-musl`\" is identified as the **x86_64-unknown-linux-musl** binary.\n   - The second part elaborates on the tokenization process and mentions tools related to tokenization (such as the AutoTokenizer API and sections about tokenization steps). However, this information is not directly relevant to determining the architecture of the binary.\n\n2. **Direct Use of Retrieved Context**:\n   - The relevant part that answers the question is the statement regarding it being the **x86_64-unknown-linux-musl** binary. This directly indicates the architecture being x86_64.\n   - This specific information is used to construct the answer: \"The `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture.\"\n\n### Calculation of Precision:\n- **Information Density**: \n  - The context provided is not highly dense with relevant information. Only 1 sentence, which indicates the architecture (identified directly), contributes to answering the question. This sentence is roughly 79 characters long.\n- **Information Utilization**: \n  - The answer utilizes the relevant information efficiently without needing additional context about tokenization processes (which occupy a substantial part of the context but donâ€™t help in answering the question).\n\n### Context Breakdown:\n- Relevant Information: â€œThis is the x86_64-unknown-linux-musl binary for tokenizersâ€\n- Total Context Length: Roughly 420 characters including irrelevant details.\n- Relevant Context Length: Only about 79 characters (1 sentence regarding architecture).\n\nThus, precision can be evaluated as follows:\n- The proportion of relevant information is low: ~79 / 420 â‰ˆ 0.188.\n- While utilization of relevant information is high since the answer is directly derived from the relevant sentence, we also consider how much of the context was needed. Given the large amount of irrelevant information, the overall precision takes a hit.\n\n### Conclusion:\nConsidering both information density and utilization, I would rate the context precision at around 0.5. While the relevant information effectively addresses the question, much of the context is unnecessary for forming the answer.\n\nFinal assessment for the precision score is:\nSCORE: 0.5",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\n```\n\nContext: \n```\nThe method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This method automatically retrieves the relevant pipeline based on the name or path to the pretrained weights. Additionally, `from_pipe()` can be used to transfer components from the original pipeline to a new one without reallocating memory.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. **Identify the main information need(s) in the question**: The question specifically asks for the method used to load a checkpoint for a task using `AutoPipeline`. This implies that the answer should provide direct information about the method.\n\n2. **Assessing the context**:\n   - The first sentence explicitly states that the method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This directly answers the main information need presented in the question.\n   - The context also provides additional information about what `from_pretrained()` does, which is to automatically retrieve the relevant pipeline based on the name or path to the pretrained weights. This information enhances the comprehensiveness of the answer by explaining how the method functions.\n   - The mention of a second method, `from_pipe()`, adds extra context about another operation related to pipelines, but it is not directly related to the primary question about loading a checkpoint and could be seen as tangential. However, it does not detract from the relevance of the answer.\n\n3. **Overall context relevancy assessment**: The context provides complete and relevant information to answer the question about loading a checkpoint with `AutoPipeline`. It directly identifies the correct method, explains its utility, and includes some related but non-essential details. Given that it fully addresses the information need with no critical information missing, I assess the overall context relevancy as perfect.\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the correctness of a generated answer based on a reference answer.\n\nAnswer Correctness measures how closely the generated answer matches the reference answer. An answer is considered correct if it contains all the necessary information from the reference answer and is free of errors.\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the reference answer (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the reference answer (assign 0 points)\n\n\nReturn a final answer correctness score between 0 and 1, where:\n- 1.0: Perfect correctness (all claims fully supported)\n- 0.0: Complete incorrectness (no claims supported by reference)\n\n```\nReference Answer: x86_64-unknown-linux-musl\n```\n\nAnswer: \n```\nThe `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture. It is specifically built for the unknown Linux environment using the musl C library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the correctness of the generated answer against the reference answer, we will examine the claims made in the text and compare them to the reference.\n\n1. **Claim 1**: \"The `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture.\"\n   - This claim matches part of the reference answer, which includes \"x86_64\" as part of \"x86_64-unknown-linux-musl\". Therefore, this claim is **fully supported** by the reference answer. \n   - **Score**: 1 point\n\n2. **Claim 2**: \"It is specifically built for the unknown Linux environment...\"\n   - The reference answer states \"unknown\" as part of the context \"x86_64-unknown-linux-musl\". Thus, this claim is also **fully supported** by the reference answer.\n   - **Score**: 1 point\n\n3. **Claim 3**: \"...using the musl C library.\"\n   - The reference answer includes \"musl\" as part of \"x86_64-unknown-linux-musl\". This claim is **fully supported** by the reference answer as well.\n   - **Score**: 1 point\n\nAll claims in the generated answer have been confirmed to be fully supported by the reference answer. Each component (architecture, environment, and library) directly corresponds to elements within the reference provided.\n\nAdding points together:\n1 (Claim 1) + 1 (Claim 2) + 1 (Claim 3) = 3/3 claims are fully supported.\n\nGiven that all claims are accurately aligned with the reference answer, the final score is 1.0.\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the precision of the retrieved context relative to the question\n\nContext Precision measures how efficiently the retrieved information is used to answer the question. High precision means most of the retrieved context was necessary and used in generating the answer, with minimal irrelevant information.\n\nEvaluation instructions:\n1. Identify which parts of the context were actually used to generate the answer.\n2. Calculate the proportion of the context that contains information relevant to and used in the answer.\n3. Consider both:\n   - Information density (how much of the context is relevant)\n   - Information utilization (how much of the relevant information was used)\n\n4. Rate the context precision on a scale from 0 to 1:\n   - 1.0: Perfect precision (all retrieved context is relevant and used efficiently)\n   - 0.5: Moderate precision (about half the context is relevant and used)\n   - 0.0: No precision (context is entirely irrelevant)\n\n```\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\n```\nContext: to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\nthe first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\n:... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir ) ` ` ` the main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in ram to the model size plus the size of the biggest shard. behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. we can load that index like any json and get a dictionary : ` ` ` py > > > import json > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ sha\n\nAnswer: \n```\nTo load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method, which automatically retrieves the relevant pipeline given the name or path to the pretrained weights. This method allows you to load a checkpoint without needing to know the specific pipeline class. Additionally, you can use the `from_pipe()` method to transfer components between pipelines without reallocating memory.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the context precision relative to the question and answer provided, I will follow the outlined steps.\n\n1. **Identify Relevant Parts of Context:**\n   - The answer specifically mentions using the `from_pretrained()` method of the `AutoPipeline` class to load checkpoints. The relevant parts of the context that support this point include:\n     - \"to load a checkpoint for a task without knowing the specific pipeline class to use... the `autopipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\"\n     - \"This method allows you to load a checkpoint without needing to know the specific pipeline class.\"\n   - The context also mentions the `from_pipe()` method in relation to transferring components without reallocating memory, which is included in the answer: \"Additionally, you can use the `from_pipe()` method to transfer components between pipelines without reallocating memory.\"\n   \n2. **Calculate Relevant Context Proportion:**\n   - The total context is lengthy and includes many extraneous details unrelated to the answer. However, the primary relevant sections have been identified and are specifically focused on the methods relevant to loading a checkpoint (from `autopipeline`).\n   - The total relevant information used is primarily confined to a few sentences directly related to the methods being discussed, suggesting a small portion of the overall text supports the answer.\n\n3. **Information Density and Utilization:**\n   - **Information Density:** The context overall is dense with information, but much of it is irrelevant to the task of loading a checkpoint. The parts related to `from_pretrained()` and `from_pipe()` are highly relevant, but they only represent a fraction of the context.\n   - **Information Utilization:** The answer efficiently extracts and utilizes these relevant parts, leading to a well-articulated response about loading checkpoints. \n\n4. **Score Evaluation:**\n   - Given that a small but critical portion of the context is relevant (the sentences about `from_pretrained` and `from_pipe`), but much of the surrounding context is irrelevant, the information density is not very high. However, the utilized information that pertains directly to the question is clear and focused.\n   - Therefore, I would assess the precision as moderate: it effectively uses relevant information, but the irrelevant context diminishes the overall precision score.\n\nThe evaluation leads me to assign a score that reflects moderate precision.\n\nSCORE: 0.5",
      "To evaluate the answer, I'll first identify the factual claims presented in the generated answer, and then assess how they align with the reference answer.\n\n1. **Claim from the answer**: The default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst2-english.\"\n\nNow, I will compare this claim to the reference answer:\n\n- The reference answer does provide the model name \"distilbert base uncased finetuned sst2 english,\" which directly matches the name of the DistilBERT model used for sentiment analysis (though it is formatted differently, using hyphens instead of spaces).\n- The generated answer states that this model is the default checkpoint \"used by the sentiment analysis pipeline in the Transformers library.\" \n\nThe key part here is that while the reference answer provides the model name, the generated answer claims it is specifically the \"default checkpoint\" which is not stated in the reference answer. Hence, this introduces an assumption not explicitly supported by the reference.\n\nThe conclusion is:\n- The model name is fully supported (1.0 point).\n- The claim about it being the \"default checkpoint\" is not stated in the reference answer, which means this part is not supported (0 points).\n\nThus, the total points would be:\n- Model Name (1 point) + Default Checkpoint (0 points) = 1 point.\n\nBecause not all claims are fully supported (due to the unverified claim about being the default checkpoint), the final correctness score should be adjusted.\n\nTo conclude, since one part is fully supported while the other is not supported at all, I will evaluate this as partially correct.\n\nFinal answer correctness score:\n\nSCORE: 0.5",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the precision of the retrieved context relative to the question\n\nContext Precision measures how efficiently the retrieved information is used to answer the question. High precision means most of the retrieved context was necessary and used in generating the answer, with minimal irrelevant information.\n\nEvaluation instructions:\n1. Identify which parts of the context were actually used to generate the answer.\n2. Calculate the proportion of the context that contains information relevant to and used in the answer.\n3. Consider both:\n   - Information density (how much of the context is relevant)\n   - Information utilization (how much of the relevant information was used)\n\n4. Rate the context precision on a scale from 0 to 1:\n   - 1.0: Perfect precision (all retrieved context is relevant and used efficiently)\n   - 0.5: Moderate precision (about half the context is relevant and used)\n   - 0.0: No precision (context is entirely irrelevant)\n\n```\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\n```\nContext: - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of\n\nguidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library.\n\n-->\r\n\r\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\r\n\r\n## Preamble\n\n- [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable\n\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent.\n\nAnswer: \n```\nThe purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of a set of guidelines to ensure safe usage of the models. It aims to mitigate issues associated with models like Stable Diffusion. This contributes to the ethical development and use of the library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the precision of the retrieved context in relation to the question regarding the purpose of the Safety Checker in the Diffusers library, we need to analyze the information provided in the context and how well it aligns with the answer generated.\n\n1. **Identification of Used Context**:\n   The answer states that the purpose of the Safety Checker is to \"check and compare the class probability of a set of guidelines\" and that it â€œaims to mitigate issues associated with models like Stable Diffusionâ€. These phrases correspond directly to the information found in the context:\n   - \"It checks and compares the class probability of a set of guidelines\" is directly mentioned in the context.\n   - The reference to mitigating issues with models like Stable Diffusion, though slightly paraphrased, aligns with the description of the Safety Checker's function concerning ethical usage.\n\n2. **Relevance and Information Density**:\n   - The crucial piece of information regarding checking class probabilities is central to the function of the Safety Checker and is included in the answer.\n   - The context also references the broader implications of ethical guidelines, but the phrase concerning ethical development and use adds some detail relating to the Safety Checker, which enhances the relevance of this part.\n\n3. **Information Utilization**:\n   - The core component of the context about the Safety Checker being involved in comparing class probabilities is fully utilized in the answer.\n   - The ethical guidelines mention provides a broader context that is relevant, supporting the main answer without diverging into irrelevant areas.\n\nOverall, the context is compact and aligned closely with the generated answer. While there is a slight expansion in wording in some areas of the answer, it effectively incorporates all relevant parts of the retrieved context.\n\nGiven this analysis:\n- The information density is high because most of the relevant information was utilized.\n- The information utilization is also high since the primary function of the Safety Checker is clearly articulated in both the context and the answer.\n\nConsidering these factors, I would rate the context precision as 1.0, as all retrieved context was relevant and used efficiently to generate the answer.\n\nFinal answer:\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\n```\n\nContext: \n```\nThe maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10 GB. Checkpoints larger than this size will be split into smaller pieces. You can control the maximum size before sharding using the `max_shard_size` parameter.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the context's relevancy concerning the question about the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0, I will follow the steps outlined:\n\n1. **Identify the main information needs in the question**:\n   - The question seeks the specific limit on the size of a model checkpoint (a numerical value) before sharding occurs in the specified version of Transformers.\n\n2. **Assess the provided context**:\n   - The context explicitly states that the maximum size of a model checkpoint before automatic sharding is 10 GB. \n   - It additionally mentions that checkpoints exceeding this size become split into smaller pieces, which is relevant but not necessarily critical to the main information need.\n   - Furthermore, it provides information on controlling this limit using the `max_shard_size` parameter, which is supplementary information that enhances understanding but is not essential for answering the primary question.\n\n3. **Summarize the evaluation**:\n   - The context contains a clear and direct answer (10 GB) to the question regarding the maximum size of the model checkpoint.\n   - It is comprehensive enough as it not only provides the maximum size limit but also explains the behavior of the model checkpoint when this limit is breached.\n   - There is no critical information missing that would prevent answering the question.\n\nGiven that the context satisfactorily meets the needs of the question, I would rate the overall context relevancy as follows:\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the precision of the retrieved context relative to the question\n\nContext Precision measures how efficiently the retrieved information is used to answer the question. High precision means most of the retrieved context was necessary and used in generating the answer, with minimal irrelevant information.\n\nEvaluation instructions:\n1. Identify which parts of the context were actually used to generate the answer.\n2. Calculate the proportion of the context that contains information relevant to and used in the answer.\n3. Consider both:\n   - Information density (how much of the context is relevant)\n   - Information utilization (how much of the relevant information was used)\n\n4. Rate the context precision on a scale from 0 to 1:\n   - 1.0: Perfect precision (all retrieved context is relevant and used efficiently)\n   - 0.5: Moderate precision (about half the context is relevant and used)\n   - 0.0: No precision (context is entirely irrelevant)\n\n```\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\n```\nContext: slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when you do ` model. save _ pretrained ( save _ dir ) `, you will end up with several partial checkpoints ( each of which being of size < 10gb ) and an index that maps parameter names to the files they are stored in. you can control the maximum size before sharding with the ` max _ shard _ size ` parameter, so for the sake of an example, we ' ll use a normal - size models with a small shard size : let ' s take a traditional bert model. ` ` ` py from transformers import automodel model = automodel. from _ pretrained ( \" bert - base - cased \" ) ` ` ` if you save it using [ ` ~\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\ntraining, each process will load the pretrained model and store these two copies in ram. < tip > note that the randomly created model is initialized with \" empty \" tensors, which take the space in memory without filling it ( thus the random values are whatever was in this chunk of memory at a given time ). the random initialization following the appropriate distribution for the kind of model / parameters instantiated ( like a normal distribution for instance ) is only performed after step 3 on the non - initialized weights, to be as fast as possible! < / tip > in this guide, we explore the solutions transformers offer to deal with this issue. note that this is an area of active development, so the apis explained here may change slightly in the future. # # sharded checkpoints since version 4. 18. 0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces. in terms of having one single checkpoint when\n\nsize of the model for now. we plan to add other information in the future : ` ` ` py > > > index [ \" metadata \" ] { ' total _ size ' : 433245184 } ` ` ` the weights map is the main part of this index, which maps each parameter name ( as usually found in a pytorch model ` state _ dict ` ) to the file it ' s stored in : ` ` ` py > > > index [ \" weight _ map \" ] { ' embeddings. layernorm. bias ' : ' pytorch _ model - 00001 - of - 00003. bin ', ' embeddings. layernorm. weight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pre\n\n... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` # # low memory loading sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the accelerate library. please read the following guide for more information : [ large model loading using accelerate ] (. / main _ classes / model # large - model - loading )\n\nAnswer: \n```\nIn Transformers version 4.18.0, model checkpoints that exceed 10 GB in size are automatically sharded into smaller pieces. Each of these smaller pieces will be less than 10 GB. You can control the maximum size before sharding using the `max_shard_size` parameter.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the precision of the retrieved context relative to the question, we need to analyze the context provided and see how much of it is relevant to answering the question about the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0.\n\n1. **Relevant Information Used for the Answer**:\nThe context mentions the following key points that were directly relevant to the question:\n   - Model checkpoints larger than **10 GB** are automatically sharded into smaller pieces.\n   - Each smaller piece will be less than **10 GB**.\n   - The **`max_shard_size` parameter** can control the maximum size before sharding.\n\n2. **Proportion of Relevant Information**:\nThe context contains multiple sentences, some of which are relevant and others that are not. Specifically:\n   - The phrase about checkpoints exceeding **10 GB** and the explanation about sharding is crucial.\n   - Additional details, such as the example using 'bert' or code snippets, although they provide context, do not directly answer the question nor add core content about the maximum checkpoint size.\n\nLet's summarize:\n- Relevant and utilized content: \"model checkpoints that end up taking more than 10 GB of space are automatically sharded... each of which being of size < 10 GB.\" and \"control the maximum size before sharding with the `max_shard_size` parameter.\"\n- Irrelevant content includes long explanations and code snippets that do not pertain to the size threshold of sharding.\n\n3. **Evaluation**:\n- The relevant parts of the context account for a significant subset of the text but are mixed in with several sentences that are not contributing to the answer.\n- While the exact proportion of relevant text may not be straightforward, we can qualitatively assess that approximately 30-40% of the context is relevant and contributes to the answer.\n\nBased on this assessment:\n- Information Density: Moderate (some relevant text, but diluted by extraneous details).\n- Information Utilization: High (the answer derived correctly encapsulates the relevant information from the context).\n\nGiven that relevant and utilized information is notable but there is also a fair amount of noise in the text, I would rate this context for precision as moderately high.\n\nSCORE: 0.7",
      "1. The main information need in the question is to identify the architecture for which the `tokenizers-linux-x64-musl` binary is intended.\n\n2. Assessment of the context:\n   - The context directly states that the `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on Linux with the musl C library. This provides a clear and direct answer to the question about the architecture.\n   - The relevant information is comprehensive as it not only mentions the architecture (x86_64) but also provides additional context about the operating system (Linux) and the C library in use (musl).\n   - There is no critical information missing; the response fully addresses the question regarding the architecture associated with that specific binary.\n\n3. Overall, the context provides complete and accurate information necessary to answer the question without any significant gaps.\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant a generated answer is to the given question, regardless of factual accuracy\n\nAnswer Relevancy measures whether the generated answer directly addresses what was asked in the question. An answer is highly relevant if it provides information that specifically responds to the query's intent and information needs.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. Assess whether the answer:\n   - Directly addresses all aspects of the question (assign 1 point)\n   - Only partially addresses the question (assign 0.5 points)\n   - Fails to address the question or is off-topic (assign 0 points)\n\n\nReturn a final answer relevancy score between 0 and 1, where:\n- 1.0: Perfect relevancy (completely addresses the question with focus)\n- 0.0: Complete irrelevancy (does not address the question at all)\n\n```\nWhat is the purpose of the Safety Checker in the Diffusers library?\n\n```\n\nAnswer: \n```\nThe purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of hard-coded harmful concepts against generated images to mitigate inappropriate degeneration in diffusion models. It aims to prevent the generation of harmful content by ensuring that such concepts are not present in the output. The harmful concepts are intentionally hidden to protect the integrity of the checker.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the relevance of the answer to the given question, I will first identify the main information need in the question:\n\n1. The question, \"What is the purpose of the Safety Checker in the Diffusers library?\" is seeking a clear explanation of the function or goal of the Safety Checker, specifically in the context of the Diffusers library. \n\nNow, I will assess whether the provided answer addresses this information need. \n\n- The answer begins by stating what the Safety Checker is supposed to do: it checks and compares the class probabilities of harmful concepts against generated images, which directly responds to the question of purpose.\n- It elaborates on the objective to mitigate inappropriate degeneration and prevent the generation of harmful content, which aligns well with the intent to explain the purpose.\n- The mention of \"harmful concepts being intentionally hidden to protect the integrity of the checker\" adds depth to the understanding of how the Safety Checker operates, but does not detract from the primary purpose being addressed.\n\nOverall, the answer provides a direct and focused response to the question about the Safety Checkerâ€™s purpose, covering its function and goals thoroughly.\n\nSince the answer directly addresses all aspects of the question with clarity and relevance, I assign it a score of 1 point.\n\nFinal score:\nSCORE: 1.0",
      "Let's evaluate the generated answer based on the provided context.\n\n1. **Identifying Factual Claims:**\n   - Claim 1: \"To load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method, which automatically retrieves the relevant pipeline given the name or path to the pretrained weights.\"\n   - Claim 2: \"This method allows you to load a checkpoint without needing to know the specific pipeline class.\"\n   - Claim 3: \"Additionally, you can use the `from_pipe()` method to transfer components between pipelines without reallocating memory.\"\n\n2. **Evaluating Each Claim:**\n   - **Claim 1 Evaluation:** This is fully supported by the context, as it mentions exactly that the `from_pretrained()` method retrieves the relevant pipeline given the name or path to pretrained weights and is used to load a checkpoint for a task. (Score: 1)\n   - **Claim 2 Evaluation:** This claim is also fully supported by the context. The context explicitly states that you can load a checkpoint for a task without knowing the specific pipeline class when using `AutoPipeline`. (Score: 1)\n   - **Claim 3 Evaluation:** This claim is also fully supported. The context mentions the `from_pipe()` method allows for transferring components without reallocating memory. (Score: 1)\n\n3. **Calculating the Faithfulness Score:**\n   - Total Number of Claims: 3\n   - Total Points: 1 + 1 + 1 = 3\n   - Faithfulness Score Calculation: \\( \\text{Score} = \\frac{\\text{Total Points}}{\\text{Total Number of Claims}} = \\frac{3}{3} = 1.0 \\)\n\nFinal score: \n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the precision of the retrieved context relative to the question\n\nContext Precision measures how efficiently the retrieved information is used to answer the question. High precision means most of the retrieved context was necessary and used in generating the answer, with minimal irrelevant information.\n\nEvaluation instructions:\n1. Identify which parts of the context were actually used to generate the answer.\n2. Calculate the proportion of the context that contains information relevant to and used in the answer.\n3. Consider both:\n   - Information density (how much of the context is relevant)\n   - Information utilization (how much of the relevant information was used)\n\n4. Rate the context precision on a scale from 0 to 1:\n   - 1.0: Perfect precision (all retrieved context is relevant and used efficiently)\n   - 0.5: Moderate precision (about half the context is relevant and used)\n   - 0.0: No precision (context is entirely irrelevant)\n\n```\nQuestion: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n\n```\nContext: the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special tokens ( if the model expect them ). here the model uses expects a cls token at the beginning and a sep token at the end of the sentence to classify. lastly, the tokenizer matches each token to its unique id in the vocabulary of the pretrained model. to load such a tokenizer, the transformers library provides the autotokenizer api. the most important method of this class is from _ pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences.\n\nhat happens inside the pipeline function? in this video, we will look at what actually happens when we use the pipeline function of the transformers library. more specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. as we have seen in the pipeline presentation, there are three stages in the pipeline. first, we convert the raw texts to numbers the model can make sense of, using a tokenizer. then, those numbers go through the model, which outputs logits. finally, the post - processing steps transforms those logits into labels and scores. let ' s look in detail at those three steps, and how to replicate them using the transformers library, beginning with the first stage, tokenization. the tokenization process has several steps. first, the text is split into small chunks called tokens. they can be words, parts of words or punctuation symbols. then the tokenizer will had some special\n\nto a given checkpoint. here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. we instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. this is done by the tokenizer with the option padding = true. with truncation = true, we ensure that any sentence longer than the maximum the model can handle is truncated. lastly, the return _ tensors option tells the tokenizer to return a pytorch tensor. looking at the result, we see we have a dictionary with two keys. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all\n\n##try by default, some data is collected by hf libraries ( ` transformers `, ` datasets `, ` gradio `,.. ) to monitor usage, debug issues and help prioritize features. each library defines its own policy ( i. e. which usage to monitor ) but the core implementation happens in ` huggingface _ hub ` ( see [ ` send _ telemetry ` ] ). you can set ` hf _ hub _ disable _ telemetry = 1 ` as environment variable to globally disable telemetry. # # # hf _ hub _ enable _ hf _ transfer set to ` true ` for faster uploads and downloads from the hub using ` hf _ transfer `. by default, ` huggingface _ hub ` uses the python - based ` requests. get ` and ` requests. post ` functions. although these are reliable and versatile, they may not be the most efficient choice for machines with high\n\n. those outputs are not probabilities yet ( we can see they don ' t sum to 1 ). this is because each model of the transformers library returns logits. to make sense of those logits, we need to dig into the third and last step of the pipeline : post - processing. to convert logits into probabilities, we need to apply a softmax layer to them. as we can see, this transforms them into positive numbers that sum up to 1. the last step is to know which of those corresponds to the positive or the negative label. this is given by the id2label field of the model config. the first probabilities ( index 0 ) correspond to the negative label, and the seconds ( index 1 ) correspond to the positive label. this is how our classifier built with the pipeline function picked those labels and computed those scores. now that you know how each steps works, you can easily tweak them to your needs.\n\nAnswer: \n```\nThe default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst2-english.\" This checkpoint is associated with a tokenizer that processes the input sentences for sentiment classification.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the precision of the retrieved context relative to the question, let's go through the process step by step.\n\n1. **Identify Used Parts of Context**: \n   The answer specifies that the default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert-base-uncased-finetuned-sst2-english.\" The context explicitly mentions this information when it states: \"here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english.\" This specific sentence directly answers the question.\n\n2. **Calculate Information Density and Utilization**: \n   - The part of the context that contains relevant information used to generate the answer consists of the specific sentence regarding the checkpoint.\n   - The context provided is long and includes a lot of information about tokenization and other aspects of the Transformers library pipeline that do not relate to the question. \n\n   Out of the entire provided context, only a small section (the one identifying the default checkpoint) is directly relevant to answering the question. The remainder of the context provides additional background information that is not necessary for answering the question about the default checkpoint. \n\n   If we evaluate the length:\n   - Relevant portion: about one sentence.\n   - Total context: multiple paragraphs (let's assume 7-10 sentences).\n  \n   Considering this ratio, the relevant information is approximately 1 out of a larger number of sentences.\n\n3. **Information Density & Utilization**: \n   - **Information Density**: Only a small fraction of the text is essential for answering the question. Therefore, the density of relevant information is low.\n   - **Information Utilization**: The relevant sentence was explicitly used in generating the answer, so utilization is high for that specific information.\n\nBased on this analysis:\n- The context includes one crucial piece of relevant information but is surrounded by a lot of irrelevant detail. Hence, I would rate this context's precision as below moderate but not entirely useless.\n\n**Final Score**: Given the low density of relevant information relative to the total context but the complete usage of the relevant detail, I would assign a precision score of 0.3.\n\nFinal answer:\n```\nSCORE: 0.3\n```",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\n```\n\nContext: \n```\nTo load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method, which automatically retrieves the relevant pipeline given the name or path to the pretrained weights. This method allows you to load a checkpoint without needing to know the specific pipeline class. Additionally, you can use the `from_pipe()` method to transfer components between pipelines without reallocating memory.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "1. **Identify the main information need(s) in the question:**\n   The question is asking for the specific method used to load a checkpoint for a task using `AutoPipeline`. The focus is on identifying the method or methods relevant to this process, particularly related to the loading of checkpoints.\n\n2. **Assess the context:**\n   - The context states: \"To load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method.\" This directly addresses the main information need by providing a specific method (`from_pretrained()`) that is used to load a checkpoint.\n   - The information about the `from_pretrained()` method is comprehensive as it explains what it doesâ€”automatically retrieving the relevant pipeline using the name or path to pretrained weightsâ€”and emphasizes that knowledge of the specific pipeline class is not necessary.\n   - The context also mentions an additional method, `from_pipe()`, which relates to components and pipelines but does not directly answer the main query about loading a checkpoint. While this information is interesting, it is not necessary to fully answer the question.\n   - There is no critical information missing regarding the specific method for loading a checkpoint since the question is primarily focused on that aspect.\n\n3. **Overall context relevancy:**\n   The context is highly relevant because it provides the exact method needed to answer the question and explains its significance, thus comprehensively meeting the requirements of the question.\n\nFinal answer: \nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the correctness of a generated answer based on a reference answer.\n\nAnswer Correctness measures how closely the generated answer matches the reference answer. An answer is considered correct if it contains all the necessary information from the reference answer and is free of errors.\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the reference answer (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the reference answer (assign 0 points)\n\n\nReturn a final answer correctness score between 0 and 1, where:\n- 1.0: Perfect correctness (all claims fully supported)\n- 0.0: Complete incorrectness (no claims supported by reference)\n\n```\nReference Answer: from_pretrained()\n```\n\nAnswer: \n```\nThe method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This method downloads and caches the configuration and vocabulary associated with the specified checkpoint. It allows for easy loading of the relevant pipeline without needing to know the specific class to use.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "Let's evaluate the generated answer against the reference answer.\n\n1. **Claim Identification**:\n    - The method used to load a checkpoint for a task is `from_pretrained()`.\n    - This method downloads and caches the configuration and vocabulary associated with the specified checkpoint.\n    - It allows for easy loading of the relevant pipeline without needing to know the specific class to use.\n\n2. **Claim Assessment**:\n    - **Claim 1**: The method used to load a checkpoint for a task is `from_pretrained()`. \n        - This claim is **fully supported** by the reference answer, which directly states `from_pretrained()`. (1 point)\n\n    - **Claim 2**: This method downloads and caches the configuration and vocabulary associated with the specified checkpoint. \n        - This claim is **partially supported**, as the reference answer solely mentions the method without detailing its functionality regarding downloading and caching. The additional information provided about caching the configuration and vocabulary while likely true is an extrapolation and not explicitly mentioned in the reference answer. (0.5 points)\n\n    - **Claim 3**: It allows for easy loading of the relevant pipeline without needing to know the specific class to use.\n        - This claim is **partially supported**, as it adds information that is consistent with the general use of `from_pretrained()`, but it elaborates on its functionality which is not explicitly found in the reference answer. Thus, it goes beyond the provided information. (0.5 points)\n\nNow, let's sum up the points:\n- Claim 1: 1 point (fully supported)\n- Claim 2: 0.5 points (partially supported)\n- Claim 3: 0.5 points (partially supported)\n\nThe total score would then be: (1 + 0.5 + 0.5) / 3 claims = 1/3 = 0.67. \n\nTo express this in a numeric scale from 0 to 1 and following the evaluation standards provided:\n\n- Given that one claim is fully supported and the other two are only partially supported, the final score reflects this balance.\n\nThus, the final answer correctness score is:\n\nSCORE: 0.67",
      "To evaluate the faithfulness of the generated answer, I'll identify the factual claims made in it and then assess if they are supported by the provided context.\n\n**Factual Claims in the Generated Answer:**\n1. The Safety Checker in the Diffusers library checks and compares the class probability of hard-coded harmful concepts against generated images.\n2. The purpose of the Safety Checker is to mitigate inappropriate degeneration in diffusion models.\n3. The Safety Checker aims to prevent the generation of harmful content by ensuring that such concepts are not present in the output.\n4. The harmful concepts are intentionally hidden to protect the integrity of the checker.\n\n**Assessment of Claims:**\n1. **Claim 1**: \"The Safety Checker in the Diffusers library checks and compares the class probability of hard-coded harmful concepts against generated images.\"\n   - Fully supported (1 point) - This information is directly present in the provided context.\n\n2. **Claim 2**: \"The purpose of the Safety Checker is to mitigate inappropriate degeneration in diffusion models.\"\n   - Fully supported (1 point) - This is explicitly stated in the context.\n\n3. **Claim 3**: \"The Safety Checker aims to prevent the generation of harmful content by ensuring that such concepts are not present in the output.\"\n   - Partially supported (0.5 points) - While the intent to compare harmful concepts is mentioned in the context, the specific wording about \"ensuring that such concepts are not present in the output\" is an inference and not directly stated.\n\n4. **Claim 4**: \"The harmful concepts are intentionally hidden to protect the integrity of the checker.\"\n   - Fully supported (1 point) - This claim is also directly present in the context.\n\nNow, let's calculate the faithfulness score:\n- Points obtained = 1 (Claim 1) + 1 (Claim 2) + 0.5 (Claim 3) + 1 (Claim 4) = 3.5\n- Total number of claims = 4\n\nFaithfulness Score:\n\\[ \\text{Score} = \\frac{3.5}{4} = 0.875 \\]\n\nThus, the final faithfulness score is:\n\nSCORE: 0.875",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the correctness of a generated answer based on a reference answer.\n\nAnswer Correctness measures how closely the generated answer matches the reference answer. An answer is considered correct if it contains all the necessary information from the reference answer and is free of errors.\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the reference answer (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the reference answer (assign 0 points)\n\n\nReturn a final answer correctness score between 0 and 1, where:\n- 1.0: Perfect correctness (all claims fully supported)\n- 0.0: Complete incorrectness (no claims supported by reference)\n\n```\nReference Answer: x86_64-unknown-linux-musl\n```\n\nAnswer: \n```\nThe `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on Linux with the musl C library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the correctness of the generated answer relative to the reference answer, I'll break down the claims made in the generated answer and compare them with the reference answer a step at a time.\n\n### Factual Claims in the Generated Answer:\n1. **Claim:** The `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture.\n   - **Evaluation:** This claim is partially supported. The reference answer specifies \"x86_64\" as part of its content, which means it aligns with this claim. However, the specific naming convention used (\"tokenizers-linux-x64-musl\") isn't mentioned in the reference answer, so while the architecture part is correct, it does not perfectly match the reference answer.\n  \n2. **Claim:** It is for Linux.\n   - **Evaluation:** This claim is supported as the reference answer mentions \"linux\". Thus, this aspect of the claim is correct and aligns with the reference.\n\n3. **Claim:** It uses the musl C library.\n   - **Evaluation:** This aligns perfectly with the reference answer, which includes \"musl\" in the text. Therefore, this claim is fully supported.\n\n### Scoring:\nFrom the evaluation:\n- The first claim is partially supported (0.5 points).\n- The second claim is fully supported (1 point).\n- The third claim is fully supported (1 point).\n\nCalculating the total score:\n\n\\[\n\\text{Total Points} = 0.5 (\\text{Claim 1}) + 1 (\\text{Claim 2}) + 1 (\\text{Claim 3}) = 2.5\n\\]\n\nThe maximum possible points are 3 (1 point for each of the 3 claims). Thus the correctness score is:\n\n\\[\n\\text{Correctness Score} = \\frac{2.5}{3} \\approx 0.83\n\\]\n\nIn line with the scoring system, this can be rounded to:\n\nFinal correctness score:\n\nSCORE: 0.83",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the correctness of a generated answer based on a reference answer.\n\nAnswer Correctness measures how closely the generated answer matches the reference answer. An answer is considered correct if it contains all the necessary information from the reference answer and is free of errors.\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the reference answer (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the reference answer (assign 0 points)\n\n\nReturn a final answer correctness score between 0 and 1, where:\n- 1.0: Perfect correctness (all claims fully supported)\n- 0.0: Complete incorrectness (no claims supported by reference)\n\n```\nReference Answer: from_pretrained()\n```\n\nAnswer: \n```\nTo load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method, which automatically retrieves the relevant pipeline given the name or path to the pretrained weights. Additionally, to switch between tasks with the same checkpoint without reallocating memory, the `from_pipe()` method can be used to transfer components from the original pipeline to the new one.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the generated answer against the reference answer, let's identify the factual claims present in both.\n\n1. The generated answer states that to load a checkpoint for a task using `AutoPipeline`, the `from_pretrained()` method can be used. This claim directly corresponds to the reference answer, which only mentions `from_pretrained()`. \n   - **Claim:** The `from_pretrained()` method is used to load checkpoints.\n   - **Assessment:** This claim is fully supported by the reference answer, as it explicitly provides the method name. (Score: 1 point)\n\n2. The generated answer mentions that the `from_pretrained()` method retrieves the relevant pipeline given the name or path to the pretrained weights. This explains a function of the `from_pretrained()` method, which supports the claim made in the reference answer, but adds additional detail about its functionality.\n   - **Claim:** `from_pretrained()` retrieves the relevant pipeline based on a name or path.\n   - **Assessment:** This claim, while not part of the reference answer, is a logical extension of its functionality, hence it is partially supported. (Score: 0.5 points)\n\n3. The generated answer also talks about switching between tasks with the same checkpoint using the `from_pipe()` method. This method is mentioned as being helpful in transferring components from the original pipeline.\n   - **Claim:** The `from_pipe()` method is used to switch between tasks with the same checkpoint.\n   - **Assessment:** This statement introduces a concept that is not referenced or mentioned in the original reference answer. Therefore, it is not supported at all. (Score: 0 points)\n\nNow, we will add these scores:\n- First claim: 1 point\n- Second claim: 0.5 points\n- Third claim: 0 points\n\nThe total score is (1 + 0.5 + 0) / 3 = 0.5.\n\nGiven that the maximum possible score is 1, the final correctness score for the generated answer compared to the reference answer is:\n\nSCORE: 0.5",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the precision of the retrieved context relative to the question\n\nContext Precision measures how efficiently the retrieved information is used to answer the question. High precision means most of the retrieved context was necessary and used in generating the answer, with minimal irrelevant information.\n\nEvaluation instructions:\n1. Identify which parts of the context were actually used to generate the answer.\n2. Calculate the proportion of the context that contains information relevant to and used in the answer.\n3. Consider both:\n   - Information density (how much of the context is relevant)\n   - Information utilization (how much of the relevant information was used)\n\n4. Rate the context precision on a scale from 0 to 1:\n   - 1.0: Perfect precision (all retrieved context is relevant and used efficiently)\n   - 0.5: Moderate precision (about half the context is relevant and used)\n   - 0.0: No precision (context is entirely irrelevant)\n\n```\nQuestion: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n\n```\nContext: ` tokenizers - linux - x64 - musl ` this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `\n\nhf _ home ` is not set! this is the default way to configure where [ user - specific non - essential ( cached ) data should be written ] ( https : / / wiki. archlinux. org / title / xdg _ base _ directory ) on linux machines. if ` hf _ home ` is not set, the default home will be ` \" $ xdg _ cache _ home / huggingface \" ` instead of ` \" ~ /. cache / huggingface \" `.\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\n. input ids contains the ids of both sentences, with 0s where the padding is applied. the second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. this is all what is inside the tokenization step. now let ' s have a look at the second step, the model. as for the tokenizer, there is an automodel api, with a from _ pretrained method. it will download and cache the configuration of the model as well as the pretrained weights. however, the automodel api will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. it will output a high - dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size\n\n! - - note that this file is in markdown but contain specific syntax for our doc - builder ( similar to mdx ) that may not be rendered properly in your markdown viewer. - - > # environment variables ` huggingface _ hub ` can be configured using environment variables. if you are unfamiliar with environment variable, here are generic articles about them [ on macos and linux ] ( https : / / linuxize. com / post / how - to - set - and - list - environment - variables - in - linux / ) and on [ windows ] ( https : / / phoenixnap. com / kb / windows - set - environment - variable ). this page will guide you through all environment variables specific to ` huggingface _ hub ` and their meaning. # # generic # # # hf _ inference _ endpoint to configure the inference api base url. you might want to set this variable if your organization is pointing at an api gateway rather\n\nAnswer: \n```\nThe `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on Linux with the musl C library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the context precision concerning the provided question and answer, we can break down the task as follows:\n\n1. **Identify the Useful Context**: \n   The key part of the context is:\n   ``` \n   tokenizers - linux - x64 - musl this is the * * x86 _ 64 - unknown - linux - musl * * binary for ` tokenizers `\n   ```\n   This explicitly states that the binary is for the `x86_64` architecture on Linux with `musl`. None of the other parts of the context contribute to answering the question about the architecture.\n\n2. **Calculate Relevant Proportion**: \n   - The total length of the context is considerable due to the presence of additional information about configuration, licensing, and unrelated technical details. \n   - Only a small portion is actually relevant to the question, mainly the segment that explains the architecture directly: `x86_64 - unknown - linux - musl`.\n\n3. **Information Density and Utilization**:\n   - Information Density: The relevant part of the context thus represents a very small fraction of the total text. We could roughly estimate that the relevant information occupies about 15-20% of the total context length due to the long irrelevant sections that follow.\n   - Information Utilization: The answer takes vital information directly from the relevant section of the context (it mentions `x86_64`, `Linux`, and `musl`), indicating complete utilization of the data that could generate the answer.\n\n4. **Rating**:\n   Considering the identified relevant context forms a small portion of the whole text but was fully utilized to construct the answer correctly, I would rate the precision as moderate. A fraction (20% of the context might be considered relevant and useful for the answer) indicates moderate context precision.\n\nTherefore, the final score based on the analysis is:\n\nSCORE: 0.5",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the correctness of a generated answer based on a reference answer.\n\nAnswer Correctness measures how closely the generated answer matches the reference answer. An answer is considered correct if it contains all the necessary information from the reference answer and is free of errors.\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the reference answer (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the reference answer (assign 0 points)\n\n\nReturn a final answer correctness score between 0 and 1, where:\n- 1.0: Perfect correctness (all claims fully supported)\n- 0.0: Complete incorrectness (no claims supported by reference)\n\n```\nReference Answer: The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.\n```\n\nAnswer: \n```\nThe purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of a set of guidelines to ensure safe usage of the models. It aims to mitigate issues associated with models like Stable Diffusion. This contributes to the ethical development and use of the library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the correctness of the generated answer against the reference answer, let's break down the claims made in both texts.\n\n1. **Claim: \"The purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of a set of guidelines.\"**\n   - This phrase fails to align fully with the reference answer, which specifically mentions a \"set of hard-coded harmful concepts\" rather than \"a set of guidelines.\" The specific mention of \"harmful concepts\" is a crucial aspect of the Safety Checker's function, and using the term \"guidelines\" is less specific and potentially misleading.\n   - **Score: 0.5 (Partially supported with some extrapolation).**\n\n2. **Claim: \"It aims to mitigate issues associated with models like Stable Diffusion.\"**\n   - While the reference indicates the purpose is to \"mitigate the risk of generating harmful content,\" this claim about mitigating \"issues associated with models like Stable Diffusion\" is vague and does not directly correspond to the specific goal outlined in the reference answer about harmful content.\n   - **Score: 0 (Not supported or contradicted by reference).**\n\n3. **Claim: \"This contributes to the ethical development and use of the library.\"**\n   - This claim introduces a new concept (ethical development) that is not covered or supported by the reference answer. The original focus was specifically on content safety rather than the broader ethical implications, making this an unsupported addition.\n   - **Score: 0 (Not supported or contradicted by reference).**\n\nSummarizing the assessments leads to the following scores:\n- First claim: 0.5\n- Second claim: 0\n- Third claim: 0\n\nTotal points accumulated: \\(0.5 + 0 + 0 = 0.5\\).\n\nSince all claims were assessed, and given the total score of 0.5 out of a possible 3 points, the correctness score reflects how many claims were supported compared to the potential maximum.\n\nThus, the final answer correctness score is:\n```\nSCORE: 0.17\n```",
      "To assess the correctness of the generated answer based on the reference answer, let's identify the factual claims made in the generated response:\n\n1. The claim states that the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB.\n\nNext, we will evaluate this claim against the reference answer:\n\n- The reference answer simply states \"10GB.\"\n- The generated answer not only provides the size (10GB) but also contextualizes it by specifying that this is the maximum size for model checkpoints in Transformers version 4.18.0 before they are automatically sharded.\n\nNow we determine if the claim is:\n- Fully supported by the reference answer: The reference simply specifies the size (10GB), which is present in the generated answer. However, the additional details about \"model checkpoints\" and \"automatically sharded in Transformers version 4.18.0\" are not found in the reference answer. Therefore, while the size itself is correct, the context provided in the generated answer is not supported directly by the reference.\n\nGiven that the claim includes correct information (the size is indeed 10GB) but also adds unsupported contextual details, we assign points as follows:\n- Claim about the size (10GB) is fully supported: 1 point.\n- The additional context about model checkpoints and sharding adds complexity and is not verified against the reference.\n\nTherefore, we can assess the correctness of the answer as partially supported, leading to a score of 0.5.\n\nFinal score based on the reasoning above is:\n\nSCORE: 0.5",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the correctness of a generated answer based on a reference answer.\n\nAnswer Correctness measures how closely the generated answer matches the reference answer. An answer is considered correct if it contains all the necessary information from the reference answer and is free of errors.\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the reference answer (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the reference answer (assign 0 points)\n\n\nReturn a final answer correctness score between 0 and 1, where:\n- 1.0: Perfect correctness (all claims fully supported)\n- 0.0: Complete incorrectness (no claims supported by reference)\n\n```\nReference Answer: The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.\n```\n\nAnswer: \n```\nThe purpose of the Safety Checker in the Diffusers library is to check and compare class probabilities against a set of guidelines to ensure safety in deployment. It aims to encourage safe usage and contributions within the library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the correctness of the generated answer compared to the reference answer, I will analyze the claims in the generated answer and assess how well they align with the claims made in the reference answer.\n\n**Factual Claims in the Generated Answer:**\n1. The purpose of the Safety Checker is to check and compare class probabilities.\n2. The Safety Checker compares class probabilities against a set of guidelines.\n3. The Safety Checker ensures safety in deployment.\n4. The Safety Checker aims to encourage safe usage and contributions within the library.\n\nNow, I will assess each of these claims against the reference answer.\n\n1. **\"The purpose of the Safety Checker is to check and compare class probabilities.\"**\n   - This is partially supported. The reference describes a checking and comparing process but specifies it is against hard-coded harmful concepts, not general class probabilities.\n   - **Score: 0.5**\n\n2. **\"The Safety Checker compares class probabilities against a set of guidelines.\"**\n   - This claim is not directly supported. The reference indicates that it compares against harmful concepts specifically, rather than just guidelines.\n   - **Score: 0**\n\n3. **\"The Safety Checker ensures safety in deployment.\"**\n   - This claim is somewhat supported, as the concept of ensuring safety is present in the reference, though it doesn't specifically mention deployment.\n   - **Score: 0.5**\n\n4. **\"The Safety Checker aims to encourage safe usage and contributions within the library.\"**\n   - This is not mentioned in the reference answer. The reference focuses on the risk of generating harmful content rather than promoting safe usage or contributions.\n   - **Score: 0**\n\nNow, summing the scores:\n- 0.5 (for claim 1) + 0 (for claim 2) + 0.5 (for claim 3) + 0 (for claim 4) = 1.0\n\nTotal claims assessed: 4\nTotal score: 1.0 out of a possible 4.0.\n\nTo calculate the final answer correctness score between 0 and 1:\n- We divide the total points (1.0) by the number of claims (4): \n  - 1.0 / 4.0 = 0.25\n\nThus, the final score for the correctness of the generated answer compared to the reference answer is:\n\nSCORE: 0.25",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the correctness of a generated answer based on a reference answer.\n\nAnswer Correctness measures how closely the generated answer matches the reference answer. An answer is considered correct if it contains all the necessary information from the reference answer and is free of errors.\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the reference answer (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the reference answer (assign 0 points)\n\n\nReturn a final answer correctness score between 0 and 1, where:\n- 1.0: Perfect correctness (all claims fully supported)\n- 0.0: Complete incorrectness (no claims supported by reference)\n\n```\nReference Answer: The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.\n```\n\nAnswer: \n```\nThe purpose of the Safety Checker in the Diffusers library is to mitigate inappropriate degeneration in diffusion models by checking and comparing the class probability of harmful concepts against generated images. It uses hard-coded harmful concepts that are intentionally hidden to prevent reverse engineering. This mechanism helps ensure safer deployment of models like stable diffusion, which may be trained on unfiltered datasets.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the correctness of the generated answer against the reference answer, let's identify and analyze the factual claims in each.\n\n1. **Claim:** The Safety Checker mitigates inappropriate degeneration in diffusion models.\n   - **Reference Assessment:** The reference does not mention \"inappropriate degeneration,\" but it describes the Safety Checker as a means to \"mitigate the risk of generating harmful content.\" This claim is somewhat aligned since both refer to risk mitigation but does not directly match the wording or focus.\n   - **Point Assignment:** 0.5 points (Partially supported with some extrapolation).\n\n2. **Claim:** The Safety Checker checks and compares the class probability of harmful concepts against generated images.\n   - **Reference Assessment:** This is fully supported by the reference answer, which states that the Safety Checker checks and compares class probabilities against an image.\n   - **Point Assignment:** 1 point (Fully supported).\n\n3. **Claim:** It uses hard-coded harmful concepts that are intentionally hidden to prevent reverse engineering.\n   - **Reference Assessment:** The reference mentions hard-coded harmful concepts but does not mention them being hidden or the intention to prevent reverse engineering. This part of the claim introduces new information that is not present in the reference.\n   - **Point Assignment:** 0 points (Not supported or contradicted).\n\n4. **Claim:** This mechanism helps ensure safer deployment of models like stable diffusion, which may be trained on unfiltered datasets.\n   - **Reference Assessment:** The reference does not speak to \"safer deployment\" or \"models like stable diffusion.\" Therefore, while there is a general theme of safety, this specific mention introduces new information not found in the reference.\n   - **Point Assignment:** 0 points (Not supported or contradicted).\n\nNow that we have analyzed the claims, we can total the points:\n- Claim 1: 0.5 points\n- Claim 2: 1 point\n- Claim 3: 0 points\n- Claim 4: 0 points\n\nTotal Points: 0.5 + 1 + 0 + 0 = 1.5 points.\n\nGiven that the scoring is out of a possible 3 points, we calculate the final score by normalizing this to a 1.0 scale:\nFinal Score = Total Points / Number of Claims = 1.5 / 4 = 0.375.\n\nThus, after evaluating the claims, we arrive at the final correctness score.\n\nSCORE: 0.375",
      "Let's evaluate the claims in the provided answer based on the context.\n\n**Claims in the Answer:**\n1. \"In Transformers version 4.18.0, model checkpoints that exceed 10 GB in size are automatically sharded into smaller pieces.\"\n2. \"Each of these smaller pieces will be less than 10 GB.\"\n3. \"You can control the maximum size before sharding using the `max_shard_size` parameter.\"\n\n**Assessment of Claims:**\n1. **Claim 1:** This claim is directly supported by the context, which mentions \"sharded checkpoints since version 4.18.0, model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces.\" \n   - **Score:** 1 point (fully supported).\n\n2. **Claim 2:** This claim is also stated in the context where it indicates that \"model checkpoints that end up taking more than 10gb of space are automatically sharded in smaller pieces (each of which being of size < 10gb).\"\n   - **Score:** 1 point (fully supported).\n\n3. **Claim 3:** The context confirms this by stating, \"you can control the maximum size before sharding with the `max_shard_size` parameter.\"\n   - **Score:** 1 point (fully supported).\n\n**Total points scored:** 3 points  \n**Total number of claims:** 3\n\nTo calculate the faithfulness score:\n\\[ \\text{Faithfulness Score} = \\frac{\\text{Total Points}}{\\text{Total Number of Claims}} = \\frac{3}{3} = 1.0 \\]\n\nThus, the answer is completely faithful to the context provided, as all claims are fully supported.\n\nFinal output:\n```\nSCORE: 1.0\n```",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the correctness of a generated answer based on a reference answer.\n\nAnswer Correctness measures how closely the generated answer matches the reference answer. An answer is considered correct if it contains all the necessary information from the reference answer and is free of errors.\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the reference answer (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the reference answer (assign 0 points)\n\n\nReturn a final answer correctness score between 0 and 1, where:\n- 1.0: Perfect correctness (all claims fully supported)\n- 0.0: Complete incorrectness (no claims supported by reference)\n\n```\nReference Answer: from_pretrained()\n```\n\nAnswer: \n```\nTo load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method, which automatically retrieves the relevant pipeline given the name or path to the pretrained weights. This method allows you to load a checkpoint without needing to know the specific pipeline class. Additionally, you can use the `from_pipe()` method to transfer components between pipelines without reallocating memory.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the generated answer against the reference answer, we need to identify the factual claims made in the generated answer and assess their correctness relative to the reference answer.\n\n1. **Claim: \"To load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method.\"**\n   - This claim is fully supported by the reference answer, which explicitly states `from_pretrained()`.\n   - Score: 1 point.\n\n2. **Claim: \"which automatically retrieves the relevant pipeline given the name or path to the pretrained weights.\"**\n   - This claim provides additional information about the `from_pretrained()` method, explaining its functionality. While this is not mentioned in the reference answer, it does not contradict it. However, the reference answer does not directly support this claim.\n   - Score: 0.5 points (partially supported with extrapolation).\n\n3. **Claim: \"This method allows you to load a checkpoint without needing to know the specific pipeline class.\"**\n   - Similar to the previous claim, this provides additional context about the functionality of `from_pretrained()`. This is not supported by the reference answer and is an extrapolation.\n   - Score: 0.5 points (partially supported with extrapolation).\n\n4. **Claim: \"Additionally, you can use the `from_pipe()` method to transfer components between pipelines without reallocating memory.\"**\n   - This claim introduces a new method, `from_pipe()`, which is not mentioned in the reference answer at all. Therefore, it is not supported and is not related to the `from_pretrained()` functionality.\n   - Score: 0 points (not supported).\n\nNow, we can summarize the scores:\n- Claim 1: 1 point\n- Claim 2: 0.5 points\n- Claim 3: 0.5 points\n- Claim 4: 0 points\n\nTotal score = 1 + 0.5 + 0.5 + 0 = 2.\nThe maximum possible score for the four claims is 4. Therefore, the final answer correctness score can be calculated as:\n\n\\[\n\\text{Final score} = \\frac{2}{4} = 0.5\n\\]\n\nThus, the final answer correctness score is:\n\nSCORE: 0.5",
      "To assess the faithfulness of the generated answer based on the provided context, let's identify the factual claims made in the answer and evaluate them against the given context. Here are the claims found in the answer:\n\n1. **Claim 1**: \"The purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of a set of guidelines.\"\n   - This claim is fully supported by the context as it states directly that the Safety Checker checks and compares class probability with regard to guidelines.\n\n2. **Claim 2**: \"It aims to mitigate issues associated with models like Stable Diffusion.\"\n   - This claim is also fully supported as the context states this explicitly in relation to \"Safe Stable Diffusion\" mitigating well-known issues with models like Stable Diffusion.\n\n3. **Claim 3**: \"This contributes to the ethical development and use of the library.\"\n   - This claim is partially supported. While the context mentions ethical guidelines, it does not explicitly say that the Safety Checker directly contributes to ethical development. Thus, this claim can be seen as a reasonable inference based on the overall intent of the guidelines discussed.\n\nNow we can assign point values:\n- Claim 1: 1 point (fully supported)\n- Claim 2: 1 point (fully supported)\n- Claim 3: 0.5 points (partially supported with inference)\n\nNext, we calculate the faithfulness score:\n- Total points: 1 + 1 + 0.5 = 2.5\n- Total claims: 3\n- Faithfulness score calculation: 2.5 / 3 = 0.8333 (approximately 0.83)\n\nNow, we round to two decimal places to provide clarity. Thus, the final faithfulness score is:\n\nSCORE: 0.83",
      "To evaluate the faithfulness of the generated answer, I'll first identify the factual claims made in the answer and then assess each claim against the provided context.\n\n1. **Claim 1**: The purpose of the Safety Checker in the Diffusers library is to check and compare class probabilities against a set of guidelines.\n   - This claim is directly supported by the context, which states, \"It checks and compares the class probability of a set of guidelines.\" \n   - Evaluation: Fully supported (1 point)\n\n2. **Claim 2**: The Safety Checker aims to ensure safety in deployment.\n   - This claim is reasonable to infer from the context. While the context mentions the Safety Checker checks class probabilities \"to guide the development,\" it is logically consistent to draw the inference that this is for ensuring safety in deployment, particularly since the context includes \"Encouraging safety in deployment.\"\n   - Evaluation: Partially supported with some extrapolation (0.5 points)\n\n3. **Claim 3**: The Safety Checker aims to encourage safe usage and contributions within the library.\n   - This claim is also supported by the context. The phrase \"to guide the development, usersâ€™ contributions, and usage of the Diffusers library\" implies that the Safety Checker is intended to promote safe practices in contributions and usage.\n   - Evaluation: Fully supported (1 point)\n\nNow, let's summarize the points:\n- Claim 1: 1 point (fully supported)\n- Claim 2: 0.5 points (partially supported)\n- Claim 3: 1 point (fully supported)\n\nTotal points = 1 + 0.5 + 1 = 2.5\nTotal number of claims = 3\n\nCalculating the faithfulness score: \nScore = (2.5 / 3) = 0.8333 (approximately 0.83)\n\nThus, the final faithfulness score is:\nSCORE: 0.8333",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the correctness of a generated answer based on a reference answer.\n\nAnswer Correctness measures how closely the generated answer matches the reference answer. An answer is considered correct if it contains all the necessary information from the reference answer and is free of errors.\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the reference answer (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the reference answer (assign 0 points)\n\n\nReturn a final answer correctness score between 0 and 1, where:\n- 1.0: Perfect correctness (all claims fully supported)\n- 0.0: Complete incorrectness (no claims supported by reference)\n\n```\nReference Answer: The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.\n```\n\nAnswer: \n```\nThe purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of hard-coded harmful concepts against generated images to mitigate inappropriate degeneration in diffusion models. It aims to prevent the generation of harmful content by ensuring that such concepts are not present in the output. The harmful concepts are intentionally hidden to protect the integrity of the checker.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the correctness of the generated answer against the reference answer, I will identify the factual claims made in the generated answer and assess their alignment with the reference.\n\n1. Claim: \"The purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of hard-coded harmful concepts against generated images...\"\n   - This claim accurately reflects the main function of the Safety Checker described in the reference answer, focusing on checking class probability against harmful concepts. It is fully supported by the reference answer. **(1 point)**\n\n2. Claim: \"...to mitigate inappropriate degeneration in diffusion models.\"\n   - While this claim suggests mitigating inappropriate degeneration, it introduces ambiguity by not directly aligning with the phrasing of \"mitigate the risk of generating harmful content\" from the reference. The reference does not mention â€œinappropriate degeneration,â€ so this claim is partially supported, requiring some extrapolation to interpret it in the context of the original meaning. **(0.5 points)**\n\n3. Claim: \"It aims to prevent the generation of harmful content by ensuring that such concepts are not present in the output.\"\n   - This phrase captures the intent behind the Safety Checker as mentioned in the reference answer, which focuses on mitigating harmful content generation. It is fully supported as its meaning corresponds closely to that of the reference. **(1 point)**\n\n4. Claim: \"The harmful concepts are intentionally hidden to protect the integrity of the checker.\"\n   - This last claim introduces a new concept that is not present in the reference answer. There is no mention of harmful concepts being \"intentionally hidden\" in the reference. This claim does not receive support from the reference. **(0 points)**\n\nNow, I will sum the points for each claim: \n- Claim 1: 1 point\n- Claim 2: 0.5 points\n- Claim 3: 1 point\n- Claim 4: 0 points\n\nTotal points = 1 + 0.5 + 1 + 0 = 2.5 points out of a possible 4 points.\n\nTo convert this score into a final correctness score between 0 and 1:\n- 2.5/4 = 0.625\n\nThus, I would assess the answer correctness score as follows:\n\nSCORE: 0.625",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the precision of the retrieved context relative to the question\n\nContext Precision measures how efficiently the retrieved information is used to answer the question. High precision means most of the retrieved context was necessary and used in generating the answer, with minimal irrelevant information.\n\nEvaluation instructions:\n1. Identify which parts of the context were actually used to generate the answer.\n2. Calculate the proportion of the context that contains information relevant to and used in the answer.\n3. Consider both:\n   - Information density (how much of the context is relevant)\n   - Information utilization (how much of the relevant information was used)\n\n4. Rate the context precision on a scale from 0 to 1:\n   - 1.0: Perfect precision (all retrieved context is relevant and used efficiently)\n   - 0.5: Moderate precision (about half the context is relevant and used)\n   - 0.0: No precision (context is entirely irrelevant)\n\n```\nQuestion: What method is used to load a checkpoint for a task using `AutoPipeline`?\n\n```\nContext: ! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # autopipeline ` autopipeline ` is designed to : 1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the\n\nto load a checkpoint for a task without knowing the specific pipeline class to use 2. use multiple pipelines in your workflow based on the task, the ` autopipeline ` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the ` from _ pretrained ( ) ` method. to seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the ` from _ pipe ( ) ` method to transfer the components from the original pipeline to the new one. ` ` ` py from diffusers import autopipelinefortext2image import torch pipeline = autopipelinefortext2image. from _ pretrained ( \" runwayml / stable - diffusion - v1 - 5 \", torch _ dtype = torch. float16, use _ safetensors = true ). to ( \" cuda \" ) prompt = \" astronaut in a jungle, cold color palette, muted colors,\n\nautodoc ] ] autopipelineforimage2image - all - from _ pretrained - from _ pipe # # autopipelineforinpainting [ [ autodoc ] ] autopipelineforinpainting - all - from _ pretrained - from _ pipe\n\nweight ' : ' pytorch _ model - 00001 - of - 00003. bin ',... ` ` ` if you want to directly load such a sharded checkpoint inside a model without using [ ` ~ pretrainedmodel. from _ pretrained ` ] ( like you would do ` model. load _ state _ dict ( ) ` for a full checkpoint ) you should use [ ` ~ modeling _ utils. load _ sharded _ checkpoint ` ] : ` ` ` py > > > from transformers. modeling _ utils import load _ sharded _ checkpoint > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... load _ sharded _ checkpoint ( model, tmp _ dir ) ` ` ` #\n\n. bin ', ' pytorch _ model - 00002 - of - 00003. bin ', ' pytorch _ model - 00003 - of - 00003. bin ', ' pytorch _ model. bin. index. json ' ] ` ` ` on top of the configuration of the model, we see three different weights files, and an ` index. json ` file which is our index. a checkpoint like this can be fully reloaded using the [ ` ~ pretrainedmodel. from _ pretrained ` ] method : ` ` ` py > > > with tempfile. temporarydirectory ( ) as tmp _ dir :... model. save _ pretrained ( tmp _ dir, max _ shard _ size = \" 200mb \" )... new _ model = automodel. from _ pretrained ( tmp _ dir\n\nAnswer: \n```\nTo load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method, which automatically retrieves the relevant pipeline given the name or path to the pretrained weights. Additionally, to switch between tasks with the same checkpoint without reallocating memory, the `from_pipe()` method can be used to transfer components from the original pipeline to the new one.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the precision of the retrieved context relative to the question, we start by identifying the relevant parts of the context that were actually utilized in generating the answer.\n\n1. **Identifying relevant parts of the context:**\n   - The context contains the explanation regarding how `AutoPipeline` functions, particularly noting that it offers an easy way to load checkpoints without needing to know the specific pipeline class. \n   - It highlights the methods `from_pretrained()` and `from_pipe()`, which are crucial for the answer.\n   - The specific mention of `from_pretrained()` and its role in retrieving the pipeline when given the name or path to pretrained weights is particularly relevant.\n   - The use of `from_pipe()` for switching tasks without reallocating memory is also directly relevant.\n\n2. **Calculating the proportion of relevant context:**\n   - The context provided contains a lot of extraneous information, such as copyright and licensing information, which is irrelevant to the task.\n   - The relevant portion of the context discussing `AutoPipeline`, `from_pretrained()`, and `from_pipe()` can be summarized and selected, making it a part of the total context.\n   - Relevant context sentences specifically detailing the methods used are distilled down in the answer.\n\n3. **Information density and utilization:**\n   - The relevant information (about loading a checkpoint) is dense and utilized well in the answer. However, a significant amount of the context is not used at all (e.g., licensing information and other irrelevant details).\n   - The answer precisely references the relevant functions utilized in the context, thus demonstrating effective information utilization.\n\nOverall, while the relevant sections of the context were efficiently used to generate the answer, a considerable portion of the retrieved context was irrelevant to the question. Therefore, we would expect a moderate rating.\n\nGiven this assessment, I conclude that the score for context precision would be on the scale of about 0.7, indicating that a good amount of relevant information was used efficiently, but there's still a significant amount of irrelevant information present.\n\nFinal answer:\nSCORE: 0.7",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the correctness of a generated answer based on a reference answer.\n\nAnswer Correctness measures how closely the generated answer matches the reference answer. An answer is considered correct if it contains all the necessary information from the reference answer and is free of errors.\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the reference answer (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the reference answer (assign 0 points)\n\n\nReturn a final answer correctness score between 0 and 1, where:\n- 1.0: Perfect correctness (all claims fully supported)\n- 0.0: Complete incorrectness (no claims supported by reference)\n\n```\nReference Answer: x86_64-unknown-linux-musl\n```\n\nAnswer: \n```\nThe `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on a Linux system using the musl C library.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the correctness of the generated answer in relation to the reference answer, I will identify and analyze the factual claims in the generated answer and see how they relate to the reference.\n\n1. The generated answer states: \"The `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture on a Linux system using the musl C library.\"\n2. Firstly, the phrase \"x86_64 architecture\" matches the reference answer \"x86_64-unknown-linux-musl\" in terms of being for the x86_64 architecture. This claim is fully supported by the reference answer, so assign **1 point** here.\n3. The next part states that it is for a \"Linux system using the musl C library.\" The reference answer also indicates \"linux\" and implicitly suggests compatibility with musl due to its use in the naming. This aspect can be interpreted as partial support, as the phrase \"unknown\" in the reference doesn't directly negate that it is a Linux system or using musl, but it is inferred rather than explicitly stated. Therefore, this claim is **0.5 points** for partial support.\n\nOverall, the generated answer has:\n- One claim fully supported: **1 point**\n- One claim partially supported: **0.5 points**\n\nAdding these together gives a total score of 1.0 + 0.5 = 1.5 points, but since I must normalize it based on the number of claims (2 claims), the final score would be calculated as:\n\nFinal Score: (1.5 points / 2 claims) = 0.75\n\nThe generated answer provides a mostly accurate description but deviates slightly from the exact terminology without losing the core meaning.\n\nThus, the evaluation would yield the following:\n\nSCORE: 0.75",
      "To evaluate the faithfulness of the generated answer, I will identify and analyze the factual claims based on the provided context.\n\n### Claims in the Answer:\n1. \"To load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method.\"\n2. \"`from_pretrained()` method automatically retrieves the relevant pipeline given the name or path to the pretrained weights.\"\n3. \"To switch between tasks with the same checkpoint without reallocating memory, the `from_pipe()` method can be used.\"\n4. \"`from_pipe()` method transfers components from the original pipeline to the new one.\"\n\n### Evaluation of Each Claim:\n1. **Claim 1**: \"To load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method.\"\n   - This is **fully supported** by the context that states the `from_pretrained()` method is used for loading checkpoints without needing to know the pipeline class.\n   - **Score: 1 point**\n\n2. **Claim 2**: \"`from_pretrained()` method automatically retrieves the relevant pipeline given the name or path to the pretrained weights.\"\n   - This is also **fully supported** by the context, which explicitly mentions that `from_pretrained()` retrieves the relevant pipeline based on the name or path to pretrained weights.\n   - **Score: 1 point**\n\n3. **Claim 3**: \"To switch between tasks with the same checkpoint without reallocating memory, the `from_pipe()` method can be used.\"\n   - This is **fully supported** by the context where it is stated that the `from_pipe()` method is used to seamlessly switch between tasks without reallocating additional memory.\n   - **Score: 1 point**\n\n4. **Claim 4**: \"`from_pipe()` method transfers components from the original pipeline to the new one.\"\n   - This claim is **fully supported** as the context describes using the `from_pipe()` method for transferring components from the original pipeline to the new one.\n   - **Score: 1 point**\n\n### Total Score Calculation:\n- Total points awarded: \\(1 + 1 + 1 + 1 = 4\\)\n- Total number of claims: 4\n- Faithfulness Score = Total Points / Total Number of Claims = \\(4 / 4 = 1.0\\)\n\n### Final Answer:\nThe generated answer is entirely faithful to the context provided, as all claims made are fully supported by the context.\n\nSCORE: 1.0",
      "To evaluate the correctness of the generated answer, I will identify the factual claims made in the answer and assess how well they match the reference answer.\n\n1. The generated answer states: \"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is 'distilbert-base-uncased-finetuned-sst2-english.'\"\n\n   - This claim consists of two main components:\n     a. The term \"default checkpoint used by the sentiment analysis pipeline in the Transformers library\" is an assertion about the context in which the model is applied.\n     b. The stated name of the model, \"distilbert-base-uncased-finetuned-sst2-english,\" is directly matching with the reference answer.\n\nBased on this analysis:\n\n- The exact name of the model provided in the generated answer is \"distilbert-base-uncased-finetuned-sst2-english,\" which is fully supported by the reference answer. This part earns 1 point.\n  \n- However, the generated answer introduces extra context regarding the \"default checkpoint\" and \"sentiment analysis pipeline in the Transformers library,\" which is not explicitly stated in the reference answer. This additional context does slightly deviate from the succinctness of the reference but does not contradict any factual claims.\n\nSince the reference answer does not include this information about being a default checkpoint and being specific to a sentiment analysis pipeline in the Transformers library, I consider this extra context as not part of the reference, thus leading to a nuanced conclusion.\n\n- Therefore, the claim about the checkpoint context would be partially supported because it is true but not explicitly mentioned in the reference. This earns 0.5 points.\n\nCalculating the final score:\n- Fully supported claim: 1 point (name of the model)\n- Partially supported claim: 0.5 points (context about default checkpoint)\n\nTotal score = (1 + 0.5) / 2 = 0.75\n\nHowever, since we need a single point reflecting overall correctness, I will consider that the core claim about the model name is important, while the additional context isn't explicitly stated.\n\nTherefore, the overall correctness score could be interpreted as leaning towards:\n\nSCORE: 0.75",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the correctness of a generated answer based on a reference answer.\n\nAnswer Correctness measures how closely the generated answer matches the reference answer. An answer is considered correct if it contains all the necessary information from the reference answer and is free of errors.\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the reference answer (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the reference answer (assign 0 points)\n\n\nReturn a final answer correctness score between 0 and 1, where:\n- 1.0: Perfect correctness (all claims fully supported)\n- 0.0: Complete incorrectness (no claims supported by reference)\n\n```\nReference Answer: 10GB\n```\n\nAnswer: \n```\nThe maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10 GB. Checkpoints larger than this size will be split into smaller pieces. You can control the maximum size before sharding using the `max_shard_size` parameter.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the generated answer against the reference answer, we will identify the factual claims made in the generated answer and assess their correctness in relation to the reference answer.\n\n1. **Claim 1**: \"The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10 GB.\"\n   - This claim is factually correct and fully supported by the reference answer, which states \"10GB\". Therefore, this claim scores 1 point.\n\n2. **Claim 2**: \"Checkpoints larger than this size will be split into smaller pieces.\"\n   - This claim is inferential but logically follows from the context of sharding mentioned in the reference answer. Although sharding isn't explicitly stated in the reference answer, it can be derived from the knowledge about model checkpoint management. Thus, this claim scores 0.5 points for being partially supported.\n\n3. **Claim 3**: \"You can control the maximum size before sharding using the `max_shard_size` parameter.\"\n   - This claim is not explicitly mentioned or supported by the reference answer. There is no mention of the ability to control checkpoint sizes or any parameters related to sharding in the reference. Thus, this claim scores 0 points.\n\nNow, we calculate the total score:\n- Claim 1: 1 point\n- Claim 2: 0.5 points\n- Claim 3: 0 points\n- Total points = 1 + 0.5 + 0 = 1.5\n\nTo find the final answer correctness score, we normalize by dividing by the maximum possible points (which is 3 points, as there are three claims):\nFinal Score = Total points / Maximum points = 1.5 / 3 = 0.5.\n\nGiven this analysis, the final correctness score is:\n\nSCORE: 0.5",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess how relevant the retrieved context is to the given question\n\nContext Relevancy measures whether the retrieved information contains content that is useful for answering the question. Highly relevant context provides the necessary information to formulate a complete and accurate answer.\n\nEvaluation instructions:\n1. Identify the main information need(s) in the question.\n2. For each piece of context, assess:\n   - Whether it contains information directly relevant to answering the question\n   - How comprehensive the relevant information is\n   - Whether critical information for answering the question is missing\n\n3. Rate the overall context relevancy on a scale from 0 to 1:\n   - 1.0: Perfect relevancy (context contains all information needed to fully answer the question)\n   - 0.5: Moderate relevancy (context contains some relevant information but has significant gaps)\n   - 0.0: No relevancy (context contains no information related to the question)\n\n```\nQuestion: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n\n```\n\nContext: \n```\nIn Transformers version 4.18.0, model checkpoints that exceed 10 GB in size are automatically sharded into smaller pieces. Each of these smaller pieces will be less than 10 GB. You can control the maximum size before sharding using the `max_shard_size` parameter.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the context relevancy, I will follow the specified instructions.\n\n1. **Identify the main information need(s) in the question**:\n   The question asks specifically for the maximum size of a model checkpoint in Transformers version 4.18.0 before it is automatically sharded.\n\n2. **Assess the context**:\n   - **Relevance to the question**: The context directly answers the question by stating that model checkpoints exceeding 10 GB are automatically sharded. This directly addresses what the question is asking.\n   - **Comprehensiveness of the relevant information**: The context is comprehensive regarding the maximum size limit (10 GB) and explains that checkpoints exceeding this limit will be sharded. It also mentions the `max_shard_size` parameter, which could be useful for users wanting to control the sharding process.\n   - **Missing critical information**: There is no critical information missing for answering the question about the maximum size before sharding, as the provided information is complete.\n\n3. **Rate the overall context relevancy**: Since the context provides precise and complete information necessary to answer the question, I would rate it as follows:\n\nSCORE: 1.0",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the precision of the retrieved context relative to the question\n\nContext Precision measures how efficiently the retrieved information is used to answer the question. High precision means most of the retrieved context was necessary and used in generating the answer, with minimal irrelevant information.\n\nEvaluation instructions:\n1. Identify which parts of the context were actually used to generate the answer.\n2. Calculate the proportion of the context that contains information relevant to and used in the answer.\n3. Consider both:\n   - Information density (how much of the context is relevant)\n   - Information utilization (how much of the relevant information was used)\n\n4. Rate the context precision on a scale from 0 to 1:\n   - 1.0: Perfect precision (all retrieved context is relevant and used efficiently)\n   - 0.5: Moderate precision (about half the context is relevant and used)\n   - 0.0: No precision (context is entirely irrelevant)\n\n```\nQuestion: What is the purpose of the Safety Checker in the Diffusers library?\n\n```\nContext: diffusion : mitigating inappropriate degeneration in diffusion models ] ( https : / / arxiv. org / abs / 2211. 05105 ). - [ * * safety checker * * ] ( https : / / github. com / huggingface / diffusers / blob / main / src / diffusers / pipelines / stable _ diffusion / safety _ checker. py ) : it checks and compares the class probability of a set of hard - coded harmful concepts in the embedding space against an image after it has been generated. the harmful concepts are intentionally hidden to prevent reverse engineering of the checker. - * * staged released on the hub * * : in particularly sensitive situations, access to some repositories should be restricted. this staged release is an intermediary step that allows the repository â€™ s authors to have more control over its use. - * * licensing * * : [ openrails ] ( https\n\nsame level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and coherent. - * * accessibility * * : the diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. doing so makes research artifacts more accessible to the community. - * * reproducibility * * : we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the diffusers library. - * * responsibility * * : as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology ' s potential risks and dangers. # # examples of implementations : safety features and mechanisms the team works daily to make the technical and non - technical tools available to deal with the potential ethical and\n\n' s responsiveness and valuable feedback. # # scope the diffusers community will apply the following ethical guidelines to the project â€™ s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns. # # ethical guidelines the following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question. - * * transparency * * : we are committed to being transparent in managing prs, explaining our choices to users, and making technical decisions. - * * consistency * * : we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent. - * * simplicity * * : with a desire to make it easy to use and exploit the diffusers library, we are committed to keeping the project â€™ s goals lean and\n\n! - - copyright 2023 the huggingface team. all rights reserved. licensed under the apache license, version 2. 0 ( the \" license \" ) ; you may not use this file except in compliance with the license. you may obtain a copy of the license at http : / / www. apache. org / licenses / license - 2. 0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \" as is \" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. - - > # diffusers â€™ ethical guidelines # # preamble [ diffusers ] ( https : / / huggingface. co / docs / diffusers / index ) provides pre - trained diffusion models and serves as a modular toolbox for inference and training. given its real case applications in the world and potential negative impacts on society, we think it\n\ndiffusers experimental we are adding experimental code to support novel applications and usages of the diffusers library. currently, the following experiments are supported : * reinforcement learning via an implementation of the [ diffuser ] ( https : / / arxiv. org / abs / 2205. 09991 ) model.\n\nAnswer: \n```\nThe purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of hard-coded harmful concepts against generated images to mitigate inappropriate degeneration in diffusion models. It aims to prevent the generation of harmful content by ensuring that such concepts are not present in the output. The harmful concepts are intentionally hidden to protect the integrity of the checker.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the precision of the retrieved context relative to the question about the purpose of the Safety Checker in the Diffusers library, we will break down the context and the answer provided.\n\n1. **Identifying Used Context**:\n   The key information from the context that was used in the answer includes:\n   - The function of the Safety Checker: â€œit checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated.â€\n   - Its aim to prevent harmful content: â€œto mitigate inappropriate degeneration in diffusion models.â€\n   - The detail that the harmful concepts are hidden: â€œthe harmful concepts are intentionally hidden to prevent reverse engineering of the checker.â€\n\n2. **Calculating Proportion of Context**:\n   The total context provided contains multiple aspects, some of which pertain to the Safety Checker while the rest discusses licensing, ethics, transparency, accessibility, etc. The only parts that relate directly to the question about the Safety Checker are its specific description and purpose. \n\n   - Relevant context includes the specific section on the Safety Checker: â€œit checks and compares the class probability â€¦ against an image â€¦ to mitigate inappropriate degeneration in diffusion models.â€\n   - Irrelevant sections include information on ethical guidelines, project management, simplicity, accessibility, and reproducibility, none of which help answer the question.\n\n3. **Information Density and Utilization**:\n   The relevant part of the context about the Safety Checker is succinct yet informative. It clearly outlines its purpose and functionality relevant to the question. Most of the specific details regarding its operation were utilized effectively in the answer.\n\n   In total, out of the entire context provided, only a small part was relevant to the question. However, within that relevant part, the answer used all necessary information to formulate a comprehensive response.\n\n4. **Precision Rating**:\n   Given this assessment:\n   - The relevant information about the Safety Checker is utilized fully.\n   - However, compared to the entire context provided, only a small portion of it was relevant.\n\nThis leads us to conclude that while the answer utilized the relevant information efficiently, the overall proportion of relevant context within the total retrieved context is low.\n\n**Score:**\nConsidering both the information density and utilization, the context precision would be rated as moderate.\n\nFinal score representation:\nSCORE: 0.5",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the correctness of a generated answer based on a reference answer.\n\nAnswer Correctness measures how closely the generated answer matches the reference answer. An answer is considered correct if it contains all the necessary information from the reference answer and is free of errors.\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the reference answer (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the reference answer (assign 0 points)\n\n\nReturn a final answer correctness score between 0 and 1, where:\n- 1.0: Perfect correctness (all claims fully supported)\n- 0.0: Complete incorrectness (no claims supported by reference)\n\n```\nReference Answer: 10GB\n```\n\nAnswer: \n```\nIn Transformers version 4.18.0, model checkpoints that exceed 10 GB in size are automatically sharded into smaller pieces. Each of these smaller pieces will be less than 10 GB. You can control the maximum size before sharding using the `max_shard_size` parameter.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "To evaluate the correctness of the generated answer in relation to the reference answer, we need to identify the factual claims made in the generated answer and compare them to the reference answer.\n\n1. **Claim: \"In Transformers version 4.18.0, model checkpoints that exceed 10 GB in size are automatically sharded into smaller pieces.\"**\n   - This information provides context about the software version as well as a process concerning model checkpoints. However, it goes beyond the reference answer, which only states \"10GB\" without any mention of sharding or specific software version. Thus, this claim contains information that is not supported by the reference answer. **Score: 0 points.**\n\n2. **Claim: \"Each of these smaller pieces will be less than 10 GB.\"**\n   - This claim relates directly to the size limitation mentioned in the reference answer. Since the reference is simply stating \"10GB,\" the implication that smaller pieces would be less than this size closely aligns with the notions of size management implied by the reference. Although the phrase is not explicitly supported (it does not mention sharding in the reference), it is a logical inference. Still, it doesn't fully restate or confirm the reference's information directly. **Score: 0.5 points.**\n\n3. **Claim: \"You can control the maximum size before sharding using the `max_shard_size` parameter.\"**\n   - This statement introduces new information about a parameter for sharding, which is not included in the reference answer at all. Therefore, since this is neither supported nor implied by the reference answer, it cannot earn any points. **Score: 0 points.**\n\nNow, summing up the scores:\n- Claim 1: 0 points\n- Claim 2: 0.5 points\n- Claim 3: 0 points\n\nThe total sum of points is 0.5 out of a possible 3 points, reflecting that only some information was relevant but not entirely aligned with the reference answer.\n\nTo calculate the final score, we need to normalize the score (0.5 points out of a maximum of 3 points) to a scale of 0 to 1. This gives us a final score of:\n\nFinal Score = 0.5 / 3 = 0.1667 (approximately)\n\nConsidering this approximation, I will round it for the final answer rating.\n\nTherefore, I will present the final evaluation:\n\nSCORE: 0.17",
      "You are evaluating text that will be pasted below. You are an expert evaluator of retrieval-augmented generation systems. Your task is to assess the correctness of a generated answer based on a reference answer.\n\nAnswer Correctness measures how closely the generated answer matches the reference answer. An answer is considered correct if it contains all the necessary information from the reference answer and is free of errors.\n\nEvaluation instructions:\n1. Identify all factual claims in the generated answer.\n2. For each claim, determine if it is:\n   - Fully supported by the reference answer (assign 1 point)\n   - Partially supported with some extrapolation (assign 0.5 points)\n   - Not supported or contradicted by the reference answer (assign 0 points)\n\n\nReturn a final answer correctness score between 0 and 1, where:\n- 1.0: Perfect correctness (all claims fully supported)\n- 0.0: Complete incorrectness (no claims supported by reference)\n\n```\nReference Answer: from_pretrained()\n```\n\nAnswer: \n```\nThe method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This method automatically retrieves the relevant pipeline based on the name or path to the pretrained weights. Additionally, `from_pipe()` can be used to transfer components from the original pipeline to a new one without reallocating memory.\n```\n\nFirst, explain your reasoning for the numeric value. Then, output your final answer in the following format on a new line: SCORE: numeric_value",
      "Let's evaluate the generated answer against the reference answer.\n\n### Step 1: Identify factual claims in the generated answer.\n1. The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`.\n2. This method automatically retrieves the relevant pipeline based on the name or path to the pretrained weights.\n3. `from_pipe()` can be used to transfer components from the original pipeline to a new one.\n4. Using `from_pipe()` does not reallocate memory.\n\n### Step 2: Determine support for each claim against the reference answer.\n1. **Claim 1**: The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`.\n   - *Evaluation*: This claim is a direct statement that matches the reference answer. It is fully supported. \n   - **Score**: 1 point.\n   \n2. **Claim 2**: This method automatically retrieves the relevant pipeline based on the name or path to the pretrained weights.\n   - *Evaluation*: While this is a logical extrapolation based on the use of `from_pretrained()`, the specific functionality of automatic retrieval is not included in the reference answer. It suggests additional functionality beyond what is explicitly stated.\n   - **Score**: 0.5 points.\n   \n3. **Claim 3**: `from_pipe()` can be used to transfer components from the original pipeline to a new one.\n   - *Evaluation*: This claim introduces a method (`from_pipe()`) that is not mentioned in the reference answer. Hence, it is not supported.\n   - **Score**: 0 points.\n\n4. **Claim 4**: Using `from_pipe()` does not reallocate memory.\n   - *Evaluation*: This is also an assertion about the `from_pipe()` method that is not mentioned in the reference answer, making it unsupported.\n   - **Score**: 0 points.\n\n### Step 3: Calculate the final score.\nNow, let's total the points:\n- Claim 1: 1 point\n- Claim 2: 0.5 points\n- Claim 3: 0 points\n- Claim 4: 0 points\n\nTotal Score = 1 + 0.5 + 0 + 0 = 1.5 points.\n\nGiven that the maximum possible score is 4 points (1 for each claim), we can convert this into a score out of 1. \n\nFinal score = Total Score / Maximum Possible Score = 1.5 / 4 = 0.375.\n\nHowever, we will consider how to represent this score in the context of possible evaluation outputs which hold more relevance to the accuracy of the claims, primarily focusing on the correctness of the claims versus the reference answer directly.\n\nThus, I would assign a final answer correctness score based on the support for the correct claims.\n\nThe best representation of the final correctness that reflects the outcome accurately would be:\nSCORE: 0.375. \n\nGiven the requirement to fit strictly within a 0-1 score while reflecting the overall correctness appropriately, it should be conjectured or rounded depending on the usage of the evaluative context.\n\n##### Final output:\n```\nSCORE: 0.5\n```",
      "Let's evaluate the generated answer against the provided context step by step.\n\n1. **Claim evaluation**:\n   1. **Claim**: \"The purpose of the Safety Checker in the Diffusers library is to mitigate inappropriate degeneration in diffusion models.\"\n      - **Evaluation**: This claim is fully supported by the context which explicitly states that the Safety Checker is related to mitigating inappropriate degeneration in diffusion models. \n      - **Score**: 1 point\n      \n   2. **Claim**: \"It checks and compares the class probability of harmful concepts against generated images.\"\n      - **Evaluation**: This is also directly supported by the context, which states that the Safety Checker compares the class probability of harmful concepts against generated images.\n      - **Score**: 1 point\n      \n   3. **Claim**: \"It uses hard-coded harmful concepts that are intentionally hidden to prevent reverse engineering.\"\n      - **Evaluation**: This is fully supported since the context mentions that harmful concepts are hard-coded and intentionally hidden to avoid reverse engineering.\n      - **Score**: 1 point\n      \n   4. **Claim**: \"This mechanism helps ensure safer deployment of models like stable diffusion, which may be trained on unfiltered datasets.\"\n      - **Evaluation**: This claim is partially supported. The context does indicate that the Safety Checker relates to safer deployment of models that suffer from issues due to unfiltered datasets, but it is not explicitly stated that the Safety Checker *helps* this deployment. However, this is a reasonable inference given the context of the Safety Checker and its purpose.\n      - **Score**: 0.5 points\n\n2. **Calculating the total score**:\n   - Total claims identified: 4\n   - Points for each claim: 1, 1, 1, 0.5\n   - Total points = 1 + 1 + 1 + 0.5 = 3.5\n\n3. **Calculating the faithfulness score**:\n   - Faithfulness Score = Total Points / Total Claims = 3.5 / 4 = 0.875\n\nThus, the final faithfulness score is:\n\nSCORE: 0.875"
    ]
  }
}